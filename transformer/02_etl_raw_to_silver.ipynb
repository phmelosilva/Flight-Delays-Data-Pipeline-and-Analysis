{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac440cbb-1ee8-4805-acd8-b3a2d2cd9d8c",
   "metadata": {},
   "source": [
    "# 02_etl_raw_to_silver\n",
    "---\n",
    "Este notebook executa o processo `ETL` que transfere os dados da camada **Raw** para a **Silver**, englobando transformações, padronização, movimentação dos arquivos e carga dos dados no **PostgreSQL**, dando continuidade ao pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85066341-bac4-4531-9b10-612b34ccc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "raw_path = \"/opt/airflow/data-layer/raw\"\n",
    "silver_path = \"/opt/airflow/data-layer/silver\"\n",
    "\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6b19d-4dbc-45d4-8fa8-7f5ef9b3a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import DataFrame, functions as F, Window\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "from transformer.utils.file_io import find_partition\n",
    "from transformer.utils.helpers import to_date_from_ymd\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres\n",
    "from transformer.utils.postgre_helpers import assert_table_rowcount\n",
    "from transformer.utils.quality_gates_silver_aggregated import run_quality_gates_silver_aggregated\n",
    "from transformer.utils.quality_gates_silver_base import run_quality_gates_silver_base\n",
    "from transformer.utils.quality_gates_silver_flights import run_quality_gates_silver_flights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f3cc8-3877-4335-bcda-58ccee791645",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 1: airlines_transform\n",
    "\n",
    "Este job realiza a transformação e validação do dataset de companhias aéreas (`airlines`) da camada **Raw** para a camada **Silver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0286f-fe80-4406-b1e0-3f53af03d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"transform_airlines\")\n",
    "\n",
    "spark = get_spark_session(\"TransformAirlines\")\n",
    "log.info(\"[Airlines] Sessão Spark iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131b72e-a2c7-4fc1-bd9c-c9fef8bc6f5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de transformação do dataset `airlines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0454bf-97c5-490b-ada7-547a7d53cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_airlines(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma e valida o DataFrame de companhias aéreas para a camada silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame bruto da camada Bronze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame padronizado pronto para a camada Silver.\n",
    "    \"\"\"\n",
    "    log.info(\"[Airlines] Iniciando transformações.\")\n",
    "\n",
    "    # Verifica se as colunas obrigatórias estão presentes\n",
    "    log.info(\"[Airlines] Verificando presença de colunas obrigatórias.\")\n",
    "    required = {\"IATA_CODE\", \"AIRLINE\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"[Airlines][ERROR] Colunas faltando no dataset: {missing}.\")\n",
    "\n",
    "    # Renomeia e converte tipos de colunas\n",
    "    log.info(\"[Airlines] Renomeando e convertendo os tipos das colunas.\")\n",
    "    df2 = (\n",
    "        df.withColumnRenamed(\"IATA_CODE\", \"airline_iata_code\")\n",
    "          .withColumnRenamed(\"AIRLINE\", \"airline_name\")\n",
    "          .withColumn(\"airline_iata_code\", F.col(\"airline_iata_code\").cast(StringType()))\n",
    "          .withColumn(\"airline_name\", F.col(\"airline_name\").cast(StringType()))\n",
    "    )\n",
    "\n",
    "    # Verifica duplicidade na chave primária\n",
    "    log.info(\"[Airlines] Verificando duplicidade da pk.\")\n",
    "    dup_count = df2.groupBy(\"airline_iata_code\").count().filter(F.col(\"count\") > 1).count()\n",
    "    if dup_count > 0:\n",
    "        raise ValueError(f\"[Airlines][ERROR] airline_iata_code não é único: {dup_count} duplicatas.\")\n",
    "\n",
    "    # Padroniza os nomes das colunas para minúsculo\n",
    "    log.info(\"[Airlines] Padronizando nomes de colunas para minúsculo.\")\n",
    "    df2 = df2.toDF(*[c.lower() for c in df2.columns])\n",
    "\n",
    "    log.info(\"[Airlines] Transformação concluída com sucesso.\")\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d923f-8541-4514-ab5c-f9f79e38a512",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `airlines_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c494168-8fed-40ae-b94a-ea924c71fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Airlines] Iniciando job de trasnformação de 'airlines'.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(raw_path, mode=run_mode, date_str=run_date)\n",
    "    src = Path(raw_path) / source_partition / \"PARQUET\" / \"airlines.parquet\"\n",
    "    dst = Path(silver_path) / source_partition / \"PARQUET\" / \"airlines.parquet\"\n",
    "\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"[Airlines][ERROR] Arquivo não encontrado: {src}.\")\n",
    "\n",
    "    # Leitura do dataset bruto\n",
    "    log.info(f\"[Airlines] Lendo dataset: {src}.\")\n",
    "    df = spark.read.parquet(str(src))\n",
    "\n",
    "    # Aplica transformações\n",
    "    df_tf = transform_airlines(df)\n",
    "\n",
    "    # Executa quality gates\n",
    "    required_cols = [\"airline_iata_code\", \"airline_name\"]\n",
    "    pk_cols = [\"airline_iata_code\"]\n",
    "    \n",
    "    log.info(\"[Airlines] Executando quality gates.\")\n",
    "    \n",
    "    run_quality_gates_silver_base(\n",
    "        df=df_tf,\n",
    "        name=\"airlines_silver\",\n",
    "        required_columns=required_cols,\n",
    "        pk_columns=pk_cols,\n",
    "    )\n",
    "    \n",
    "    log.info(\"[Airlines] Quality gates concluídos com sucesso.\")\n",
    "\n",
    "    # Cria diretório de destino e grava o resultado na silver\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_tf.write.mode(\"overwrite\").parquet(str(dst))\n",
    "    log.info(f\"[Refinement][Airlines] Dataset salvo na camada silver: {dst}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Airlines][ERROR] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Airlines] Job de trasnformação de 'airlines' encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d730b79a-12b5-4257-a4ec-660674056e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df_tf.printSchema()\n",
    "\n",
    "df_tf.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc56ea5-9809-40f5-b947-544d941cdd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Airlines] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d26e6-6552-485b-a920-466ad7f952d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 2: airports_transform\n",
    "\n",
    "Este job realiza a transformação e validação do dataset de aeroportos (`airports`) da camada **Raw** para a camada **Silver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd760de4-2bff-403a-ada4-ec59eab26835",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"transform_airports\")\n",
    "\n",
    "spark = get_spark_session(\"TransformAirports\")\n",
    "log.info(\"[Airports] Sessão Spark iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa78b7d-407b-4fde-84e1-4d22d6b5a55e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de trasformação do dataset `airports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecbe176-c0e5-40c1-9404-dc596ab45bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_airports(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma e valida o DataFrame de aeroportos para a camada silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame bruto lido da camada bronze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame limpo e padronizado para a camada silver.\n",
    "    \"\"\"\n",
    "    log.info(\"[Airports] Iniciando transformações.\")\n",
    "\n",
    "    # Verifica colunas obrigatórias\n",
    "    required = {\"IATA_CODE\", \"LATITUDE\", \"LONGITUDE\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"[Airports][ERROR] Colunas faltando no dataset: {missing}.\")\n",
    "\n",
    "    # Correções de coordenadas faltosas\n",
    "    corrections = {\n",
    "        \"ECP\": {\"LATITUDE\": 30.3549, \"LONGITUDE\": -86.6160},\n",
    "        \"PBG\": {\"LATITUDE\": 44.6895, \"LONGITUDE\": -68.0448},\n",
    "        \"UST\": {\"LATITUDE\": 42.0703, \"LONGITUDE\": -87.9539},\n",
    "    }\n",
    "\n",
    "    # Tipagem e renomeação de colunas principais\n",
    "    df2 = (\n",
    "        df.withColumnRenamed(\"IATA_CODE\", \"airport_iata_code\")\n",
    "          .withColumn(\"airport_iata_code\", F.col(\"airport_iata_code\").cast(StringType()))\n",
    "          .withColumn(\"LATITUDE\", F.col(\"LATITUDE\").cast(DoubleType()))\n",
    "          .withColumn(\"LONGITUDE\", F.col(\"LONGITUDE\").cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "    # Remove coluna COUNTRY\n",
    "    if \"COUNTRY\" in df2.columns:\n",
    "        df2 = df2.drop(\"COUNTRY\")\n",
    "\n",
    "    # Aplica correções manuais de coordenadas\n",
    "    for code, coords in corrections.items():\n",
    "        df2 = df2.withColumn(\n",
    "            \"LATITUDE\",\n",
    "            F.when(F.col(\"airport_iata_code\") == code, F.lit(coords[\"LATITUDE\"])).otherwise(F.col(\"LATITUDE\"))\n",
    "        ).withColumn(\n",
    "            \"LONGITUDE\",\n",
    "            F.when(F.col(\"airport_iata_code\") == code, F.lit(coords[\"LONGITUDE\"])).otherwise(F.col(\"LONGITUDE\"))\n",
    "        )\n",
    "\n",
    "    # Renomeia colunas e força lowercase\n",
    "    rename_map = {\n",
    "        \"AIRPORT\": \"airport_name\",\n",
    "        \"CITY\": \"city\",\n",
    "        \"STATE\": \"state\",\n",
    "        \"LATITUDE\": \"latitude\",\n",
    "        \"LONGITUDE\": \"longitude\",\n",
    "    }\n",
    "    for old, new in rename_map.items():\n",
    "        if old in df2.columns:\n",
    "            df2 = df2.withColumnRenamed(old, new)\n",
    "\n",
    "    # Normaliza nomes para minúsculo\n",
    "    df2 = df2.toDF(*[c.lower() for c in df2.columns])\n",
    "\n",
    "    log.info(\"[Airports] Transformação concluída com sucesso.\")\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7224d44-bfd3-47f3-927f-5383d30e98e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `airports_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f357c3-c198-42aa-923f-9e71cffd374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Airports] Iniciando job de trasnformação de 'airports'.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(bronze_path, mode=run_mode, date_str=run_date)\n",
    "    src = Path(bronze_path) / source_partition / \"PARQUET\" / \"airports.parquet\"\n",
    "    dst = Path(silver_path) / source_partition / \"PARQUET\" / \"airports.parquet\"\n",
    "\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"[Airports][ERROR] Arquivo não encontrado: {src}.\")\n",
    "\n",
    "    # Lê dataset e aplica transformações\n",
    "    log.info(f\"[Airports] Lendo dataset: {src}.\")\n",
    "    df = spark.read.parquet(str(src))\n",
    "    \n",
    "    df_tf = transform_airports(df)\n",
    "\n",
    "    # Executa quality gates\n",
    "    log.info(\"[Airports] Executando quality gates.\")\n",
    "\n",
    "    required_cols = [\"airport_iata_code\", \"airport_name\", \"city\", \"state\", \"latitude\", \"longitude\"]\n",
    "    pk_cols = [\"airport_iata_code\"]\n",
    "\n",
    "    run_quality_gates_silver_base(\n",
    "        df=df_tf,\n",
    "        name=\"airports_silver\",\n",
    "        required_columns=required_cols,\n",
    "        pk_columns=pk_cols,\n",
    "    )\n",
    "\n",
    "    log.info(\"[Airports] Quality gates concluídos.\")\n",
    "\n",
    "    # Cria diretório e escreve resultado\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_tf.write.mode(\"overwrite\").parquet(str(dst))\n",
    "\n",
    "    log.info(f\"[Airports] Dataset salvo na camada silver: {dst}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Airports][ERROR] Falha na execução do job: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Airports] Job de trasnformação de 'airports' encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998559c3-b78d-46a8-a21f-c09819446360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df_tf.printSchema()\n",
    "\n",
    "df_tf.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea774e-ce90-492c-8a47-cbcfe4668f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Airlines] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408151b6-0ac1-4c05-a753-30864093ecd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 3: flights_transform\n",
    "\n",
    "Este job transforma e normaliza o dataset de voos (`flights`) da **Raw** para a **Silver** (pré-join), gerando um arquivo intermediário a ser consumido na etapa de agregação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2b8b5-0c7e-4dea-800d-3d02ef0e172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"transform_flights_silver\")\n",
    "\n",
    "spark = get_spark_session(\"TransformFlightsSilver\")\n",
    "log.info(\"[Flights] Sessão Spark iniciada.\")\n",
    "\n",
    "# Ajustes performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9db60-6a6f-4088-b5bb-f90076c058c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo funções de normalização das colunas de datas, de diferença de tempo e transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0921e8e-4794-4897-8005-bcd7e695939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_time_expr(col_name: str) -> F.Column:\n",
    "    \"\"\"\n",
    "    Normaliza valores de horário removendo casas decimais e preenchendo zeros à esquerda.\n",
    "    \"\"\"\n",
    "    return F.when(\n",
    "        F.col(col_name).isNotNull(),\n",
    "        F.lpad(F.regexp_replace(F.col(col_name).cast(\"string\"), r\"\\.0$\", \"\"), 4, \"0\")\n",
    "    )\n",
    "\n",
    "\n",
    "def abs_min_diff(c1: str, c2: str) -> F.Column:\n",
    "    \"\"\"\n",
    "    Retorna a diferença absoluta entre dois horários em minutos.\n",
    "    \"\"\"\n",
    "    return F.abs(F.col(c1).cast(\"long\") - F.col(c2).cast(\"long\")) / 60.0\n",
    "\n",
    "\n",
    "def transform_flights(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma e valida o DataFrame de voos (pré-join) para a camada Silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame bruto lido da camada Bronze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame transformado e validado.\n",
    "    \"\"\"\n",
    "    log.info(\"[Flights] Iniciando transformações.\")\n",
    "\n",
    "    # Validação mínima de colunas obrigatórias\n",
    "    log.info(\"[Flights] Verificando presença de colunas obrigatórias.\")\n",
    "    required = {\"YEAR\", \"MONTH\", \"DAY\", \"AIRLINE\", \"FLIGHT_NUMBER\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"[Flights][ERROR] Colunas faltando: {missing}.\")\n",
    "\n",
    "    # Filtro inicial e padronização de nomes\n",
    "    log.info(\"[Flights] Aplicando filtros iniciais e padronizando nomes.\")\n",
    "    df2 = (\n",
    "        df.filter((F.col(\"DIVERTED\") != 1) & (F.col(\"CANCELLED\") != 1))\n",
    "          .toDF(*[c.lower() for c in df.columns])\n",
    "          .withColumn(\"flight_date\", to_date_from_ymd(F.col(\"year\"), F.col(\"month\"), F.col(\"day\")))\n",
    "          .withColumnRenamed(\"year\", \"flight_year\")\n",
    "          .withColumnRenamed(\"month\", \"flight_month\")\n",
    "          .withColumnRenamed(\"day\", \"flight_day\")\n",
    "          .withColumnRenamed(\"day_of_week\", \"flight_day_of_week\")\n",
    "    )\n",
    "\n",
    "    # Normalização de horários\n",
    "    log.info(\"[Flights] Normalização de colunas com horários.\")\n",
    "    time_cols = [\n",
    "        \"scheduled_departure\", \"departure_time\",\n",
    "        \"scheduled_arrival\", \"arrival_time\",\n",
    "        \"wheels_off\", \"wheels_on\",\n",
    "    ]\n",
    "\n",
    "    for col_name in time_cols:\n",
    "        if col_name in df2.columns:\n",
    "            tmp = f\"{col_name}_str\"\n",
    "            df2 = (\n",
    "                df2.withColumn(tmp, normalize_time_expr(col_name))\n",
    "                    .withColumn(\n",
    "                        col_name,\n",
    "                        F.to_timestamp(\n",
    "                            F.concat_ws(\" \", F.col(\"flight_date\").cast(\"string\"), F.col(tmp)),\n",
    "                            \"yyyy-MM-dd HHmm\"\n",
    "                        )\n",
    "                    )\n",
    "                    .drop(tmp)\n",
    "            )\n",
    "\n",
    "    # Detecção de horários trocados\n",
    "    log.info(\"[Flights] Correção de horários trocados.\")\n",
    "    df2 = (\n",
    "        df2\n",
    "        .withColumn(\"diff_dep_sched_arr\", abs_min_diff(\"departure_time\", \"scheduled_arrival\"))\n",
    "        .withColumn(\"diff_dep_sched_dep\", abs_min_diff(\"departure_time\", \"scheduled_departure\"))\n",
    "        .withColumn(\"diff_arr_sched_dep\", abs_min_diff(\"arrival_time\", \"scheduled_departure\"))\n",
    "        .withColumn(\"diff_arr_sched_arr\", abs_min_diff(\"arrival_time\", \"scheduled_arrival\"))\n",
    "        .withColumn(\n",
    "            \"is_swapped\",\n",
    "            (F.col(\"diff_dep_sched_arr\") < F.col(\"diff_dep_sched_dep\")) &\n",
    "            (F.col(\"diff_arr_sched_dep\") < F.col(\"diff_arr_sched_arr\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"departure_time_tmp\",\n",
    "            F.when(F.col(\"is_swapped\"), F.col(\"arrival_time\")).otherwise(F.col(\"departure_time\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"arrival_time_tmp\",\n",
    "            F.when(F.col(\"is_swapped\"), F.col(\"departure_time\")).otherwise(F.col(\"arrival_time\"))\n",
    "        )\n",
    "        .drop(\"departure_time\", \"arrival_time\")\n",
    "        .withColumnRenamed(\"departure_time_tmp\", \"departure_time\")\n",
    "        .withColumnRenamed(\"arrival_time_tmp\", \"arrival_time\")\n",
    "        .drop(\n",
    "            \"diff_dep_sched_arr\", \"diff_dep_sched_dep\",\n",
    "            \"diff_arr_sched_dep\", \"diff_arr_sched_arr\", \"is_swapped\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Conversão numérica e substituição de nulos\n",
    "    log.info(\"[Flights] Normalizando colunas numéricas.\")\n",
    "    numeric_cols = [\n",
    "        \"departure_delay\", \"arrival_delay\", \"taxi_out\", \"taxi_in\",\n",
    "        \"air_time\", \"elapsed_time\", \"scheduled_time\", \"distance\",\n",
    "        \"air_system_delay\", \"security_delay\", \"airline_delay\",\n",
    "        \"late_aircraft_delay\", \"weather_delay\",\n",
    "    ]\n",
    "    delay_cols = [\n",
    "        \"air_system_delay\", \"security_delay\", \"airline_delay\",\n",
    "        \"late_aircraft_delay\", \"weather_delay\",\n",
    "    ]\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        if c in df2.columns:\n",
    "            expr = F.col(c).cast(DoubleType())\n",
    "            if c in delay_cols:\n",
    "                expr = F.coalesce(expr, F.lit(0.0))\n",
    "            df2 = df2.withColumn(c, expr)\n",
    "\n",
    "    # Ajuste de voos overnight\n",
    "    log.info(\"[Flights] Ajustes nos voos que atravessa dia.\")\n",
    "    df2 = (\n",
    "        df2.withColumn(\n",
    "            \"is_overnight_flight\",\n",
    "            F.when(\n",
    "                (F.col(\"arrival_time\").isNotNull()) &\n",
    "                (F.col(\"departure_time\").isNotNull()) &\n",
    "                (F.hour(F.col(\"arrival_time\")) < F.hour(F.col(\"departure_time\"))),\n",
    "                F.lit(True)\n",
    "            ).otherwise(F.lit(False))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"arrival_time\",\n",
    "            F.when(F.col(\"is_overnight_flight\"),\n",
    "                   F.col(\"arrival_time\") + F.expr(\"INTERVAL 1 DAY\"))\n",
    "             .otherwise(F.col(\"arrival_time\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filtros finais\n",
    "    log.info(\"[Flights] Aplicando filtros finais.\")\n",
    "    df2 = df2.filter(\n",
    "        (F.col(\"departure_time\").isNotNull()) &\n",
    "        (F.col(\"arrival_time\").isNotNull()) &\n",
    "        (F.col(\"arrival_time\") > F.col(\"departure_time\")) &\n",
    "        (~F.col(\"origin_airport\").rlike(\"^[0-9]+$\")) &\n",
    "        (~F.col(\"destination_airport\").rlike(\"^[0-9]+$\"))\n",
    "    )\n",
    "\n",
    "    # Remoção de colunas desnecessárias\n",
    "    log.info(\"[Flights] Removendo colunas desnecessárias.\")\n",
    "    drop_cols = [c for c in [\"diverted\", \"cancelled\", \"cancellation_reason\"] if c in df2.columns]\n",
    "    if drop_cols:\n",
    "        df2 = df2.drop(*drop_cols)\n",
    "\n",
    "    log.info(\"[Flights] Transformação concluída.\")\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be8b7d-23dd-44c9-9d0b-3cad38c3b6b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `flights_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adf024-dab1-441e-9f03-48479c9b26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Flights] Iniciando job de trasnformação de 'flights'.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(raw_path, mode=run_mode, date_str=run_date)\n",
    "    src = Path(raw_path) / source_partition / \"PARQUET\" / \"flights.parquet\"\n",
    "    dst_dir = Path(silver_path) / source_partition / \"PARQUET\"\n",
    "    dst = dst_dir / \"flights_pre_join.parquet\"\n",
    "    airports_src = dst_dir / \"airports.parquet\"\n",
    "\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"[Flights][ERROR] Arquivo não encontrado: {src}.\")\n",
    "    if not airports_src.exists():\n",
    "        raise FileNotFoundError(f\"[Flights][ERROR] Airports não encontrado na silver: {airports_src}.\")\n",
    "\n",
    "    # Leitura e transformação\n",
    "    log.info(f\"[Flights] Lendo datasets: {src} e {airports_src}.\")\n",
    "    \n",
    "    df = spark.read.parquet(str(src))\n",
    "    airports_df = spark.read.parquet(str(airports_src))\n",
    "    df_tf = transform_flights(df)\n",
    "    \n",
    "    # Quality gate\n",
    "    run_quality_gates_silver_flights(df_tf, airports_df)\n",
    "\n",
    "    # Escrita do resultado intermediário\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "    df_tf.coalesce(1).write.mode(\"overwrite\").parquet(str(dst))\n",
    "    spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
    "\n",
    "    log.info(f\"[Flights] Dataset salvo na silver: {dst}.\")\n",
    "\n",
    "    # Libera cache\n",
    "    df_tf.unpersist()\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Flights][ERROR] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Flights] Fim do job de transformação de 'flights'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20855b7-21d2-4ae2-80be-bbd879a6bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "path_df = Path(silver_path) / \"2025-11-12\" / \"PARQUET\" / \"flights_pre_join.parquet\" # Verificar a data quando rodar\n",
    "\n",
    "df = spark.read.parquet(str(path_df))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.limit(5).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac082c-49d9-4ac4-954b-8a876bd3ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Flights] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c0620-ccae-4d1e-bce0-9266c557c35f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 4: silver_aggregate\n",
    "\n",
    "Este job realiza a agregação da camada **Silver**, unindo os datasets já tratados de `flights`, `airlines` e `airports` em um único dataset consolidado `flights_aggregated.parquet`, conforme o **DDL** da camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f43b4-2954-4232-831c-4cdefabd816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"silver_aggregate\")\n",
    "\n",
    "spark = get_spark_session(\"RefinementSilverAggregate\")\n",
    "log.info(\"[Aggregate] Sessão Spark iniciada.\")\n",
    "\n",
    "# Ajustes performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c54a97-3760-4977-9468-3cd015d8a7ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definido função de agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e671e026-7e80-4864-a9d2-c329dfe991bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_flights_df(\n",
    "    flights_silver_df: DataFrame,\n",
    "    airlines_silver_df: DataFrame,\n",
    "    airports_silver_df: DataFrame,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Constrói o DataFrame agregado da camada Silver (flights_aggregated), unindo:\n",
    "        - flights_pre_join.parquet\n",
    "        - airlines.parquet\n",
    "        - airports.parquet\n",
    "\n",
    "    Args:\n",
    "        flights_silver_df (DataFrame): Dataset de voos já transformado (pré-join).\n",
    "        airlines_silver_df (DataFrame): Dataset transformado de companhias aéreas.\n",
    "        airports_silver_df (DataFrame): Dataset transformado de aeroportos.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Dataset consolidado no formato final da camada Silver.\n",
    "    \"\"\"\n",
    "\n",
    "    log.info(\"[Aggregate] Iniciando agregação dos datasets Silver.\")\n",
    "\n",
    "    # Detecta coluna de companhia aérea no dataset 'flights'\n",
    "    if \"airline_iata_code\" in flights_silver_df.columns:\n",
    "        airline_col = \"airline_iata_code\"\n",
    "    elif \"airline\" in flights_silver_df.columns:\n",
    "        airline_col = \"airline\"\n",
    "    else:\n",
    "        raise KeyError(\"Nenhuma coluna de companhia aérea encontrada no dataset flights.\")\n",
    "\n",
    "    # Detecta colunas de origem e destino no dataset 'flights'\n",
    "    if \"origin_airport_iata_code\" in flights_silver_df.columns:\n",
    "        origin_col = \"origin_airport_iata_code\"\n",
    "    elif \"origin_airport\" in flights_silver_df.columns:\n",
    "        origin_col = \"origin_airport\"\n",
    "    else:\n",
    "        raise KeyError(\"Coluna de aeroporto de origem não encontrada.\")\n",
    "\n",
    "    if \"dest_airport_iata_code\" in flights_silver_df.columns:\n",
    "        dest_col = \"dest_airport_iata_code\"\n",
    "    elif \"destination_airport\" in flights_silver_df.columns:\n",
    "        dest_col = \"destination_airport\"\n",
    "    else:\n",
    "        raise KeyError(\"Coluna de aeroporto de destino não encontrada.\")\n",
    "\n",
    "    # Join com airlines\n",
    "    df_joined = flights_silver_df.join(\n",
    "        airlines_silver_df,\n",
    "        flights_silver_df[airline_col] == airlines_silver_df[\"airline_iata_code\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Seleciona campos para aeroportos de origem\n",
    "    df_origin = (\n",
    "        airports_silver_df.select(\n",
    "            F.col(\"airport_iata_code\").alias(\"origin_airport_iata_code\"),\n",
    "            F.col(\"airport_name\").alias(\"origin_airport_name\"),\n",
    "            F.col(\"city\").alias(\"origin_city\"),\n",
    "            F.col(\"state\").alias(\"origin_state\"),\n",
    "            F.col(\"latitude\").alias(\"origin_latitude\"),\n",
    "            F.col(\"longitude\").alias(\"origin_longitude\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Seleciona campos para aeroportos de destino\n",
    "    df_dest = (\n",
    "        airports_silver_df.select(\n",
    "            F.col(\"airport_iata_code\").alias(\"dest_airport_iata_code\"),\n",
    "            F.col(\"airport_name\").alias(\"dest_airport_name\"),\n",
    "            F.col(\"city\").alias(\"dest_city\"),\n",
    "            F.col(\"state\").alias(\"dest_state\"),\n",
    "            F.col(\"latitude\").alias(\"dest_latitude\"),\n",
    "            F.col(\"longitude\").alias(\"dest_longitude\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join com aeroportos de origem e destino\n",
    "    df_joined = (\n",
    "        df_joined.join(\n",
    "            df_origin,\n",
    "            df_joined[origin_col] == F.col(\"origin_airport_iata_code\"),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .join(\n",
    "            df_dest,\n",
    "            df_joined[dest_col] == F.col(\"dest_airport_iata_code\"),\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Mapeamento dos tipos\n",
    "    schema_casts = {\n",
    "        \"flight_id\": \"bigint\",\n",
    "        \"flight_year\": \"smallint\",\n",
    "        \"flight_month\": \"smallint\",\n",
    "        \"flight_day\": \"smallint\",\n",
    "        \"flight_day_of_week\": \"smallint\",\n",
    "        \"flight_date\": \"date\",\n",
    "\n",
    "        \"airline_iata_code\": \"string\",\n",
    "        \"airline_name\": \"string\",\n",
    "\n",
    "        \"flight_number\": \"int\",\n",
    "        \"tail_number\": \"string\",\n",
    "\n",
    "        \"origin_airport_iata_code\": \"string\",\n",
    "        \"origin_airport_name\": \"string\",\n",
    "        \"origin_city\": \"string\",\n",
    "        \"origin_state\": \"string\",\n",
    "        \"origin_latitude\": \"double\",\n",
    "        \"origin_longitude\": \"double\",\n",
    "\n",
    "        \"dest_airport_iata_code\": \"string\",\n",
    "        \"dest_airport_name\": \"string\",\n",
    "        \"dest_city\": \"string\",\n",
    "        \"dest_state\": \"string\",\n",
    "        \"dest_latitude\": \"double\",\n",
    "        \"dest_longitude\": \"double\",\n",
    "\n",
    "        \"scheduled_departure\": \"timestamp\",\n",
    "        \"departure_time\": \"timestamp\",\n",
    "        \"scheduled_arrival\": \"timestamp\",\n",
    "        \"arrival_time\": \"timestamp\",\n",
    "        \"wheels_off\": \"timestamp\",\n",
    "        \"wheels_on\": \"timestamp\",\n",
    "\n",
    "        \"departure_delay\": \"double\",\n",
    "        \"arrival_delay\": \"double\",\n",
    "        \"taxi_out\": \"double\",\n",
    "        \"taxi_in\": \"double\",\n",
    "        \"air_time\": \"double\",\n",
    "        \"elapsed_time\": \"double\",\n",
    "        \"scheduled_time\": \"double\",\n",
    "        \"distance\": \"double\",\n",
    "\n",
    "        \"is_overnight_flight\": \"boolean\",\n",
    "\n",
    "        \"air_system_delay\": \"double\",\n",
    "        \"security_delay\": \"double\",\n",
    "        \"airline_delay\": \"double\",\n",
    "        \"late_aircraft_delay\": \"double\",\n",
    "        \"weather_delay\": \"double\",\n",
    "    }\n",
    "\n",
    "    for col_name, spark_type in schema_casts.items():\n",
    "        if col_name in df_joined.columns:\n",
    "            df_joined = df_joined.withColumn(col_name, F.col(col_name).cast(spark_type))\n",
    "\n",
    "    # Seleção final conforme o ddl\n",
    "    final_df = df_joined.select(\n",
    "        \"flight_year\",\n",
    "        \"flight_month\",\n",
    "        \"flight_day\",\n",
    "        \"flight_day_of_week\",\n",
    "        \"flight_date\",\n",
    "        \"airline_iata_code\",\n",
    "        \"airline_name\",\n",
    "        \"flight_number\",\n",
    "        \"tail_number\",\n",
    "        \"origin_airport_iata_code\",\n",
    "        \"origin_airport_name\",\n",
    "        \"origin_city\",\n",
    "        \"origin_state\",\n",
    "        \"origin_latitude\",\n",
    "        \"origin_longitude\",\n",
    "        \"dest_airport_iata_code\",\n",
    "        \"dest_airport_name\",\n",
    "        \"dest_city\",\n",
    "        \"dest_state\",\n",
    "        \"dest_latitude\",\n",
    "        \"dest_longitude\",\n",
    "        \"scheduled_departure\",\n",
    "        \"departure_time\",\n",
    "        \"scheduled_arrival\",\n",
    "        \"arrival_time\",\n",
    "        \"wheels_off\",\n",
    "        \"wheels_on\",\n",
    "        \"departure_delay\",\n",
    "        \"arrival_delay\",\n",
    "        \"taxi_out\",\n",
    "        \"taxi_in\",\n",
    "        \"air_time\",\n",
    "        \"elapsed_time\",\n",
    "        \"scheduled_time\",\n",
    "        \"distance\",\n",
    "        \"air_system_delay\",\n",
    "        \"security_delay\",\n",
    "        \"airline_delay\",\n",
    "        \"late_aircraft_delay\",\n",
    "        \"weather_delay\",\n",
    "    )\n",
    "\n",
    "    # Cria pk sequencial ordenada\n",
    "    window_spec = (\n",
    "        Window\n",
    "        .orderBy(\n",
    "            F.col(\"flight_date\").asc(),\n",
    "            F.col(\"airline_iata_code\").asc(),\n",
    "            F.col(\"flight_number\").asc(),\n",
    "            F.col(\"origin_airport_iata_code\").asc(),\n",
    "            F.col(\"departure_time\").asc(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Gera pk determinística e sequencial\n",
    "    final_df = final_df.withColumn(\n",
    "        \"flight_id\",\n",
    "        F.row_number().over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Reordena colunas para a pk ser a primeira\n",
    "    final_df = final_df.select(\n",
    "        \"flight_id\",\n",
    "        *[c for c in final_df.columns if c != \"flight_id\"]\n",
    "    )\n",
    "\n",
    "    log.info(\"[Aggregate] Agregação concluída com sucesso.\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a59fb-d37e-4798-8ee4-f05d0c2d2f74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `silver_aggregate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b3585-276d-48a9-b931-052d71574d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Aggregate] Iniciando job de agregação da camada Silver.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition: str = find_partition(\n",
    "        base_path=silver_path,\n",
    "        mode=run_mode,\n",
    "        date_str=run_date,\n",
    "    )\n",
    "    base_dir: Path = Path(silver_path) / source_partition / \"PARQUET\"\n",
    "\n",
    "    flights_path: Path  = base_dir / \"flights_pre_join.parquet\"\n",
    "    airlines_path: Path = base_dir / \"airlines.parquet\"\n",
    "    airports_path: Path = base_dir / \"airports.parquet\"\n",
    "\n",
    "    # Verifica existência dos arquivos necessários\n",
    "    for required_file in [flights_path, airlines_path, airports_path]:\n",
    "        if not required_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"[Refinement][Aggregate][ERROR] Arquivo esperado não encontrado: {required_file}\"\n",
    "            )\n",
    "\n",
    "    # Leitura dos datasets\n",
    "    log.info(\"[Aggregate] Lendo datasets silver (flights, airlines, airports).\")\n",
    "\n",
    "    df_flights  = spark.read.parquet(str(flights_path))\n",
    "    df_airlines = spark.read.parquet(str(airlines_path))\n",
    "    df_airports = spark.read.parquet(str(airports_path))\n",
    "\n",
    "    # Construção do DataFrame agregado\n",
    "    aggregated_df: DataFrame = create_aggregated_flights_df(\n",
    "        flights_silver_df=df_flights,\n",
    "        airlines_silver_df=df_airlines,\n",
    "        airports_silver_df=df_airports,\n",
    "    )\n",
    "\n",
    "    # Filtra registros inválidos de aeroportos\n",
    "    aggregated_df = aggregated_df.filter(\n",
    "        F.col(\"origin_airport_iata_code\").isNotNull()\n",
    "        & F.col(\"dest_airport_iata_code\").isNotNull()\n",
    "    )\n",
    "\n",
    "    # Quality gates\n",
    "    run_quality_gates_silver_aggregated(aggregated_df)\n",
    "\n",
    "    # Escrita do arquivo final\n",
    "    output_path: Path = base_dir / \"flights_aggregated.parquet\"\n",
    "    aggregated_df.coalesce(1).write.mode(\"overwrite\").parquet(str(output_path))\n",
    "\n",
    "    log.info(f\"[Aggregate] Dataset agregado salvo em: {output_path}\")\n",
    "\n",
    "    # Libera cache após uso\n",
    "    aggregated_df.unpersist()\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Aggregate][ERROR] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[Aggregate] Job de agregação encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30cf775f-331b-4cf7-9e78-620cf56097eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "aggregated_df.printSchema()\n",
    "\n",
    "aggregated_df.limit(5).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4beb8-6e6b-4620-ac6e-30612137803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Aggregate] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b167fbc-1606-4441-9878-328c373d6032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 5: silver_load\n",
    "\n",
    "Este job realiza a carga do dataset agregado da camada **Silver** (`flights_aggregated.parquet`) para o PostgreSQL, populando a tabela `silver.silver_flights` conforme o ddl da camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e0ab9-fa80-492b-a841-f561b1c95d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"silver_load\")\n",
    "\n",
    "spark = get_spark_session(\"SilverLoad\")\n",
    "log.info(\"[SilverLoad] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb45d99-ebd7-4917-808f-c8db6b3f00ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `silver_load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e9ac4-740a-451b-a703-b63490d25be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[SilverLoad] Iniciando execução do job de carga.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    partition = find_partition(\n",
    "        base_path=silver_path,\n",
    "        mode=run_mode,\n",
    "        date_str=run_date,\n",
    "    )\n",
    "\n",
    "    base_dir = Path(silver_path) / partition / \"PARQUET\"\n",
    "    parquet_path = base_dir / \"flights_aggregated.parquet\"\n",
    "\n",
    "    if not parquet_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[SilverLoad][ERROR] Arquivo não encontrado: {parquet_path}.\"\n",
    "        )\n",
    "\n",
    "    log.info(f\"[SilverLoad] Lendo dataset agregado: {parquet_path}.\")\n",
    "    df = spark.read.parquet(str(parquet_path))\n",
    "\n",
    "    log.info(f\"[SilverLoad] Tuplas encontradas: {df.count():,}.\")\n",
    "\n",
    "    # Carga para PostgreSQL\n",
    "    try:\n",
    "        log.info(\"[SilverLoad] Inserindo dados em silver.silver_flights.\")\n",
    "\n",
    "        load_to_postgres(\n",
    "            df=df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"silver.silver_flights\",\n",
    "        )\n",
    "\n",
    "        log.info(\"[SilverLoad] Validando tuplas da tabela após a carga.\")\n",
    "        \n",
    "        # Valida carga\n",
    "        expected_count = df.count()\n",
    "        assert_table_rowcount(\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"silver.silver_flights\",\n",
    "            expected_count=expected_count,\n",
    "        )\n",
    "\n",
    "        log.info(\"[SilverLoad] Validação pós-carga encerrada.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[Refinement][SilverLoad][ERROR] Erro durante carga no PostgreSQL: {e}\")\n",
    "        raise\n",
    "\n",
    "    log.info(\"[SilverLoad] Carga concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[SilverLoad][ERROR] Falha na execução: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[SilverLoad] Job de carga encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c32da8a-caef-45be-92f6-cce5dbfaa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.limit(2).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70a5e5-78ce-4411-8d0c-89a480f4a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[SilverLoad] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be14c8-f036-4a3b-8a9f-990ed238dd20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Jog 6: silver_cleanup\n",
    "\n",
    "Este job remove arquivos intermediários da camada **Silver**, mantendo apenas o arquivo final `flights_aggregated.parquet` na partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f7105-b9e8-4711-a6fa-5356fd0a79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"silver_cleanup\")\n",
    "\n",
    "spark = get_spark_session(\"SilverCleanupJob\")\n",
    "log.info(\"[SilverCleanup] Sessão Spark iniciada.\")\n",
    "\n",
    "# Ajustes performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a1660-fb5b-44ae-aeb7-f240533853af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `silver_cleanup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edb61d-eb10-4450-9210-07b40dc9b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[SilverCleanup] Iniciando job de limpeza da camada silver.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    partition = find_partition(\n",
    "        base_path=silver_path,\n",
    "        mode=run_mode,\n",
    "        date_str=run_date,\n",
    "    )\n",
    "    partition_dir = Path(silver_path) / partition / \"PARQUET\"\n",
    "\n",
    "    if not partition_dir.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[Refinement][SilverCleanup][ERROR] Diretório de partição não encontrado: {partition_dir}.\"\n",
    "        )\n",
    "\n",
    "    # Apenas este diretório deve permanecer\n",
    "    keep = {\"flights_aggregated.parquet\"}\n",
    "\n",
    "    log.info(f\"[SilverCleanup] Diretório alvo: {partition_dir}.\")\n",
    "    log.info(f\"[SilverCleanup] Mantendo apenas: {keep}.\")\n",
    "\n",
    "    for item in partition_dir.iterdir():\n",
    "        # Se for o agregado, não mexe\n",
    "        if item.name in keep:\n",
    "            log.info(f\"[SilverCleanup] Mantendo: {item}\")\n",
    "            continue\n",
    "\n",
    "        # Remove diretórios intermediários\n",
    "        if item.is_dir():\n",
    "            try:\n",
    "                shutil.rmtree(item)\n",
    "                log.info(f\"[SilverCleanup] Diretório removido: {item}.\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[SilverCleanup] Falha ao remover diretório {item}: {e}.\")\n",
    "            continue\n",
    "\n",
    "        # Remove arquivos soltos\n",
    "        if item.is_file():\n",
    "            try:\n",
    "                item.unlink()\n",
    "                log.info(f\"[SilverCleanup] Arquivo removido: {item}.\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[SilverCleanup] Falha ao remover arquivo {item}: {e}.\")\n",
    "\n",
    "    log.info(\"[SilverCleanup] Limpeza concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[SilverCleanup][ERROR] Falha durante o cleanup: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[SilverCleanup] Job de limpeza da camada silver encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0b89aef-035e-4a0f-823f-f7b959694353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "partition = find_partition(silver_path, mode=run_mode, date_str=run_date)\n",
    "for item in (Path(silver_path) / partition / \"PARQUET\").iterdir():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106554bc-5ef2-47c3-a439-037568aceea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[SilverCleanup] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e8614-a99a-4a03-a507-a37c35eb8bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
