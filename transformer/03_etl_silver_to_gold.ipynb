{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d86d33-181e-40d2-a0fe-90c4548220af",
   "metadata": {},
   "source": [
    "# 03_etl_silver_to_gold\n",
    "---\n",
    "Este notebook executa o processo `ETL` que transfere os dados da camada **Silver** para a **Gold**, englobando normalização, movimentação dos arquivos e carga dos dados no **PostgreSQL**, dando finalidade ao pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ebf692-724f-4183-9db8-f827aa80f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "silver_path = \"/opt/airflow/data-layer/silver\"\n",
    "gold_path = \"/opt/airflow/data-layer/gold\"\n",
    "\n",
    "aggregated_name = \"flights_aggregated.parquet\"\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90039f5-ae72-4d8b-97e4-0ba25c56ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from transformer.utils.file_io import find_partition, delete_files\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres\n",
    "from transformer.utils.postgre_helpers import assert_table_rowcount\n",
    "from transformer.utils.quality_gates_gold import run_quality_gates_gold\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630fdbd-bc0c-46dc-be32-65246714750b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 1: move_file_silver_to_gold\n",
    "\n",
    "Este job realiza a copia o dataset agregado `flights_aggregated.parquet` da camada **Silver** para a **Gold**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566ee99-11fc-4154-82ec-05d6c027809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"move_file_silver_to_gold\")\n",
    "\n",
    "spark = get_spark_session(\"MoveFileSilverToGold\")\n",
    "log.info(\"[MoveFileGold] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a12d99-de99-4546-a2a5-352a3de9231c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `move_file_silver_to_gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f17fb6-6cb6-4b5e-b971-68497748bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[MoveFileGold] Iniciando job de movimentação.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(silver_path, mode=run_mode, date_str=run_date)\n",
    "    source_dir = Path(silver_path) / source_partition / \"PARQUET\" / aggregated_name\n",
    "\n",
    "    if not source_dir.exists():\n",
    "        raise FileNotFoundError(f\"[MoveFileGold][ERROR] Arquivo agregado não encontrado: {source_dir}.\")\n",
    "\n",
    "    # Define data de processamento e caminho destino\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    dest_dir = Path(gold_path) / processing_date / \"PARQUET\" / aggregated_name\n",
    "    dest_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log.info(f\"[MoveFileGold] Movendo '{source_dir}' -> '{dest_dir}'.\")\n",
    "\n",
    "    # Cópia do arquivo\n",
    "    shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)\n",
    "    log.info(\"[MoveFileGold] Cópia concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[MoveFileGold][ERROR] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[MoveFileGold] Job de movimentação encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37415680-3303-4ca2-946c-c0e3c74f0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "for item in (Path(gold_path) / partition / \"PARQUET\").iterdir():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a4416-4f5e-4f01-9440-990d219f0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerrando a sessão do Spark.\n",
    "spark.stop()\n",
    "log.info(\"[MoveFileGold] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdbbf0-e811-40f5-8a53-c08b22d520e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 2: build_gold_star_schema\n",
    "\n",
    "Este job realiza a construção do esquema estrela da camada **Gold**, materializando as tabelas dimensionais e fato a partir do dataset `flights_aggregated.parquet`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97709fef-fbb7-492c-9ebb-01d905ec42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 01:41:50 [INFO] build_gold_star_schema | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-17 01:41:51 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'BuildGoldStarSchema' (master=local[*]).\n",
      "2025-11-17 01:41:51 [INFO] build_gold_star_schema | [Materialize] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"build_gold_star_schema\")\n",
    "\n",
    "spark = get_spark_session(\"BuildGoldStarSchema\")\n",
    "log.info(\"[Materialize] SparkSession iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228db41-f5f1-4fe7-87f6-60f1faacfadf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de materialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e3b960-7ef3-4f87-a5bb-62ffed79ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def materialize_gold_layer(df):\n",
    "    \"\"\"\n",
    "    Gera as tabelas dimensionais e fato da camada gold a partir do dataset agregado.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame consolidado da camada gold.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, DataFrame]: DataFrames correspondentes a dim_airline, dim_airport, dim_date e fato_flights.\n",
    "    \"\"\"\n",
    "    # Feriados federais nos EUA em 2015 (UTC)\n",
    "    us_holidays_2015 = [\n",
    "        \"2015-01-01\",\n",
    "        \"2015-01-19\",\n",
    "        \"2015-02-16\",\n",
    "        \"2015-05-25\",\n",
    "        \"2015-07-04\",\n",
    "        \"2015-09-07\",\n",
    "        \"2015-10-12\",\n",
    "        \"2015-11-11\",\n",
    "        \"2015-11-26\",\n",
    "        \"2015-12-25\",\n",
    "    ]\n",
    "    holidays_df = (\n",
    "        spark.createDataFrame([(d,) for d in us_holidays_2015], [\"holiday_date\"])\n",
    "            .withColumn(\"holiday_date\", F.col(\"holiday_date\").cast(DateType()))\n",
    "    )\n",
    "    \n",
    "    log.info(\"[Materialize] Iniciando materialização da camada gold.\")\n",
    "\n",
    "    # dim_airline\n",
    "    log.info(\"[Materialize] Materializando 'dim_airline'.\")\n",
    "    dim_airline = (\n",
    "        df.select(\"airline_iata_code\", \"airline_name\")\n",
    "            .distinct()\n",
    "            .withColumn(\"airline_id\", F.monotonically_increasing_id())\n",
    "            .select(\"airline_id\", \"airline_iata_code\", \"airline_name\")\n",
    "    )\n",
    "\n",
    "    # dim_airport\n",
    "    log.info(\"[Materialize] Materializando 'dim_airport'.\")\n",
    "    dim_airport = (\n",
    "        df.select(\n",
    "            F.col(\"origin_airport_iata_code\").alias(\"airport_iata_code\"),\n",
    "            F.col(\"origin_airport_name\").alias(\"airport_name\"),\n",
    "            F.col(\"origin_city\").alias(\"city_name\"),\n",
    "            F.col(\"origin_state\").alias(\"state_code\"),\n",
    "            F.col(\"origin_latitude\").alias(\"latitude\"),\n",
    "            F.col(\"origin_longitude\").alias(\"longitude\")\n",
    "        )\n",
    "            .union(\n",
    "                df.select(\n",
    "                    F.col(\"dest_airport_iata_code\").alias(\"airport_iata_code\"),\n",
    "                    F.col(\"dest_airport_name\").alias(\"airport_name\"),\n",
    "                    F.col(\"dest_city\").alias(\"city_name\"),\n",
    "                    F.col(\"dest_state\").alias(\"state_code\"),\n",
    "                    F.col(\"dest_latitude\").alias(\"latitude\"),\n",
    "                    F.col(\"dest_longitude\").alias(\"longitude\")\n",
    "                )\n",
    "            )\n",
    "            .distinct()\n",
    "            .withColumn(\"airport_id\", F.monotonically_increasing_id())\n",
    "            .select(\"airport_id\", \"airport_iata_code\", \"airport_name\", \"city_name\",\n",
    "                    \"state_code\", \"latitude\", \"longitude\")\n",
    "    )\n",
    "\n",
    "    # dim_date\n",
    "    log.info(\"[Materialize] Materializando 'dim_date'.\")\n",
    "    dim_date = (\n",
    "        df.select(F.col(\"flight_date\").alias(\"full_date\"))\n",
    "            .distinct()\n",
    "            .withColumn(\"year\", F.year(\"full_date\"))\n",
    "            .withColumn(\"month\", F.month(\"full_date\"))\n",
    "            .withColumn(\"day\", F.dayofmonth(\"full_date\"))\n",
    "            .withColumn(\"day_of_week\", F.dayofweek(\"full_date\"))\n",
    "            .withColumn(\"quarter\", F.quarter(\"full_date\"))\n",
    "            # Augmentation de feriado\n",
    "            .join(holidays_df, F.col(\"full_date\") == F.col(\"holiday_date\"), \"left\")\n",
    "            .withColumn(\"is_holiday\", F.when(F.col(\"holiday_date\").isNotNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "            .drop(\"holiday_date\")\n",
    "            # Fim do augmentation de feriado\n",
    "            .select(\"full_date\", \"year\", \"month\", \"day\", \"day_of_week\", \"quarter\", \"is_holiday\")\n",
    "    )\n",
    "\n",
    "    # fato_flights\n",
    "    log.info(\"[Materialize] Materializando 'fato_flights'.\")\n",
    "    fato_flights = (\n",
    "        df.withColumn(\"flight_id\", F.monotonically_increasing_id())\n",
    "            .withColumnRenamed(\"flight_date\", \"full_date\")\n",
    "            .select(\n",
    "                \"flight_id\", \"full_date\", \"airline_iata_code\",\n",
    "                \"origin_airport_iata_code\", \"dest_airport_iata_code\",\n",
    "                \"distance\",\n",
    "                \"air_time\", \"elapsed_time\", \"scheduled_time\",\n",
    "                \"taxi_out\", \"taxi_in\",\n",
    "                \"departure_delay\", \"arrival_delay\", \"air_system_delay\", \n",
    "                \"security_delay\", \"airline_delay\", \"late_aircraft_delay\", \"weather_delay\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Joins para adicionar fk's\n",
    "    log.info(\"[Materialize] Adicionando fk's na 'fato_flights'.\")\n",
    "    fato_flights = (\n",
    "        fato_flights\n",
    "            .join(dim_airline, on=\"airline_iata_code\", how=\"left\")\n",
    "            .join(dim_airport.select(\n",
    "                F.col(\"airport_id\").alias(\"origin_airport_id\"),\n",
    "                F.col(\"airport_iata_code\").alias(\"origin_airport_iata_code\")\n",
    "            ), on=\"origin_airport_iata_code\", how=\"left\")\n",
    "            .join(dim_airport.select(\n",
    "                F.col(\"airport_id\").alias(\"dest_airport_id\"),\n",
    "                F.col(\"airport_iata_code\").alias(\"dest_airport_iata_code\")\n",
    "            ), on=\"dest_airport_iata_code\", how=\"left\")\n",
    "            .select(\n",
    "                \"flight_id\", \"full_date\", \"airline_id\", \"origin_airport_id\", \"dest_airport_id\",\n",
    "                \"distance\", \"air_time\", \"elapsed_time\", \"scheduled_time\", \"taxi_out\",\n",
    "                \"taxi_in\", \"departure_delay\", \"arrival_delay\",\n",
    "                \"air_system_delay\", \"security_delay\", \"airline_delay\",\n",
    "                \"late_aircraft_delay\", \"weather_delay\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    log.info(\"[Materialize] Materialização concluída.\")\n",
    "    \n",
    "    return {\n",
    "        \"dim_airline\": dim_airline,\n",
    "        \"dim_airport\": dim_airport,\n",
    "        \"dim_date\": dim_date,\n",
    "        \"fato_flights\": fato_flights\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b51023-792b-4e73-908f-dcd02c7cac2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `build_gold_star_schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf860a-e188-4e9d-8f11-b6875777dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Materialize] Iniciando job de materialização da gold.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "    source_dir = Path(gold_path) / source_partition / \"PARQUET\" / aggregated_name\n",
    "\n",
    "    if not source_dir.exists():\n",
    "        raise FileNotFoundError(f\"[Materialize][ERROR] Arquivo agregado não encontrado: {source_dir}\")\n",
    "\n",
    "    # Lê dataset agregado\n",
    "    df = spark.read.parquet(str(source_dir))\n",
    "    log.info(f\"[Materialize] Dataset carregado de {source_dir}\")\n",
    "\n",
    "    # Realiza a materialização\n",
    "    tables = materialize_gold_layer(df)\n",
    "    dim_airline = tables[\"dim_airline\"]\n",
    "    dim_airport = tables[\"dim_airport\"]\n",
    "    dim_date = tables[\"dim_date\"]\n",
    "    fato_flights = tables[\"fato_flights\"]\n",
    "\n",
    "    # Executa quality gate\n",
    "    log.info(\"[Materialize] Iniciando quality gate.\")\n",
    "    run_quality_gates_gold(\n",
    "        dim_airline=dim_airline,\n",
    "        dim_airport=dim_airport,\n",
    "        dim_date=dim_date,\n",
    "        fato_flights=fato_flights\n",
    "    )\n",
    "    log.info(\"[Materialize] Quality gate concluído com sucesso.\")\n",
    "\n",
    "    # Define partição de saída\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    output_dir = Path(gold_path) / processing_date / \"PARQUET\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Escreve os arquivos na gold\n",
    "    log.info(\"[Materialize] Iniciando escrita dos arquivos na camada gold.\")\n",
    "\n",
    "    dim_airline.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_airline.parquet\"))\n",
    "    dim_airport.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_airport.parquet\"))\n",
    "    dim_date.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_date.parquet\"))\n",
    "    fato_flights.write.mode(\"overwrite\").parquet(str(output_dir / \"fato_flights.parquet\"))\n",
    "\n",
    "    log.info(\"[Materialize] Escrita concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Materialize][ERROR] Falha na construção do esquema estrela: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[Materialize] Job de materialização da gold encerrado.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be8a5123-d9eb-4353-84ac-3fedeed12803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df_show = {\n",
    "    \"dim_airline\": dim_airline,\n",
    "    \"dim_airport\": dim_airport,\n",
    "    \"dim_date\": dim_date,\n",
    "    \"fato_flights\": fato_flights\n",
    "}\n",
    "\n",
    "for name, d in df_show.items():\n",
    "    print(f\"\\n{name}\\n\")\n",
    "    d.printSchema()\n",
    "    d.limit(1).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c38424-aa86-46e4-8f6e-65e0e532c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 01:44:44 [INFO] build_gold_star_schema | [Materialize] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerrando a sessão do Spark.\n",
    "spark.stop()\n",
    "log.info(\"[Materialize] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df256730-fa0c-43e4-9f96-81dcd02301de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 3: load_gold_to_postgres_job\n",
    "\n",
    "Este job executa a carga das tabelas da camada **Gold** no PostgreSQL (schema `gold`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ccfc7-47c0-4ca2-bfcd-468994883a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"load_gold_to_postgres\")\n",
    "\n",
    "spark = get_spark_session(\"LoadGoldToPostgres\")\n",
    "log.info(\"[LoadGold] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919fad9-42f5-423c-8573-66f406834748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `load_gold_to_postgres_job`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbcdf9-a110-4fb1-bee6-35574a986429",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[LoadGold] Iniciando job de carga da gold.\")\n",
    "    \n",
    "    # Resolve partição e caminhos\n",
    "    partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "    partition_dir = Path(gold_path) / partition / \"PARQUET\"\n",
    "\n",
    "    if not partition_dir.exists():\n",
    "        raise FileNotFoundError(f\"[LoadGold][ERROR] Diretório de partição não encontrado: {partition_dir}\")\n",
    "\n",
    "    log.info(f\"[LoadGold] Partição selecionada: {partition_dir}.\")\n",
    "\n",
    "    # Lê os parquets em memória\n",
    "    tables = {\n",
    "        \"dim_airline\": spark.read.parquet(str(partition_dir / \"dim_airline.parquet\")),\n",
    "        \"dim_airport\": spark.read.parquet(str(partition_dir / \"dim_airport.parquet\")),\n",
    "        \"dim_date\": spark.read.parquet(str(partition_dir / \"dim_date.parquet\")),\n",
    "        \"fato_flights\": spark.read.parquet(str(partition_dir / \"fato_flights.parquet\")),\n",
    "    }\n",
    "\n",
    "    # Carga no PostgreSQL e validação\n",
    "    for table_name, df in tables.items():\n",
    "        full_table_name = f\"gold.{table_name}\"\n",
    "\n",
    "        log.info(f\"[LoadGold] Carregando tabela: {full_table_name}.\")\n",
    "        expected_count = df.count()\n",
    "\n",
    "        # Carga no PostgreSQL\n",
    "        load_to_postgres(\n",
    "            df=df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=full_table_name,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "\n",
    "        log.info(f\"[LoadGold] Tabela '{full_table_name}' carregada. Validando integridade.\")\n",
    "\n",
    "        # Validação (fallback se falhar)\n",
    "        try:\n",
    "            assert_table_rowcount(\n",
    "                db_conn_id=postgres_conn_id,\n",
    "                table_name=full_table_name,\n",
    "                expected_count=expected_count,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log.error(f\"[LoadGold][ERROR] Validação falhou para '{full_table_name}'. Limpando tabela.\")\n",
    "\n",
    "            import psycopg2\n",
    "\n",
    "            with psycopg2.connect(\n",
    "                host=os.getenv(\"DB_HOST\", \"localhost\"),\n",
    "                dbname=os.getenv(\"DB_NAME\", \"postgres\"),\n",
    "                user=os.getenv(\"DB_USER\", \"postgres\"),\n",
    "                password=os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "            ) as conn_pg:\n",
    "                with conn_pg.cursor() as cur:\n",
    "                    cur.execute(f\"TRUNCATE TABLE {full_table_name} CASCADE;\")\n",
    "                    conn_pg.commit()\n",
    "\n",
    "            raise ValueError(f\"[LoadGold][ERROR] Falha na validação da tabela '{full_table_name}'.\") from e\n",
    "\n",
    "        log.info(f\"[LoadGold] Validação concluída com sucesso: {full_table_name}.\")\n",
    "\n",
    "    log.info(\"[LoadGold] Carga de todas as tabelas concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[LoadGold][ERROR] Falha durante carga no PostgreSQL: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[LoadGold] Job de carga da gold encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01f54fb0-8f2a-44d7-b5b6-617161da413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " gold.dim_airline \n",
      "\n",
      "+----------+-----------------+--------------------+\n",
      "|airline_id|airline_iata_code|        airline_name|\n",
      "+----------+-----------------+--------------------+\n",
      "|         0|               EV|Atlantic Southeas...|\n",
      "+----------+-----------------+--------------------+\n",
      "\n",
      "\n",
      " gold.dim_airport \n",
      "\n",
      "+----------+-----------------+--------------------+----------+----------+-----------+--------+----------+\n",
      "|airport_id|airport_iata_code|        airport_name|state_code|state_name|  city_name|latitude| longitude|\n",
      "+----------+-----------------+--------------------+----------+----------+-----------+--------+----------+\n",
      "|         0|              LAX|Los Angeles Inter...|        CA|      NULL|Los Angeles|33.94254|-118.40807|\n",
      "+----------+-----------------+--------------------+----------+----------+-----------+--------+----------+\n",
      "\n",
      "\n",
      " gold.dim_date \n",
      "\n",
      "+----------+----+-----+---+-----------+-------+----------+\n",
      "| full_date|year|month|day|day_of_week|quarter|is_holiday|\n",
      "+----------+----+-----+---+-----------+-------+----------+\n",
      "|2015-06-12|2015|    6| 12|          6|      2|     false|\n",
      "+----------+----+-----+---+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "# Vareficação manual da carga das tabelas\n",
    "import os\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('DB_HOST', 'localhost')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME', 'postgres')}\"\n",
    "connection_properties = {\n",
    "    \"user\": os.getenv(\"DB_USER\", \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "tables_to_check = [\"dim_airline\", \"dim_airport\", \"dim_date\"]\n",
    "for tbl in tables_to_check:\n",
    "    print(f\"\\n gold.{tbl} \\n\")\n",
    "    df_check = spark.read.jdbc(url=jdbc_url, table=f\"gold.{tbl}\", properties=connection_properties)\n",
    "    df_check.limit(1).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96d653a0-a52a-422c-bbb8-42383a7aa43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 02:35:04 [INFO] load_gold_to_postgres | [LoadGold] Sessão Spark finalizada.\n",
      "INFO:load_gold_to_postgres:[LoadGold] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerrando a sessão do Spark.\n",
    "spark.stop()\n",
    "log.info(\"[LoadGold] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864960bd-cb3b-4963-b8dd-7dc708cf50e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 4: cleanup_gold\n",
    "\n",
    "Este job realiza a limpeza dos arquivos temporários presentes na camada gold, removendo o dataset intermediário `flights_aggregated.parquet` gerado na transição, mantendo a organização da camada **Gold**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe92ad96-71e5-4772-a887-a8fb433f8c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 02:35:56 [INFO] cleanup_gold | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ba35de13-648a-466e-a1a2-ec0688084796;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 152ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ba35de13-648a-466e-a1a2-ec0688084796\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/11/17 02:35:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-11-17 02:35:59 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'GoldCleanup' (master=local[*]).\n",
      "2025-11-17 02:35:59 [INFO] cleanup_gold | [GoldCleanup] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"cleanup_gold\")\n",
    "\n",
    "spark = get_spark_session(\"GoldCleanup\")\n",
    "log.info(\"[GoldCleanup] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437c95e-1fbe-4d18-b357-7134ca2f2d65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `cleanup_gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6716e53c-337b-4d9c-9263-a334e0327df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 02:39:57 [INFO] cleanup_gold | [GoldCleanup] Iniciando job de limpeza da gold.\n",
      "2025-11-17 02:39:57 [INFO] file_io | [INFO] Partição selecionada: 2025-11-17\n",
      "2025-11-17 02:39:57 [INFO] cleanup_gold | [GoldCleanup] Removendo arquivo: /opt/airflow/data-layer/gold/2025-11-17/PARQUET/flights_aggregated.parquet.\n",
      "2025-11-17 02:39:57 [INFO] file_io | [INFO] Deletando 1 arquivo(s).\n",
      "2025-11-17 02:39:57 [INFO] file_io | [INFO] '/opt/airflow/data-layer/gold/2025-11-17/PARQUET/flights_aggregated.parquet' deletado com sucesso.\n",
      "2025-11-17 02:39:57 [INFO] file_io | [INFO] Deleção concluída.\n",
      "2025-11-17 02:39:57 [INFO] cleanup_gold | [GoldCleanup] Limpeza concluída com sucesso.\n",
      "2025-11-17 02:39:57 [INFO] cleanup_gold | [GoldCleanup] Job de limpeza da gold encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[GoldCleanup] Iniciando job de limpeza da gold.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "    partition_dir = Path(gold_path) / partition / \"PARQUET\"\n",
    "    target_file = partition_dir / \"flights_aggregated.parquet\"\n",
    "\n",
    "    # Verifica se o arquivo temporário existe e limpando\n",
    "    if not target_file.exists():\n",
    "        log.warning(f\"[GoldCleanup][WARN] Arquivo temporário não encontrado: {target_file}.\")\n",
    "    else:\n",
    "        log.info(f\"[GoldCleanup] Removendo arquivo: {target_file}.\")\n",
    "        delete_files(spark, [str(target_file)])\n",
    "\n",
    "    log.info(\"[GoldCleanup] Limpeza concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Serving][Cleanup][ERROR] Falha durante limpeza da camada gold: {e}.\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[GoldCleanup] Job de limpeza da gold encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7bc34b5-4ac0-4889-adb8-d3f22f06c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 02:40:07 [INFO] file_io | [INFO] Partição selecionada: 2025-11-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/airflow/data-layer/gold/2025-11-17/PARQUET/dim_airline.parquet\n",
      "/opt/airflow/data-layer/gold/2025-11-17/PARQUET/dim_airport.parquet\n",
      "/opt/airflow/data-layer/gold/2025-11-17/PARQUET/dim_date.parquet\n",
      "/opt/airflow/data-layer/gold/2025-11-17/PARQUET/fato_flights.parquet\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "for item in (Path(gold_path) / partition / \"PARQUET\").iterdir():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3436ad6a-265b-48aa-82b2-87bdea51eb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 02:40:14 [INFO] cleanup_gold | [GoldCleanup] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "log.info(\"[GoldCleanup] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b070f4-6652-4d0c-96cf-5cb8f77ac23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
