{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac440cbb-1ee8-4805-acd8-b3a2d2cd9d8c",
   "metadata": {},
   "source": [
    "# etl_raw_to_silver\n",
    "---\n",
    "Este notebook executa o processo `ETL` que transfere os dados da camada **Raw** para a **Silver**, englobando transformações, limpeza, padronização, movimentação de arquivos e materialização dos resultados tanto em arquivos `parquet` na camada **Silver** quanto em tabelas correspondentes no *PostgreSQL*. Além disso, ele realiza a carga parcial dos dados da **Raw** para inicializar a pipeline com *dbt*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85066341-bac4-4531-9b10-612b34ccc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "stage_path = \"/opt/airflow/data-layer/raw/stage\"\n",
    "raw_path = \"/opt/airflow/data-layer/raw\"\n",
    "silver_path = \"/opt/airflow/data-layer/silver\"\n",
    "\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c6b19d-4dbc-45d4-8fa8-7f5ef9b3a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import DataFrame, functions as F, Window, SparkSession\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "from transformer.utils.file_io import check_files_in_folder, delete_files, find_partition, move_files\n",
    "from transformer.utils.helpers import to_date_from_ymd\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres\n",
    "from transformer.utils.postgre_helpers import assert_table_rowcount\n",
    "from transformer.utils.quality_gates_raw import run_quality_gates_raw\n",
    "from transformer.utils.quality_gates_silver_aggregated import run_quality_gates_silver_aggregated\n",
    "from transformer.utils.quality_gates_silver_base import run_quality_gates_silver_base\n",
    "from transformer.utils.quality_gates_silver_flights import run_quality_gates_silver_flights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525e30b-931b-4e5f-9270-64039e6bfa74",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40c216-9900-4550-9e07-cbdcd24d2718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 0: kaggle_download_and_prepare\n",
    "\n",
    "Este job executa o download, a descompactação e a preparação dos arquivos na pasta **Stage**. Sua execução permanece desativada porque depende de configurações manuais de acesso à API do Kaggle. Reative as células e informe suas credenciais caso deseje executá-la.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107291e8-7291-4a6f-b2a6-5830cd5798ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "log = get_logger(\"kaggle_and_prepare\")\n",
    "\n",
    "NUM_CHUNKS=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bce60d-e98b-4273-92e0-f2e9523e9544",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo funções de download e de divisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8beccf3-19da-4b36-8558-5eb925ecc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "def download_kaggle_dataset(dataset_name: str, dest_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Faz o download e a extração de um dataset do Kaggle, caso ainda não exista localmente.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Nome do dataset no Kaggle (ex: 'usdot/flight-delays').\n",
    "        dest_path (Path): Caminho destino para os arquivos extraídos.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Para erros de autenticação ou rede.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing_files = os.listdir(dest_path)\n",
    "        has_csv_files = any(file.endswith(\".csv\") for file in existing_files)\n",
    "\n",
    "        if has_csv_files:\n",
    "            log.info(f\"[Kaggle] Arquivos já existem em '{dest_path}'. Pulando download.\")\n",
    "            return\n",
    "\n",
    "        log.info(f\"[Kaggle] Nenhum arquivo CSV encontrado. Baixando o dataset '{dataset_name}'.\")\n",
    "        kaggle.api.dataset_download_files(dataset_name, path=dest_path, unzip=True)\n",
    "        log.info(\"[Kaggle] Download e extração concluídos com sucesso.\")\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[Kaggle] Ocorreu um erro! Verifique se a API do Kaggle está configurada: {e}.\")\n",
    "        log.exception(f\"[Kaggle] Erro: {e}.\")\n",
    "        raise\n",
    "\n",
    "def split_main_file(source_file: Path, num_chunks: int = NUM_CHUNKS) -> None:\n",
    "    \"\"\"\n",
    "    Divide o arquivo flights.csv em múltiplos chunks de tamanho aproximadamente igual.\n",
    "\n",
    "    Args:\n",
    "        source_file (Path): Caminho do arquivo csv principal.\n",
    "        num_chunks (int): Número de partes a gerar.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Se o arquivo não existir.\n",
    "        ValueError: Se o arquivo estiver vazio.\n",
    "        Exception: Para erros inesperados durante a divisão.\n",
    "    \"\"\"\n",
    "    if not source_file.exists():\n",
    "        raise FileNotFoundError(f\"[Landing] Arquivo não encontrado: '{source_file}'.\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(source_file)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"O arquivo '{source_file}' está vazio.\")\n",
    "\n",
    "        log.info(f\"[Split] Dividindo '{source_file.name}' em {num_chunks} partes aproximadamente iguais.\")\n",
    "\n",
    "        chunks = np.array_split(df, num_chunks)\n",
    "\n",
    "        for i, chunk_df in enumerate(chunks, start=1):\n",
    "            chunk_name = f\"{source_file.stem}_part_{i:02d}{source_file.suffix}\"\n",
    "            chunk_path = source_file.parent / chunk_name\n",
    "            chunk_df.to_csv(chunk_path, index=False)\n",
    "            log.info(f\"[Split] Chunk salvo: '{chunk_name}', {len(chunk_df)} linhas.\")\n",
    "\n",
    "        log.info(\"[Split] Divisão do arquivo concluída com sucesso.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"[Split] Erro ao dividir o arquivo: {e}.\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f8b0d-c42a-4e46-99e9-5c14fb4d10ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `kaggle_download_and_prepare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337877b-2031-4a0f-ab1a-7b628fea44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "try:\n",
    "    # Download\n",
    "    download_kaggle_dataset(DATASET_NAME, stage_path)\n",
    "\n",
    "    # Divisão\n",
    "    flights_file = Path(stage_path) / \"flights.csv\"\n",
    "    if flights_file.exists():\n",
    "        split_main_file(flights_file, num_chunks=NUM_CHUNKS)\n",
    "    else:\n",
    "        log.warning(f\"[Kaggle] Arquivo 'flights.csv' não encontrado. Verifique a pasta.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"[Kaggle] Falha no processo de preparação: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72a42c-62bd-47fb-8f9b-48fac534b9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1485e6aa-2ed2-44ff-b4eb-6d593b773c94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 1: unify_flight_chunks\n",
    "\n",
    "Este job realiza a unificação dos arquivos `flights_part_*.csv` presentes na pasta **Stage**, consolidando-os em um único arquivo Parquet `flights.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23e40e9-cc70-44ca-b91a-cd16e44b2071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4055d71-0f99-4a25-8293-e2c43ca333ae;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.7.3!postgresql.jar (1121ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.42.0/checker-qual-3.42.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.42.0!checker-qual.jar (243ms)\n",
      ":: resolution report :: resolve 1237ms :: artifacts dl 1374ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4055d71-0f99-4a25-8293-e2c43ca333ae\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (1289kB/12ms)\n",
      "25/12/02 17:44:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-12-02 17:44:34 [INFO] spark_helpers: [INFO] SparkSession criada: 'UnifyFlightChunks' (master=local[*]).\n",
      "2025-12-02 17:44:34 [INFO] unify_chunks: [UnifyChunks] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"unify_chunks\")\n",
    "\n",
    "spark = get_spark_session(\"UnifyFlightChunks\")\n",
    "log.info(\"[UnifyChunks] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f533a-a954-4657-8acc-0f1a884abfb8",
   "metadata": {},
   "source": [
    "### Definindo função de unificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6322b0-3594-4896-b662-90238262dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassemble_chunks(spark: SparkSession, chunk_files: list[str], header: bool = True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Lê múltiplos arquivos de chunk (flights_part_*.csv) e os unifica em um único DataFrame Spark.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        chunk_files (list[str]): Lista de caminhos completos dos arquivos csv a serem unificados.\n",
    "        header (bool, opcional): Define se os arquivos csv possuem cabeçalho. Padrão: True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame Spark consolidado com todos os chunks.\n",
    "    \"\"\"\n",
    "    if not chunk_files:\n",
    "        raise ValueError(\"[UnifyChunks][Error] Nenhum arquivo de chunk fornecido para unificação.\")\n",
    "\n",
    "    log.info(f\"[UnifyChunks] Lendo e concatenando {len(chunk_files)} arquivo(s) de chunk.\")\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", header)\n",
    "            .option(\"inferSchema\", False)\n",
    "            .csv(chunk_files)\n",
    "        )\n",
    "        \n",
    "        log.info(\"[UnifyChunks] Unificação dos chunks concluída com sucesso.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"[UnifyChunks][Error] Falha ao ler os arquivos csv: {e}.\")\n",
    "        \n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bf19e-fa03-45e1-9349-387deeb698c7",
   "metadata": {},
   "source": [
    "### Runner para o job `unify_flight_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01a748f-6efd-4d47-bf6e-e4d0b43f42e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:44:39 [INFO] unify_chunks: [UnifyChunks] Iniciando job de unificação de chunks.\n",
      "2025-12-02 17:44:39 [INFO] file_io: [INFO] Encontrados 12 arquivo(s).\n",
      "2025-12-02 17:44:39 [INFO] unify_chunks: [UnifyChunks] Lendo e concatenando 10 arquivo(s) de chunk.\n",
      "2025-12-02 17:44:43 [INFO] unify_chunks: [UnifyChunks] Unificação dos chunks concluída com sucesso.\n",
      "2025-12-02 17:44:43 [INFO] quality_gates_raw: [Quality][Raw] Iniciando validações do dataset 'raw_flights'.\n",
      "2025-12-02 17:44:44 [INFO] quality_gates_raw: [Quality][Raw]       _check_row_count_not_empty: raw_flights OK.\n",
      "2025-12-02 17:44:44 [INFO] quality_gates_raw: [Quality][Raw]       _check_schema_columns: raw_flights OK.\n",
      "2025-12-02 17:44:44 [INFO] quality_gates_raw: [Quality][Raw] Todas as validações para 'raw_flights' concluídas com sucesso.\n",
      "2025-12-02 17:45:05 [INFO] unify_chunks: [UnifyChunks] Arquivo unificado salvo em: /opt/airflow/data-layer/raw/stage/flights.parquet.\n",
      "2025-12-02 17:45:05 [INFO] unify_chunks: [UnifyChunks] Job de unificação de chunks encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[UnifyChunks] Iniciando job de unificação de chunks.\")\n",
    "\n",
    "    # Localiza arquivos csv na stage\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    chunk_files = [f for f in csv_files if \"flights_part\" in f]\n",
    "\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"[UnifyChunks][Error] Nenhum arquivo de chunk encontrado em {stage_path}.\")\n",
    "\n",
    "    # Unifica os chunks\n",
    "    df_unified = reassemble_chunks(spark, chunk_files)\n",
    "\n",
    "    # Executa quality gates\n",
    "    required_columns = [\n",
    "        'YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', \n",
    "        'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', \n",
    "        'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'ELAPSED_TIME', \n",
    "        'AIR_TIME', 'DISTANCE', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', \n",
    "        'ARRIVAL_DELAY', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', \n",
    "        'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY'\n",
    "    ]\n",
    "    run_quality_gates_raw(df_unified, \"raw_flights\", required_columns)\n",
    "\n",
    "    # Salva o arquivo unificado\n",
    "    stage_output = f\"{stage_path}/flights.parquet\"\n",
    "    df_unified.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(stage_output)\n",
    "    \n",
    "    log.info(f\"[UnifyChunks] Arquivo unificado salvo em: {stage_output}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[UnifyChunks][Error] Falha durante execução: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[UnifyChunks] Job de unificação de chunks encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e4f2c5-bc66-41e0-9625-bdefeedd277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:46:21 [INFO] unify_chunks: [UnifyChunks] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[UnifyChunks] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32caa90-059e-4dc1-8f66-15e07d480b5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 2: convert_csv_to_parquet\n",
    "\n",
    "Este job realiza a conversão dos arquivos csv presentes na pasta **Stage** (exceto os chunks `flights_part_*`) para o formato `parquet`, mantendo o schema original dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96eb742b-cb48-483f-b0c9-e66a5f905eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:46:22 [INFO] spark_helpers: [INFO] SparkSession criada: 'ConvertCsvToParquet' (master=local[*]).\n",
      "2025-12-02 17:46:22 [INFO] convert_csv: [ConvertCSV] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"convert_csv\")\n",
    "\n",
    "spark = get_spark_session(\"ConvertCsvToParquet\")\n",
    "log.info(\"[ConvertCSV] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66158890-5a38-47d8-a445-df86376d8557",
   "metadata": {},
   "source": [
    "### Definindo função de conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9fbe8a7-3ec1-4d45-8c9f-9bb22c01ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(spark: SparkSession, csv_files: list[str], stage_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converte arquivos csv em formato parquet.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        csv_files (list[str]): Lista de caminhos dos arquivos csv a converter.\n",
    "        stage_path (str): Caminho base da camada Stage.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"[ConvertCSV][Error] Nenhum arquivo csv fornecido para conversão.\")\n",
    "\n",
    "    for csv in csv_files:\n",
    "        try:\n",
    "            # Resolve caminhho do arquivo\n",
    "            base_name = os.path.basename(csv).replace(\".csv\", \".parquet\")\n",
    "\n",
    "            log.info(f\"[ConvertCSV] Lendo arquivo csv: {csv}.\")\n",
    "\n",
    "            df = (\n",
    "                spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", False)\n",
    "                .csv(csv)\n",
    "            )\n",
    "\n",
    "            # Executa quality gates\n",
    "            if df.columns[1] == 'AIRLINE':\n",
    "                required_columns = ['IATA_CODE', 'AIRLINE']\n",
    "            else: \n",
    "                required_columns = ['IATA_CODE', 'AIRPORT', 'CITY', 'STATE', 'COUNTRY', 'LATITUDE', 'LONGITUDE']\n",
    "            \n",
    "            run_quality_gates_raw(df, base_name, required_columns)\n",
    "\n",
    "            # Converte em parquet\n",
    "            parquet_path = f\"{stage_path}/{base_name}\"\n",
    "            df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "            \n",
    "            log.info(f\"[ConvertCSV] Arquivo convertido: {parquet_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"[ConvertCSV][ERROR] Falha ao converter {csv}: {e}.\")\n",
    "            raise IOError(f\"Erro ao processar {csv}: {e}.\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653d79e-60e7-4a03-bbe6-a6268f0c3ae4",
   "metadata": {},
   "source": [
    "### Runner para o job `convert_csv_to_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d361e6-ac46-48e6-9b4a-aca3b2d500fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:46:25 [INFO] convert_csv: [ConvertCSV] Iniciando job de conversão de csv para parquet.\n",
      "2025-12-02 17:46:25 [INFO] file_io: [INFO] Encontrados 12 arquivo(s).\n",
      "2025-12-02 17:46:25 [INFO] convert_csv: [ConvertCSV] Lendo arquivo csv: /opt/airflow/data-layer/raw/stage/airlines.csv.\n",
      "2025-12-02 17:46:25 [INFO] quality_gates_raw: [Quality][Raw] Iniciando validações do dataset 'airlines.parquet'.\n",
      "2025-12-02 17:46:26 [INFO] quality_gates_raw: [Quality][Raw]       _check_row_count_not_empty: airlines.parquet OK.\n",
      "2025-12-02 17:46:26 [INFO] quality_gates_raw: [Quality][Raw]       _check_schema_columns: airlines.parquet OK.\n",
      "2025-12-02 17:46:26 [INFO] quality_gates_raw: [Quality][Raw] Todas as validações para 'airlines.parquet' concluídas com sucesso.\n",
      "2025-12-02 17:46:27 [INFO] convert_csv: [ConvertCSV] Arquivo convertido: /opt/airflow/data-layer/raw/stage/airlines.parquet.\n",
      "2025-12-02 17:46:27 [INFO] convert_csv: [ConvertCSV] Lendo arquivo csv: /opt/airflow/data-layer/raw/stage/airports.csv.\n",
      "2025-12-02 17:46:27 [INFO] quality_gates_raw: [Quality][Raw] Iniciando validações do dataset 'airports.parquet'.\n",
      "2025-12-02 17:46:27 [INFO] quality_gates_raw: [Quality][Raw]       _check_row_count_not_empty: airports.parquet OK.\n",
      "2025-12-02 17:46:27 [INFO] quality_gates_raw: [Quality][Raw]       _check_schema_columns: airports.parquet OK.\n",
      "2025-12-02 17:46:27 [INFO] quality_gates_raw: [Quality][Raw] Todas as validações para 'airports.parquet' concluídas com sucesso.\n",
      "2025-12-02 17:46:28 [INFO] convert_csv: [ConvertCSV] Arquivo convertido: /opt/airflow/data-layer/raw/stage/airports.parquet.\n",
      "2025-12-02 17:46:28 [INFO] convert_csv: [ConvertCSV] Conversão concluída. 2 arquivo(s) processado(s).\n",
      "2025-12-02 17:46:28 [INFO] convert_csv: [ConvertCSV] Job de conversão encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[ConvertCSV] Iniciando job de conversão de csv para parquet.\")\n",
    "\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    target_files = [f for f in csv_files if \"flights_part\" not in f]\n",
    "\n",
    "    if not target_files:\n",
    "        raise FileNotFoundError(f\"[ConvertCSV][Error] Nenhum arquivo csv com o padrão encontrado em {stage_path}.\")\n",
    "\n",
    "    convert_csv_to_parquet(spark, target_files, stage_path)\n",
    "\n",
    "    log.info(f\"[ConvertCSV] Conversão concluída. {len(target_files)} arquivo(s) processado(s).\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[ConvertCSV][Error] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[ConvertCSV] Job de conversão encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c72d0b-ee89-42e1-a0e8-9fc3f225e31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:20:48 [INFO] convert_csv | [ConvertCSV] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[ConvertCSV] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecc39a-fe65-46ae-9493-81665eabeefe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 3: move_files_to_raw\n",
    "\n",
    "Este job move os arquivos `parquet` da pasta **Stage** para a camada **Raw**, organizando-os por data de processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a2401c-8919-4e19-a7b4-54a2c205c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/02 17:47:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025-12-02 17:47:03 [INFO] spark_helpers: [INFO] SparkSession criada: 'MoveStageToRaw' (master=local[*]).\n",
      "2025-12-02 17:47:03 [INFO] move_to_raw: [MoveToRaw] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"move_to_raw\")\n",
    "\n",
    "spark = get_spark_session(\"MoveStageToRaw\")\n",
    "log.info(\"[MoveToRaw] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef3ff0-15d7-432f-bb54-09a37c9cde74",
   "metadata": {},
   "source": [
    "### Runner para o job `move_files_to_raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0679e950-e36e-4882-8b4b-a0a2bba2d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:47:07 [INFO] move_to_raw: [MoveToRaw] Iniciando job de movimentação de arquivos.\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] Encontrados 3 arquivo(s).\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] Movendo arquivos para '/opt/airflow/data-layer/raw'.\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] Diretório criado: /opt/airflow/data-layer/raw/2025-12-02/PARQUET\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] 'airlines.parquet' movido para '/opt/airflow/data-layer/raw/2025-12-02/PARQUET/airlines.parquet'.\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] 'airports.parquet' movido para '/opt/airflow/data-layer/raw/2025-12-02/PARQUET/airports.parquet'.\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] 'flights.parquet' movido para '/opt/airflow/data-layer/raw/2025-12-02/PARQUET/flights.parquet'.\n",
      "2025-12-02 17:47:07 [INFO] file_io: [INFO] Movimentação concluída com sucesso.\n",
      "2025-12-02 17:47:07 [INFO] move_to_raw: [MoveToRaw] 3 arquivo(s) movido(s) para raw/2025-12-02.\n",
      "2025-12-02 17:47:07 [INFO] move_to_raw: [MoveToRaw] Job de movimentação encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[MoveToRaw] Iniciando job de movimentação de arquivos.\")\n",
    "\n",
    "    parquet_files = check_files_in_folder(stage_path, \"*.parquet\")\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"[MoveToRaw][Error] Nenhum arquivo parquet encontrado em {stage_path}.\")\n",
    "\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    move_files(\n",
    "        spark=spark,\n",
    "        source_files=parquet_files,\n",
    "        base_dest_path=raw_path,\n",
    "        processing_date=processing_date,\n",
    "    )\n",
    "\n",
    "    log.info(f\"[MoveToRaw] {len(parquet_files)} arquivo(s) movido(s) para raw/{processing_date}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[MoveToRaw][Error] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[MoveToRaw] Job de movimentação encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6116a43d-7811-4cbd-bff0-8ca5e0ad4805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:47:09 [INFO] move_to_raw: [MoveToRaw] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[MoveToRaw] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cca310-da7f-419b-852c-ddb9c6d72c5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 4: load_raw_dbt\n",
    "\n",
    "Este job carrega os **dados brutos** dos arquivos `airlines.parquet`, `airports.parquet` e `flights_part_1.csv` da pasta **Stage** para o schema `dbt_raw`, que servirá de base para as transformações realizadas pelo *dbt*. A opção por carregar apenas o arquivo `flights_part_1.csv` na tabela de voos foi tomada por motivos de performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8038f3fa-eabe-47f4-8044-cf867e812dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 19:51:17 [INFO] load_raw_dbt | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-25 19:51:18 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'LoadRawdbt' (master=local[*]).\n",
      "2025-11-25 19:51:18 [INFO] load_raw_dbt | [LoadRaw][dbt] Sessão Spark iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"load_raw_dbt\")\n",
    "spark = get_spark_session(\"LoadRawdbt\")\n",
    "\n",
    "log.info(\"[LoadRaw][dbt] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa875db-30b8-46fe-9e21-d9af1261c31c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `load_raw_dbt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bd84236-3b48-46e5-9921-b7f3f99754b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 19:54:50 [INFO] load_raw_dbt | [LoadRaw][dbt] Iniciando job de carga dos dados para o dbt.\n",
      "2025-11-25 19:54:50 [INFO] load_raw_dbt | [dbt][Raw] Lendo airlines: /opt/airflow/data-layer/raw/stage/airlines.csv.\n",
      "2025-11-25 19:54:51 [INFO] load_raw_dbt | [LoadRaw][dbt] Inserindo dados em 'dbt_raw.airlines'.\n",
      "2025-11-25 19:54:51 [WARN] spark_helpers | [WARN] Airflow indisponível, utilizando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-11-25 19:54:51 [INFO] spark_helpers | [LOAD] Limpando tabela 'dbt_raw.airlines'.\n",
      "2025-11-25 19:54:51 [INFO] spark_helpers | [LOAD] Dados carregados em 'dbt_raw.airlines' com sucesso (modo: append).\n",
      "2025-11-25 19:54:51 [INFO] load_raw_dbt | [LoadRaw][dbt] Lendo airports: /opt/airflow/data-layer/raw/stage/airports.csv.\n",
      "2025-11-25 19:54:52 [INFO] load_raw_dbt | [LoadRaw][dbt] Inserindo dados em 'dbt_raw.airports'.\n",
      "2025-11-25 19:54:52 [WARN] spark_helpers | [WARN] Airflow indisponível, utilizando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-11-25 19:54:52 [INFO] spark_helpers | [LOAD] Limpando tabela 'dbt_raw.airports'.\n",
      "2025-11-25 19:54:52 [INFO] spark_helpers | [LOAD] Dados carregados em 'dbt_raw.airports' com sucesso (modo: append).\n",
      "2025-11-25 19:54:52 [INFO] load_raw_dbt | [LoadRaw][dbt] Lendo chunk de flights: /opt/airflow/data-layer/raw/stage/flights_part_01.csv.\n",
      "2025-11-25 19:54:52 [INFO] load_raw_dbt | [LoadRaw][dbt] Inserindo dados em 'dbt_raw.flights'.\n",
      "2025-11-25 19:54:52 [WARN] spark_helpers | [WARN] Airflow indisponível, utilizando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-11-25 19:54:52 [INFO] spark_helpers | [LOAD] Limpando tabela 'dbt_raw.flights'.\n",
      "2025-11-25 19:55:01 [INFO] spark_helpers | [LOAD] Dados carregados em 'dbt_raw.flights' com sucesso (modo: append).\n",
      "2025-11-25 19:55:01 [INFO] load_raw_dbt | [LoadRaw][dbt] Carga concluída com sucesso.\n",
      "2025-11-25 19:55:01 [INFO] load_raw_dbt | [LoadRaw][dbt] Job de carga dos dados para o dbt encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[LoadRaw][dbt] Iniciando job de carga dos dados para o dbt.\")\n",
    "\n",
    "    read_options = {\n",
    "        \"header\": \"true\",\n",
    "        \"inferSchema\": \"false\",\n",
    "        \"delimiter\": \",\",\n",
    "    }\n",
    "\n",
    "    # Carregar airlines.csv (RAW via Spark)\n",
    "    try:\n",
    "        airlines_path = Path(stage_path) / \"airlines.csv\"\n",
    "\n",
    "        log.info(f\"[dbt][Raw] Lendo airlines: {airlines_path}.\")\n",
    "\n",
    "        airlines_df = spark.read.options(**read_options).csv(str(airlines_path))\n",
    "\n",
    "        log.info(\"[LoadRaw][dbt] Inserindo dados em 'dbt_raw.airlines'.\")\n",
    "        load_to_postgres(\n",
    "            df=airlines_df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"dbt_raw.airlines\",\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[LoadRaw][dbt][Error] Falha ao carregar airlines: {e}.\")\n",
    "        raise\n",
    "\n",
    "    # Carregar airports.csv\n",
    "    try:\n",
    "        airports_path = Path(stage_path) / \"airports.csv\"\n",
    "\n",
    "        log.info(f\"[LoadRaw][dbt] Lendo airports: {airports_path}.\")\n",
    "\n",
    "        airports_df = spark.read.options(**read_options).csv(str(airports_path))\n",
    "\n",
    "        log.info(\"[LoadRaw][dbt] Inserindo dados em 'dbt_raw.airports'.\")\n",
    "        load_to_postgres(\n",
    "            df=airports_df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"dbt_raw.airports\",\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[LoadRaw][dbt][Error] Falha ao carregar airports: {e}.\")\n",
    "        raise\n",
    "\n",
    "    # Carregar flights_part_01.csv\n",
    "    try:\n",
    "        flights_chunk_path = Path(stage_path) / \"flights_part_01.csv\"\n",
    "\n",
    "        log.info(f\"[LoadRaw][dbt] Lendo chunk de flights: {flights_chunk_path}.\")\n",
    "\n",
    "        flights_df = spark.read.options(**read_options).csv(str(flights_chunk_path))\n",
    "\n",
    "        log.info(\"[LoadRaw][dbt] Inserindo dados em 'dbt_raw.flights'.\")\n",
    "        load_to_postgres(\n",
    "            df=flights_df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"dbt_raw.flights\",\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[LoadRaw][dbt][Error] Falha ao carregar flights: {e}.\")\n",
    "        raise\n",
    "\n",
    "    log.info(\"[LoadRaw][dbt] Carga concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[LoadRaw][dbt][Error] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[LoadRaw][dbt] Job de carga dos dados para o dbt encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "971cb863-aa3a-4caf-8566-f74fc3f628ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 19:55:27 [INFO] load_raw_dbt | [LoadRaw][dbt] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[LoadRaw][dbt] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3df3e-921f-4995-b49d-1d3b4e2e69fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 5: cleanup_stage\n",
    "\n",
    "Este job remove os arquivos `csv` e `parquet` da pasta **Stage** após a conclusão da movimentação para a camada **Raw**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42387796-623a-4513-96d6-906dfe66e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:20:58 [INFO] cleanup_stage | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-25 18:20:58 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'CleanupStage' (master=local[*]).\n",
      "2025-11-25 18:20:58 [INFO] cleanup_stage | [CleanupStage] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"cleanup_stage\")\n",
    "\n",
    "spark = get_spark_session(\"CleanupStage\")\n",
    "log.info(\"[CleanupStage] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d8ef2-53d3-4eee-874f-b7a9a49eb99f",
   "metadata": {},
   "source": [
    "### Runner para o job `cleanup_stage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a9af3e5-7204-4d7f-a08a-9abf9b2ae7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:49:01 [INFO] move_to_raw: [CleanupStage] Iniciando job de limpeza da stage.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] Encontrados 12 arquivo(s).\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] Deletando 12 arquivo(s).\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/airlines.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/airports.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_01.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_02.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_03.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_04.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_05.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_06.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_07.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_08.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_09.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] '/opt/airflow/data-layer/raw/stage/flights_part_10.csv' deletado com sucesso.\n",
      "2025-12-02 17:49:01 [INFO] file_io: [INFO] Deleção concluída.\n",
      "2025-12-02 17:49:01 [INFO] move_to_raw: [CleanupStage] 12 arquivo(s) removido(s) da stage.\n",
      "2025-12-02 17:49:01 [INFO] move_to_raw: [CleanupStage] Job de limpeza encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[CleanupStage] Iniciando job de limpeza da stage.\")\n",
    "\n",
    "    removable_files = check_files_in_folder(stage_path, \"*.*\")\n",
    "    if not removable_files:\n",
    "        log.warning(\"[CleanupStage] Nenhum arquivo encontrado para remoção.\")\n",
    "    else:\n",
    "        delete_files(spark, removable_files)\n",
    "        log.info(f\"[CleanupStage] {len(removable_files)} arquivo(s) removido(s) da stage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[CleanupStage][Error] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[CleanupStage] Job de limpeza encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a181aacc-8129-4008-bdf6-895118116d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:49:02 [INFO] move_to_raw: [CleanupStage] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[CleanupStage] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbfc46-a8f3-4111-b490-841f2e1eac84",
   "metadata": {},
   "source": [
    "# Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f3cc8-3877-4335-bcda-58ccee791645",
   "metadata": {},
   "source": [
    "## Job 6: airlines_transform\n",
    "\n",
    "Este job realiza a transformação e validação do dataset de companhias aéreas (`airlines.parquet`) da camada **Raw** para a camada **Silver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de0286f-fe80-4406-b1e0-3f53af03d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:23:04 [INFO] transform_airlines | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-25 18:23:04 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'TransformAirlines' (master=local[*]).\n",
      "2025-11-25 18:23:04 [INFO] transform_airlines | [Airlines] Sessão Spark iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"transform_airlines\")\n",
    "\n",
    "spark = get_spark_session(\"TransformAirlines\")\n",
    "log.info(\"[Airlines] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131b72e-a2c7-4fc1-bd9c-c9fef8bc6f5c",
   "metadata": {},
   "source": [
    "### Definindo função de transformação do dataset `airlines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df0454bf-97c5-490b-ada7-547a7d53cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_airlines(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma e valida o DataFrame de companhias aéreas para a camada silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame bruto da camada Bronze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame padronizado pronto para a camada Silver.\n",
    "    \"\"\"\n",
    "    log.info(\"[Airlines] Iniciando transformações.\")\n",
    "\n",
    "    # Verifica se as colunas obrigatórias estão presentes\n",
    "    log.info(\"[Airlines] Verificando presença de colunas obrigatórias.\")\n",
    "    required = {\"IATA_CODE\", \"AIRLINE\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"[Airlines][ERROR] Colunas faltando no dataset: {missing}.\")\n",
    "\n",
    "    # Renomeia e converte tipos de colunas\n",
    "    log.info(\"[Airlines] Renomeando e convertendo os tipos das colunas.\")\n",
    "    df2 = (\n",
    "        df.withColumnRenamed(\"IATA_CODE\", \"airline_iata_code\")\n",
    "          .withColumnRenamed(\"AIRLINE\", \"airline_name\")\n",
    "          .withColumn(\"airline_iata_code\", F.col(\"airline_iata_code\").cast(StringType()))\n",
    "          .withColumn(\"airline_name\", F.col(\"airline_name\").cast(StringType()))\n",
    "    )\n",
    "\n",
    "    # Padroniza os nomes das colunas para minúsculo\n",
    "    log.info(\"[Airlines] Padronizando nomes de colunas para minúsculo.\")\n",
    "    df2 = df2.toDF(*[c.lower() for c in df2.columns])\n",
    "\n",
    "    log.info(\"[Airlines] Transformação concluída com sucesso.\")\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d923f-8541-4514-ab5c-f9f79e38a512",
   "metadata": {},
   "source": [
    "### Runner para o job `airlines_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c494168-8fed-40ae-b94a-ea924c71fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Iniciando job de trasnformação de 'airlines'.\n",
      "2025-11-25 18:25:11 [INFO] file_io | [INFO] Partição selecionada: 2025-11-25\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Lendo dataset: /opt/airflow/data-layer/raw/2025-11-25/PARQUET/airlines.parquet.\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Iniciando transformações.\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Verificando presença de colunas obrigatórias.\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Renomeando e convertendo os tipos das colunas.\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Padronizando nomes de colunas para minúsculo.\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Transformação concluída com sucesso.\n",
      "2025-11-25 18:25:11 [INFO] transform_airlines | [Airlines] Executando quality gates.\n",
      "2025-11-25 18:25:11 [INFO] quality_gates_silver_base | [Quality][Refinement] Iniciando validações do dataset 'airlines_silver'.\n",
      "2025-11-25 18:25:11 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_row_count_not_empty: OK\n",
      "2025-11-25 18:25:11 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_schema_columns: OK\n",
      "2025-11-25 18:25:11 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_no_null_primary_key: OK\n",
      "2025-11-25 18:25:11 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_unique_primary_key: OK\n",
      "2025-11-25 18:25:12 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_no_full_duplicates: OK\n",
      "2025-11-25 18:25:12 [INFO] quality_gates_silver_base | [Quality][Refinement] Validações para 'airlines_silver' concluídas.\n",
      "2025-11-25 18:25:12 [INFO] transform_airlines | [Airlines] Quality gates concluídos com sucesso.\n",
      "2025-11-25 18:25:13 [INFO] transform_airlines | [Refinement][Airlines] Dataset salvo na camada silver: /opt/airflow/data-layer/silver/2025-11-25/PARQUET/airlines.parquet.\n",
      "2025-11-25 18:25:13 [INFO] transform_airlines | [Airlines] Job de trasnformação de 'airlines' encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[Airlines] Iniciando job de trasnformação de 'airlines'.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(raw_path, mode=run_mode, date_str=run_date)\n",
    "    src = Path(raw_path) / source_partition / \"PARQUET\" / \"airlines.parquet\"\n",
    "    dst = Path(silver_path) / source_partition / \"PARQUET\" / \"airlines.parquet\"\n",
    "\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"[Airlines][ERROR] Arquivo não encontrado: {src}.\")\n",
    "\n",
    "    # Leitura do dataset bruto\n",
    "    log.info(f\"[Airlines] Lendo dataset: {src}.\")\n",
    "    df = spark.read.parquet(str(src))\n",
    "\n",
    "    # Aplica transformações\n",
    "    df_tf = transform_airlines(df)\n",
    "\n",
    "    # Executa quality gates\n",
    "    required_cols = [\"airline_iata_code\", \"airline_name\"]\n",
    "    pk_cols = [\"airline_iata_code\"]\n",
    "    \n",
    "    log.info(\"[Airlines] Executando quality gates.\")\n",
    "    \n",
    "    run_quality_gates_silver_base(\n",
    "        df=df_tf,\n",
    "        name=\"airlines_silver\",\n",
    "        required_columns=required_cols,\n",
    "        pk_columns=pk_cols,\n",
    "    )\n",
    "    \n",
    "    log.info(\"[Airlines] Quality gates concluídos com sucesso.\")\n",
    "\n",
    "    # Cria diretório de destino e grava o resultado na silver\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_tf.write.mode(\"overwrite\").parquet(str(dst))\n",
    "    log.info(f\"[Refinement][Airlines] Dataset salvo na camada silver: {dst}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Airlines][ERROR] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Airlines] Job de trasnformação de 'airlines' encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d730b79a-12b5-4257-a4ec-660674056e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df_tf.printSchema()\n",
    "\n",
    "df_tf.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acc56ea5-9809-40f5-b947-544d941cdd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:25:22 [INFO] transform_airlines | [Airlines] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Airlines] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d26e6-6552-485b-a920-466ad7f952d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 7: airports_transform\n",
    "\n",
    "Este job realiza a transformação e validação do dataset de aeroportos (`airports.parquet`) da camada **Raw** para a camada **Silver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd760de4-2bff-403a-ada4-ec59eab26835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:25:25 [INFO] transform_airports | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-25 18:25:25 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'TransformAirports' (master=local[*]).\n",
      "2025-11-25 18:25:25 [INFO] transform_airports | [Airports] Sessão Spark iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"transform_airports\")\n",
    "\n",
    "spark = get_spark_session(\"TransformAirports\")\n",
    "log.info(\"[Airports] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa78b7d-407b-4fde-84e1-4d22d6b5a55e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de trasformação do dataset `airports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ecbe176-c0e5-40c1-9404-dc596ab45bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_airports(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma e valida o DataFrame de aeroportos para a camada silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame bruto lido da camada bronze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame limpo e padronizado para a camada silver.\n",
    "    \"\"\"\n",
    "    log.info(\"[Airports] Iniciando transformações.\")\n",
    "\n",
    "    # Verifica colunas obrigatórias\n",
    "    required = {\"IATA_CODE\", \"LATITUDE\", \"LONGITUDE\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"[Airports][ERROR] Colunas faltando no dataset: {missing}.\")\n",
    "\n",
    "    # Correções de coordenadas faltosas\n",
    "    corrections = {\n",
    "        \"ECP\": {\"LATITUDE\": 30.3549, \"LONGITUDE\": -86.6160},\n",
    "        \"PBG\": {\"LATITUDE\": 44.6895, \"LONGITUDE\": -68.0448},\n",
    "        \"UST\": {\"LATITUDE\": 42.0703, \"LONGITUDE\": -87.9539},\n",
    "    }\n",
    "\n",
    "    # Tipagem e renomeação de colunas principais\n",
    "    df2 = (\n",
    "        df.withColumnRenamed(\"IATA_CODE\", \"airport_iata_code\")\n",
    "          .withColumn(\"airport_iata_code\", F.col(\"airport_iata_code\").cast(StringType()))\n",
    "          .withColumn(\"LATITUDE\", F.col(\"LATITUDE\").cast(DoubleType()))\n",
    "          .withColumn(\"LONGITUDE\", F.col(\"LONGITUDE\").cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "    # Remove coluna COUNTRY\n",
    "    if \"COUNTRY\" in df2.columns:\n",
    "        df2 = df2.drop(\"COUNTRY\")\n",
    "\n",
    "    # Aplica correções manuais de coordenadas\n",
    "    for code, coords in corrections.items():\n",
    "        df2 = df2.withColumn(\n",
    "            \"LATITUDE\",\n",
    "            F.when(F.col(\"airport_iata_code\") == code, F.lit(coords[\"LATITUDE\"])).otherwise(F.col(\"LATITUDE\"))\n",
    "        ).withColumn(\n",
    "            \"LONGITUDE\",\n",
    "            F.when(F.col(\"airport_iata_code\") == code, F.lit(coords[\"LONGITUDE\"])).otherwise(F.col(\"LONGITUDE\"))\n",
    "        )\n",
    "\n",
    "    # Renomeia colunas e força lowercase\n",
    "    rename_map = {\n",
    "        \"AIRPORT\": \"airport_name\",\n",
    "        \"CITY\": \"city\",\n",
    "        \"STATE\": \"state\",\n",
    "        \"LATITUDE\": \"latitude\",\n",
    "        \"LONGITUDE\": \"longitude\",\n",
    "    }\n",
    "    for old, new in rename_map.items():\n",
    "        if old in df2.columns:\n",
    "            df2 = df2.withColumnRenamed(old, new)\n",
    "\n",
    "    # Normaliza nomes para minúsculo\n",
    "    df2 = df2.toDF(*[c.lower() for c in df2.columns])\n",
    "\n",
    "    log.info(\"[Airports] Transformação concluída com sucesso.\")\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7224d44-bfd3-47f3-927f-5383d30e98e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `airports_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1f357c3-c198-42aa-923f-9e71cffd374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:26:08 [INFO] transform_airports | [Airports] Iniciando job de trasnformação de 'airports'.\n",
      "2025-11-25 18:26:08 [INFO] file_io | [INFO] Partição selecionada: 2025-11-25\n",
      "2025-11-25 18:26:08 [INFO] transform_airports | [Airports] Lendo dataset: /opt/airflow/data-layer/raw/2025-11-25/PARQUET/airports.parquet.\n",
      "2025-11-25 18:26:08 [INFO] transform_airports | [Airports] Iniciando transformações.\n",
      "2025-11-25 18:26:08 [INFO] transform_airports | [Airports] Transformação concluída com sucesso.\n",
      "2025-11-25 18:26:08 [INFO] transform_airports | [Airports] Executando quality gates.\n",
      "2025-11-25 18:26:08 [INFO] quality_gates_silver_base | [Quality][Refinement] Iniciando validações do dataset 'airports_silver'.\n",
      "2025-11-25 18:26:09 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_row_count_not_empty: OK\n",
      "2025-11-25 18:26:09 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_schema_columns: OK\n",
      "2025-11-25 18:26:09 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_no_null_primary_key: OK\n",
      "2025-11-25 18:26:09 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_unique_primary_key: OK\n",
      "2025-11-25 18:26:10 [INFO] quality_gates_silver_base | [Quality][Refinement]    _check_no_full_duplicates: OK\n",
      "2025-11-25 18:26:10 [INFO] quality_gates_silver_base | [Quality][Refinement] Validações para 'airports_silver' concluídas.\n",
      "2025-11-25 18:26:10 [INFO] transform_airports | [Airports] Quality gates concluídos.\n",
      "2025-11-25 18:26:11 [INFO] transform_airports | [Airports] Dataset salvo na camada silver: /opt/airflow/data-layer/silver/2025-11-25/PARQUET/airports.parquet.\n",
      "2025-11-25 18:26:11 [INFO] transform_airports | [Airports] Job de trasnformação de 'airports' encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[Airports] Iniciando job de trasnformação de 'airports'.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(raw_path, mode=run_mode, date_str=run_date)\n",
    "    src = Path(raw_path) / source_partition / \"PARQUET\" / \"airports.parquet\"\n",
    "    dst = Path(silver_path) / source_partition / \"PARQUET\" / \"airports.parquet\"\n",
    "\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"[Airports][ERROR] Arquivo não encontrado: {src}.\")\n",
    "\n",
    "    # Lê dataset e aplica transformações\n",
    "    log.info(f\"[Airports] Lendo dataset: {src}.\")\n",
    "    df = spark.read.parquet(str(src))\n",
    "    \n",
    "    df_tf = transform_airports(df)\n",
    "\n",
    "    # Executa quality gates\n",
    "    log.info(\"[Airports] Executando quality gates.\")\n",
    "\n",
    "    required_cols = [\"airport_iata_code\", \"airport_name\", \"city\", \"state\", \"latitude\", \"longitude\"]\n",
    "    pk_cols = [\"airport_iata_code\"]\n",
    "\n",
    "    run_quality_gates_silver_base(\n",
    "        df=df_tf,\n",
    "        name=\"airports_silver\",\n",
    "        required_columns=required_cols,\n",
    "        pk_columns=pk_cols,\n",
    "    )\n",
    "\n",
    "    log.info(\"[Airports] Quality gates concluídos.\")\n",
    "\n",
    "    # Cria diretório e escreve resultado\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_tf.write.mode(\"overwrite\").parquet(str(dst))\n",
    "\n",
    "    log.info(f\"[Airports] Dataset salvo na camada silver: {dst}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Airports][ERROR] Falha na execução do job: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Airports] Job de trasnformação de 'airports' encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "998559c3-b78d-46a8-a21f-c09819446360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df_tf.printSchema()\n",
    "\n",
    "df_tf.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecea774e-ce90-492c-8a47-cbcfe4668f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:26:15 [INFO] transform_airports | [Airlines] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Airlines] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408151b6-0ac1-4c05-a753-30864093ecd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 8: flights_transform\n",
    "\n",
    "Este job transforma e normaliza o dataset de voos (`flights.parquet`) da **Raw** para a **Silver** (pré-join), gerando um arquivo intermediário a ser consumido na etapa de agregação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2b8b5-0c7e-4dea-800d-3d02ef0e172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"transform_flights_silver\")\n",
    "\n",
    "spark = get_spark_session(\"TransformFlightsSilver\")\n",
    "log.info(\"[Flights] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9db60-6a6f-4088-b5bb-f90076c058c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo funções de normalização das colunas de datas, de diferença de tempo e transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0921e8e-4794-4897-8005-bcd7e695939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_time_expr(col_name: str) -> F.Column:\n",
    "    \"\"\"\n",
    "    Normaliza valores de horário removendo casas decimais e preenchendo zeros à esquerda.\n",
    "    \"\"\"\n",
    "    return F.when(\n",
    "        F.col(col_name).isNotNull(),\n",
    "        F.lpad(F.regexp_replace(F.col(col_name).cast(\"string\"), r\"\\.0$\", \"\"), 4, \"0\")\n",
    "    )\n",
    "\n",
    "\n",
    "def abs_min_diff(c1: str, c2: str) -> F.Column:\n",
    "    \"\"\"\n",
    "    Retorna a diferença absoluta entre dois horários em minutos.\n",
    "    \"\"\"\n",
    "    return F.abs(F.col(c1).cast(\"long\") - F.col(c2).cast(\"long\")) / 60.0\n",
    "\n",
    "\n",
    "def transform_flights(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma e valida o DataFrame de voos (pré-join) para a camada Silver.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame bruto lido da camada Bronze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame transformado e validado.\n",
    "    \"\"\"\n",
    "    log.info(\"[Flights] Iniciando transformações.\")\n",
    "\n",
    "    # Validação mínima de colunas obrigatórias\n",
    "    log.info(\"[Flights] Verificando presença de colunas obrigatórias.\")\n",
    "    required = {\"YEAR\", \"MONTH\", \"DAY\", \"AIRLINE\", \"FLIGHT_NUMBER\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"[Flights][ERROR] Colunas faltando: {missing}.\")\n",
    "\n",
    "    # Filtro inicial e padronização de nomes\n",
    "    log.info(\"[Flights] Aplicando filtros iniciais e padronizando nomes.\")\n",
    "    df2 = (\n",
    "        df.filter((F.col(\"DIVERTED\") != 1) & (F.col(\"CANCELLED\") != 1))\n",
    "          .toDF(*[c.lower() for c in df.columns])\n",
    "          .withColumn(\"flight_date\", to_date_from_ymd(F.col(\"year\"), F.col(\"month\"), F.col(\"day\")))\n",
    "          .withColumnRenamed(\"year\", \"flight_year\")\n",
    "          .withColumnRenamed(\"month\", \"flight_month\")\n",
    "          .withColumnRenamed(\"day\", \"flight_day\")\n",
    "          .withColumnRenamed(\"day_of_week\", \"flight_day_of_week\")\n",
    "    )\n",
    "\n",
    "    # Normalização de horários\n",
    "    log.info(\"[Flights] Normalização de colunas com horários.\")\n",
    "    time_cols = [\n",
    "        \"scheduled_departure\", \"departure_time\",\n",
    "        \"scheduled_arrival\", \"arrival_time\",\n",
    "        \"wheels_off\", \"wheels_on\",\n",
    "    ]\n",
    "\n",
    "    for col_name in time_cols:\n",
    "        if col_name in df2.columns:\n",
    "            tmp = f\"{col_name}_str\"\n",
    "            df2 = (\n",
    "                df2.withColumn(tmp, normalize_time_expr(col_name))\n",
    "                    .withColumn(\n",
    "                        col_name,\n",
    "                        F.to_timestamp(\n",
    "                            F.concat_ws(\" \", F.col(\"flight_date\").cast(\"string\"), F.col(tmp)),\n",
    "                            \"yyyy-MM-dd HHmm\"\n",
    "                        )\n",
    "                    )\n",
    "                    .drop(tmp)\n",
    "            )\n",
    "\n",
    "    # Detecção de horários trocados\n",
    "    log.info(\"[Flights] Correção de horários trocados.\")\n",
    "    df2 = (\n",
    "        df2\n",
    "        .withColumn(\"diff_dep_sched_arr\", abs_min_diff(\"departure_time\", \"scheduled_arrival\"))\n",
    "        .withColumn(\"diff_dep_sched_dep\", abs_min_diff(\"departure_time\", \"scheduled_departure\"))\n",
    "        .withColumn(\"diff_arr_sched_dep\", abs_min_diff(\"arrival_time\", \"scheduled_departure\"))\n",
    "        .withColumn(\"diff_arr_sched_arr\", abs_min_diff(\"arrival_time\", \"scheduled_arrival\"))\n",
    "        .withColumn(\n",
    "            \"is_swapped\",\n",
    "            (F.col(\"diff_dep_sched_arr\") < F.col(\"diff_dep_sched_dep\")) &\n",
    "            (F.col(\"diff_arr_sched_dep\") < F.col(\"diff_arr_sched_arr\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"departure_time_tmp\",\n",
    "            F.when(F.col(\"is_swapped\"), F.col(\"arrival_time\")).otherwise(F.col(\"departure_time\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"arrival_time_tmp\",\n",
    "            F.when(F.col(\"is_swapped\"), F.col(\"departure_time\")).otherwise(F.col(\"arrival_time\"))\n",
    "        )\n",
    "        .drop(\"departure_time\", \"arrival_time\")\n",
    "        .withColumnRenamed(\"departure_time_tmp\", \"departure_time\")\n",
    "        .withColumnRenamed(\"arrival_time_tmp\", \"arrival_time\")\n",
    "        .drop(\n",
    "            \"diff_dep_sched_arr\", \"diff_dep_sched_dep\",\n",
    "            \"diff_arr_sched_dep\", \"diff_arr_sched_arr\", \"is_swapped\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Conversão numérica e substituição de nulos\n",
    "    log.info(\"[Flights] Normalizando colunas numéricas.\")\n",
    "    numeric_cols = [\n",
    "        \"departure_delay\", \"arrival_delay\", \"taxi_out\", \"taxi_in\",\n",
    "        \"air_time\", \"elapsed_time\", \"scheduled_time\", \"distance\",\n",
    "        \"air_system_delay\", \"security_delay\", \"airline_delay\",\n",
    "        \"late_aircraft_delay\", \"weather_delay\",\n",
    "    ]\n",
    "    delay_cols = [\n",
    "        \"air_system_delay\", \"security_delay\", \"airline_delay\",\n",
    "        \"late_aircraft_delay\", \"weather_delay\",\n",
    "    ]\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        if c in df2.columns:\n",
    "            expr = F.col(c).cast(DoubleType())\n",
    "            if c in delay_cols:\n",
    "                expr = F.coalesce(expr, F.lit(0.0))\n",
    "            df2 = df2.withColumn(c, expr)\n",
    "\n",
    "    # Ajuste de voos overnight\n",
    "    log.info(\"[Flights] Ajustes nos voos que atravessa dia.\")\n",
    "    df2 = (\n",
    "        df2.withColumn(\n",
    "            \"is_overnight_flight\",\n",
    "            F.when(\n",
    "                (F.col(\"arrival_time\").isNotNull()) &\n",
    "                (F.col(\"departure_time\").isNotNull()) &\n",
    "                (F.hour(F.col(\"arrival_time\")) < F.hour(F.col(\"departure_time\"))),\n",
    "                F.lit(True)\n",
    "            ).otherwise(F.lit(False))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"arrival_time\",\n",
    "            F.when(F.col(\"is_overnight_flight\"),\n",
    "                   F.col(\"arrival_time\") + F.expr(\"INTERVAL 1 DAY\"))\n",
    "             .otherwise(F.col(\"arrival_time\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filtros finais\n",
    "    log.info(\"[Flights] Aplicando filtros finais.\")\n",
    "    df2 = df2.filter(\n",
    "        (F.col(\"departure_time\").isNotNull()) &\n",
    "        (F.col(\"arrival_time\").isNotNull()) &\n",
    "        (F.col(\"arrival_time\") > F.col(\"departure_time\")) &\n",
    "        (~F.col(\"origin_airport\").rlike(\"^[0-9]+$\")) &\n",
    "        (~F.col(\"destination_airport\").rlike(\"^[0-9]+$\"))\n",
    "    )\n",
    "\n",
    "    # Remoção de colunas desnecessárias\n",
    "    log.info(\"[Flights] Removendo colunas desnecessárias.\")\n",
    "    drop_cols = [c for c in [\"diverted\", \"cancelled\", \"cancellation_reason\"] if c in df2.columns]\n",
    "    if drop_cols:\n",
    "        df2 = df2.drop(*drop_cols)\n",
    "\n",
    "    log.info(\"[Flights] Transformação concluída.\")\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be8b7d-23dd-44c9-9d0b-3cad38c3b6b2",
   "metadata": {},
   "source": [
    "### Runner para o job `flights_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adf024-dab1-441e-9f03-48479c9b26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Flights] Iniciando job de trasnformação de 'flights'.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition = find_partition(raw_path, mode=run_mode, date_str=run_date)\n",
    "    src = Path(raw_path) / source_partition / \"PARQUET\" / \"flights.parquet\"\n",
    "    dst_dir = Path(silver_path) / source_partition / \"PARQUET\"\n",
    "    dst = dst_dir / \"flights_pre_join.parquet\"\n",
    "    airports_src = dst_dir / \"airports.parquet\"\n",
    "\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"[Flights][ERROR] Arquivo não encontrado: {src}.\")\n",
    "    if not airports_src.exists():\n",
    "        raise FileNotFoundError(f\"[Flights][ERROR] Airports não encontrado na silver: {airports_src}.\")\n",
    "\n",
    "    # Leitura e transformação\n",
    "    log.info(f\"[Flights] Lendo datasets: {src} e {airports_src}.\")\n",
    "    \n",
    "    df = spark.read.parquet(str(src))\n",
    "    airports_df = spark.read.parquet(str(airports_src))\n",
    "    df_tf = transform_flights(df)\n",
    "    \n",
    "    # Quality gate\n",
    "    run_quality_gates_silver_flights(df_tf, airports_df)\n",
    "\n",
    "    # Escrita do resultado intermediário\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "    df_tf.coalesce(1).write.mode(\"overwrite\").parquet(str(dst))\n",
    "    spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
    "\n",
    "    log.info(f\"[Flights] Dataset salvo na silver: {dst}.\")\n",
    "\n",
    "    # Libera cache\n",
    "    df_tf.unpersist()\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Flights][ERROR] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Flights] Fim do job de transformação de 'flights'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20855b7-21d2-4ae2-80be-bbd879a6bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "path_df = Path(silver_path) / \"2025-11-12\" / \"PARQUET\" / \"flights_pre_join.parquet\" # Verificar a data quando rodar\n",
    "\n",
    "df = spark.read.parquet(str(path_df))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.limit(5).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87ac082c-49d9-4ac4-954b-8a876bd3ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:43:49 [INFO] transform_flights_silver | [Flights] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Flights] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c0620-ccae-4d1e-bce0-9266c557c35f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 9: silver_aggregate\n",
    "\n",
    "Este job realiza a agregação da camada **Silver**, unindo os datasets já tratados de `flights`, `airlines` e `airports` em um único dataset consolidado `flights_aggregated.parquet`, conforme o **DDL** da camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f43b4-2954-4232-831c-4cdefabd816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"silver_aggregate\")\n",
    "\n",
    "spark = get_spark_session(\"RefinementSilverAggregate\")\n",
    "log.info(\"[Aggregate] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c54a97-3760-4977-9468-3cd015d8a7ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definido função de agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e671e026-7e80-4864-a9d2-c329dfe991bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_flights_df(\n",
    "    flights_silver_df: DataFrame,\n",
    "    airlines_silver_df: DataFrame,\n",
    "    airports_silver_df: DataFrame,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Constrói o DataFrame agregado da camada Silver (flights_aggregated), unindo:\n",
    "        - flights_pre_join.parquet\n",
    "        - airlines.parquet\n",
    "        - airports.parquet\n",
    "\n",
    "    Args:\n",
    "        flights_silver_df (DataFrame): Dataset de voos já transformado (pré-join).\n",
    "        airlines_silver_df (DataFrame): Dataset transformado de companhias aéreas.\n",
    "        airports_silver_df (DataFrame): Dataset transformado de aeroportos.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Dataset consolidado no formato final da camada Silver.\n",
    "    \"\"\"\n",
    "\n",
    "    log.info(\"[Aggregate] Iniciando agregação dos datasets Silver.\")\n",
    "\n",
    "    # Detecta coluna de companhia aérea no dataset 'flights'\n",
    "    if \"airline_iata_code\" in flights_silver_df.columns:\n",
    "        airline_col = \"airline_iata_code\"\n",
    "    elif \"airline\" in flights_silver_df.columns:\n",
    "        airline_col = \"airline\"\n",
    "    else:\n",
    "        raise KeyError(\"Nenhuma coluna de companhia aérea encontrada no dataset flights.\")\n",
    "\n",
    "    # Detecta colunas de origem e destino no dataset 'flights'\n",
    "    if \"origin_airport_iata_code\" in flights_silver_df.columns:\n",
    "        origin_col = \"origin_airport_iata_code\"\n",
    "    elif \"origin_airport\" in flights_silver_df.columns:\n",
    "        origin_col = \"origin_airport\"\n",
    "    else:\n",
    "        raise KeyError(\"Coluna de aeroporto de origem não encontrada.\")\n",
    "\n",
    "    if \"dest_airport_iata_code\" in flights_silver_df.columns:\n",
    "        dest_col = \"dest_airport_iata_code\"\n",
    "    elif \"destination_airport\" in flights_silver_df.columns:\n",
    "        dest_col = \"destination_airport\"\n",
    "    else:\n",
    "        raise KeyError(\"Coluna de aeroporto de destino não encontrada.\")\n",
    "\n",
    "    # Join com airlines\n",
    "    df_joined = flights_silver_df.join(\n",
    "        airlines_silver_df,\n",
    "        flights_silver_df[airline_col] == airlines_silver_df[\"airline_iata_code\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Seleciona campos para aeroportos de origem\n",
    "    df_origin = (\n",
    "        airports_silver_df.select(\n",
    "            F.col(\"airport_iata_code\").alias(\"origin_airport_iata_code\"),\n",
    "            F.col(\"airport_name\").alias(\"origin_airport_name\"),\n",
    "            F.col(\"city\").alias(\"origin_city\"),\n",
    "            F.col(\"state\").alias(\"origin_state\"),\n",
    "            F.col(\"latitude\").alias(\"origin_latitude\"),\n",
    "            F.col(\"longitude\").alias(\"origin_longitude\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Seleciona campos para aeroportos de destino\n",
    "    df_dest = (\n",
    "        airports_silver_df.select(\n",
    "            F.col(\"airport_iata_code\").alias(\"dest_airport_iata_code\"),\n",
    "            F.col(\"airport_name\").alias(\"dest_airport_name\"),\n",
    "            F.col(\"city\").alias(\"dest_city\"),\n",
    "            F.col(\"state\").alias(\"dest_state\"),\n",
    "            F.col(\"latitude\").alias(\"dest_latitude\"),\n",
    "            F.col(\"longitude\").alias(\"dest_longitude\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join com aeroportos de origem e destino\n",
    "    df_joined = (\n",
    "        df_joined.join(\n",
    "            df_origin,\n",
    "            df_joined[origin_col] == F.col(\"origin_airport_iata_code\"),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .join(\n",
    "            df_dest,\n",
    "            df_joined[dest_col] == F.col(\"dest_airport_iata_code\"),\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Mapeamento dos tipos\n",
    "    schema_casts = {\n",
    "        \"flight_id\": \"bigint\",\n",
    "        \"flight_year\": \"smallint\",\n",
    "        \"flight_month\": \"smallint\",\n",
    "        \"flight_day\": \"smallint\",\n",
    "        \"flight_day_of_week\": \"smallint\",\n",
    "        \"flight_date\": \"date\",\n",
    "\n",
    "        \"airline_iata_code\": \"string\",\n",
    "        \"airline_name\": \"string\",\n",
    "\n",
    "        \"flight_number\": \"int\",\n",
    "        \"tail_number\": \"string\",\n",
    "\n",
    "        \"origin_airport_iata_code\": \"string\",\n",
    "        \"origin_airport_name\": \"string\",\n",
    "        \"origin_city\": \"string\",\n",
    "        \"origin_state\": \"string\",\n",
    "        \"origin_latitude\": \"double\",\n",
    "        \"origin_longitude\": \"double\",\n",
    "\n",
    "        \"dest_airport_iata_code\": \"string\",\n",
    "        \"dest_airport_name\": \"string\",\n",
    "        \"dest_city\": \"string\",\n",
    "        \"dest_state\": \"string\",\n",
    "        \"dest_latitude\": \"double\",\n",
    "        \"dest_longitude\": \"double\",\n",
    "\n",
    "        \"scheduled_departure\": \"timestamp\",\n",
    "        \"departure_time\": \"timestamp\",\n",
    "        \"scheduled_arrival\": \"timestamp\",\n",
    "        \"arrival_time\": \"timestamp\",\n",
    "        \"wheels_off\": \"timestamp\",\n",
    "        \"wheels_on\": \"timestamp\",\n",
    "\n",
    "        \"departure_delay\": \"double\",\n",
    "        \"arrival_delay\": \"double\",\n",
    "        \"taxi_out\": \"double\",\n",
    "        \"taxi_in\": \"double\",\n",
    "        \"air_time\": \"double\",\n",
    "        \"elapsed_time\": \"double\",\n",
    "        \"scheduled_time\": \"double\",\n",
    "        \"distance\": \"double\",\n",
    "\n",
    "        \"is_overnight_flight\": \"boolean\",\n",
    "\n",
    "        \"air_system_delay\": \"double\",\n",
    "        \"security_delay\": \"double\",\n",
    "        \"airline_delay\": \"double\",\n",
    "        \"late_aircraft_delay\": \"double\",\n",
    "        \"weather_delay\": \"double\",\n",
    "    }\n",
    "\n",
    "    for col_name, spark_type in schema_casts.items():\n",
    "        if col_name in df_joined.columns:\n",
    "            df_joined = df_joined.withColumn(col_name, F.col(col_name).cast(spark_type))\n",
    "\n",
    "    # Seleção final conforme o ddl\n",
    "    final_df = df_joined.select(\n",
    "        \"flight_year\",\n",
    "        \"flight_month\",\n",
    "        \"flight_day\",\n",
    "        \"flight_day_of_week\",\n",
    "        \"flight_date\",\n",
    "        \"airline_iata_code\",\n",
    "        \"airline_name\",\n",
    "        \"flight_number\",\n",
    "        \"tail_number\",\n",
    "        \"origin_airport_iata_code\",\n",
    "        \"origin_airport_name\",\n",
    "        \"origin_city\",\n",
    "        \"origin_state\",\n",
    "        \"origin_latitude\",\n",
    "        \"origin_longitude\",\n",
    "        \"dest_airport_iata_code\",\n",
    "        \"dest_airport_name\",\n",
    "        \"dest_city\",\n",
    "        \"dest_state\",\n",
    "        \"dest_latitude\",\n",
    "        \"dest_longitude\",\n",
    "        \"scheduled_departure\",\n",
    "        \"departure_time\",\n",
    "        \"scheduled_arrival\",\n",
    "        \"arrival_time\",\n",
    "        \"wheels_off\",\n",
    "        \"wheels_on\",\n",
    "        \"departure_delay\",\n",
    "        \"arrival_delay\",\n",
    "        \"taxi_out\",\n",
    "        \"taxi_in\",\n",
    "        \"air_time\",\n",
    "        \"elapsed_time\",\n",
    "        \"scheduled_time\",\n",
    "        \"distance\",\n",
    "        \"air_system_delay\",\n",
    "        \"security_delay\",\n",
    "        \"airline_delay\",\n",
    "        \"late_aircraft_delay\",\n",
    "        \"weather_delay\",\n",
    "    )\n",
    "\n",
    "    # Cria pk sequencial ordenada\n",
    "    window_spec = (\n",
    "        Window\n",
    "        .orderBy(\n",
    "            F.col(\"flight_date\").asc(),\n",
    "            F.col(\"airline_iata_code\").asc(),\n",
    "            F.col(\"flight_number\").asc(),\n",
    "            F.col(\"origin_airport_iata_code\").asc(),\n",
    "            F.col(\"departure_time\").asc(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Gera pk determinística e sequencial\n",
    "    final_df = final_df.withColumn(\n",
    "        \"flight_id\",\n",
    "        F.row_number().over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Reordena colunas para a pk ser a primeira\n",
    "    final_df = final_df.select(\n",
    "        \"flight_id\",\n",
    "        *[c for c in final_df.columns if c != \"flight_id\"]\n",
    "    )\n",
    "\n",
    "    log.info(\"[Aggregate] Agregação concluída com sucesso.\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a59fb-d37e-4798-8ee4-f05d0c2d2f74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `silver_aggregate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b3585-276d-48a9-b931-052d71574d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[Aggregate] Iniciando job de agregação da camada Silver.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    source_partition: str = find_partition(\n",
    "        base_path=silver_path,\n",
    "        mode=run_mode,\n",
    "        date_str=run_date,\n",
    "    )\n",
    "    base_dir: Path = Path(silver_path) / source_partition / \"PARQUET\"\n",
    "\n",
    "    flights_path: Path  = base_dir / \"flights_pre_join.parquet\"\n",
    "    airlines_path: Path = base_dir / \"airlines.parquet\"\n",
    "    airports_path: Path = base_dir / \"airports.parquet\"\n",
    "\n",
    "    # Verifica existência dos arquivos necessários\n",
    "    for required_file in [flights_path, airlines_path, airports_path]:\n",
    "        if not required_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"[Refinement][Aggregate][ERROR] Arquivo esperado não encontrado: {required_file}\"\n",
    "            )\n",
    "\n",
    "    # Leitura dos datasets\n",
    "    log.info(\"[Aggregate] Lendo datasets silver (flights, airlines, airports).\")\n",
    "\n",
    "    df_flights  = spark.read.parquet(str(flights_path))\n",
    "    df_airlines = spark.read.parquet(str(airlines_path))\n",
    "    df_airports = spark.read.parquet(str(airports_path))\n",
    "\n",
    "    # Construção do DataFrame agregado\n",
    "    aggregated_df: DataFrame = create_aggregated_flights_df(\n",
    "        flights_silver_df=df_flights,\n",
    "        airlines_silver_df=df_airlines,\n",
    "        airports_silver_df=df_airports,\n",
    "    )\n",
    "\n",
    "    # Filtra registros inválidos de aeroportos\n",
    "    aggregated_df = aggregated_df.filter(\n",
    "        F.col(\"origin_airport_iata_code\").isNotNull()\n",
    "        & F.col(\"dest_airport_iata_code\").isNotNull()\n",
    "    )\n",
    "\n",
    "    # Quality gates\n",
    "    run_quality_gates_silver_aggregated(aggregated_df)\n",
    "\n",
    "    # Escrita do arquivo final (para debug)\n",
    "    output_path: Path = base_dir / \"flights_aggregated.parquet\"\n",
    "    aggregated_df.coalesce(1).write.mode(\"overwrite\").parquet(str(output_path))\n",
    "\n",
    "    log.info(f\"[Aggregate] Dataset agregado salvo em: {output_path}\")\n",
    "\n",
    "    # Libera cache após uso\n",
    "    aggregated_df.unpersist()\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Aggregate][ERROR] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[Aggregate] Job de agregação encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30cf775f-331b-4cf7-9e78-620cf56097eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "aggregated_df.printSchema()\n",
    "\n",
    "aggregated_df.limit(5).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4f4beb8-6e6b-4620-ac6e-30612137803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:46:26 [INFO] silver_aggregate | [Aggregate] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Aggregate] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b167fbc-1606-4441-9878-328c373d6032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 10: silver_load\n",
    "\n",
    "Este job realiza a carga do dataset agregado da camada **Silver** (`flights_aggregated.parquet`) para o PostgreSQL, populando a tabela `silver.silver_flights` conforme o ddl da camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e0ab9-fa80-492b-a841-f561b1c95d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"silver_load\")\n",
    "\n",
    "spark = get_spark_session(\"SilverLoad\")\n",
    "log.info(\"[SilverLoad] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb45d99-ebd7-4917-808f-c8db6b3f00ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `silver_load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e9ac4-740a-451b-a703-b63490d25be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[SilverLoad] Iniciando execução do job de carga.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    partition = find_partition(\n",
    "        base_path=silver_path,\n",
    "        mode=run_mode,\n",
    "        date_str=run_date,\n",
    "    )\n",
    "\n",
    "    base_dir = Path(silver_path) / partition / \"PARQUET\"\n",
    "    parquet_path = base_dir / \"flights_aggregated.parquet\"\n",
    "\n",
    "    if not parquet_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[SilverLoad][ERROR] Arquivo não encontrado: {parquet_path}.\"\n",
    "        )\n",
    "\n",
    "    log.info(f\"[SilverLoad] Lendo dataset agregado: {parquet_path}.\")\n",
    "    df = spark.read.parquet(str(parquet_path))\n",
    "\n",
    "    log.info(f\"[SilverLoad] Tuplas encontradas: {df.count():,}.\")\n",
    "\n",
    "    # Carga para PostgreSQL\n",
    "    try:\n",
    "        log.info(\"[SilverLoad] Inserindo dados em silver.silver_flights.\")\n",
    "\n",
    "        load_to_postgres(\n",
    "            df=df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"silver.silver_flights\",\n",
    "        )\n",
    "\n",
    "        log.info(\"[SilverLoad] Validando tuplas da tabela após a carga.\")\n",
    "        \n",
    "        # Valida carga\n",
    "        expected_count = df.count()\n",
    "        assert_table_rowcount(\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"silver.silver_flights\",\n",
    "            expected_count=expected_count,\n",
    "        )\n",
    "\n",
    "        log.info(\"[SilverLoad] Validação pós-carga encerrada.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[Refinement][SilverLoad][ERROR] Erro durante carga no PostgreSQL: {e}\")\n",
    "        raise\n",
    "\n",
    "    log.info(\"[SilverLoad] Carga concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[SilverLoad][ERROR] Falha na execução: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[SilverLoad] Job de carga encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c32da8a-caef-45be-92f6-cce5dbfaa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.limit(2).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70a5e5-78ce-4411-8d0c-89a480f4a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[SilverLoad] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be14c8-f036-4a3b-8a9f-990ed238dd20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Jog 11: silver_cleanup\n",
    "\n",
    "Este job remove arquivos intermediários da camada **Silver**, mantendo apenas o arquivo final `flights_aggregated.parquet` na partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f7105-b9e8-4711-a6fa-5356fd0a79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"silver_cleanup\")\n",
    "\n",
    "spark = get_spark_session(\"SilverCleanupJob\")\n",
    "log.info(\"[SilverCleanup] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a1660-fb5b-44ae-aeb7-f240533853af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `silver_cleanup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edb61d-eb10-4450-9210-07b40dc9b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[SilverCleanup] Iniciando job de limpeza da camada silver.\")\n",
    "\n",
    "    # Resolve partição e caminhos\n",
    "    partition = find_partition(\n",
    "        base_path=silver_path,\n",
    "        mode=run_mode,\n",
    "        date_str=run_date,\n",
    "    )\n",
    "    partition_dir = Path(silver_path) / partition / \"PARQUET\"\n",
    "\n",
    "    if not partition_dir.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[Refinement][SilverCleanup][ERROR] Diretório de partição não encontrado: {partition_dir}.\"\n",
    "        )\n",
    "\n",
    "    # Apenas este diretório deve permanecer (DEBUG)\n",
    "    keep = {\"flights_aggregated.parquet\"}\n",
    "\n",
    "    log.info(f\"[SilverCleanup] Diretório alvo: {partition_dir}.\")\n",
    "    #log.info(f\"[SilverCleanup] Mantendo apenas: {keep}.\")\n",
    "\n",
    "    for item in partition_dir.iterdir():\n",
    "        \"\"\"\n",
    "        # DEBUG\n",
    "        # Se for o agregado, não mexe\n",
    "        if item.name in keep:\n",
    "            log.info(f\"[SilverCleanup] Mantendo: {item}\")\n",
    "            continue\n",
    "        \"\"\"\n",
    "        # Remove diretórios intermediários\n",
    "        if item.is_dir():\n",
    "            try:\n",
    "                shutil.rmtree(item)\n",
    "                log.info(f\"[SilverCleanup] Diretório removido: {item}.\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[SilverCleanup] Falha ao remover diretório {item}: {e}.\")\n",
    "            continue\n",
    "\n",
    "        # Remove arquivos soltos\n",
    "        if item.is_file():\n",
    "            try:\n",
    "                item.unlink()\n",
    "                log.info(f\"[SilverCleanup] Arquivo removido: {item}.\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[SilverCleanup] Falha ao remover arquivo {item}: {e}.\")\n",
    "\n",
    "    log.info(\"[SilverCleanup] Limpeza concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[SilverCleanup][ERROR] Falha durante o cleanup: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[SilverCleanup] Job de limpeza da camada silver encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0b89aef-035e-4a0f-823f-f7b959694353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "partition = find_partition(silver_path, mode=run_mode, date_str=run_date)\n",
    "for item in (Path(silver_path) / partition / \"PARQUET\").iterdir():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106554bc-5ef2-47c3-a439-037568aceea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[SilverCleanup] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abd9c7-b4ef-4ac7-a10f-ea99df52d69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
