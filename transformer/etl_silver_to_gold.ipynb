{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d86d33-181e-40d2-a0fe-90c4548220af",
   "metadata": {},
   "source": [
    "# etl_silver_to_gold\n",
    "---\n",
    "Este notebook executa o processo `ETL` que transfere os dados da camada **Silver** para a **Gold**, englobando normalização, movimentação dos arquivos e carga dos dados no *PostgreSQL*, dando finalidade ao pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ebf692-724f-4183-9db8-f827aa80f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "silver_path = \"/opt/airflow/data-layer/silver\"\n",
    "gold_path = \"/opt/airflow/data-layer/gold\"\n",
    "\n",
    "aggregated_name = \"flights_aggregated.parquet\"\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90039f5-ae72-4d8b-97e4-0ba25c56ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from transformer.utils.file_io import find_partition\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres, read_from_postgres\n",
    "from transformer.utils.postgre_helpers import assert_table_rowcount\n",
    "from transformer.utils.quality_gates_gold import run_quality_gates_gold\n",
    "\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdbbf0-e811-40f5-8a53-c08b22d520e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 1: build_and_load_gold_star_schema\n",
    "\n",
    "Este job realiza a construção do esquema estrela da camada **Gold**, materializando as tabelas dimensionais e fato a partir da tabela `silver_flights`, salva os dados em formato `parquet` na camada **Gold** e carregando os dados no *PostgreSQL* de acordo com o ddl da camada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97709fef-fbb7-492c-9ebb-01d905ec42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"build_and_load_gold_star_schema\")\n",
    "\n",
    "spark = get_spark_session(\"BuildLoadGoldStarSchema\")\n",
    "log.info(\"[BuildLoad] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228db41-f5f1-4fe7-87f6-60f1faacfadf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de materialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e3b960-7ef3-4f87-a5bb-62ffed79ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def materialize_gold_layer(df: DataFrame) -> dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Materializa as tabelas dimensionais e fato da camada gold a partir do DataFrame agregado.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame consolidado da camada gold.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, DataFrame]: DataFrames correspondentes a dim_air, dim_apt, dim_dat e fat_flt.\n",
    "    \"\"\"\n",
    "    # Feriados federais nos EUA em 2015 (UTC)\n",
    "    us_holidays_2015 = [\n",
    "        \"2015-01-01\",\n",
    "        \"2015-01-19\",\n",
    "        \"2015-02-16\",\n",
    "        \"2015-05-25\",\n",
    "        \"2015-07-04\",\n",
    "        \"2015-09-07\",\n",
    "        \"2015-10-12\",\n",
    "        \"2015-11-11\",\n",
    "        \"2015-11-26\",\n",
    "        \"2015-12-25\",\n",
    "    ]\n",
    "    holidays_df = (\n",
    "        spark.createDataFrame([(d,) for d in us_holidays_2015], [\"holiday_date\"])\n",
    "            .withColumn(\"holiday_date\", F.col(\"holiday_date\").cast(DateType()))\n",
    "    )\n",
    "    \n",
    "    log.info(\"[Materialize] Iniciando materialização da camada gold.\")\n",
    "\n",
    "    # dim_air\n",
    "    log.info(\"[Materialize] Materializando 'dim_air'.\")\n",
    "    dim_air = (\n",
    "        df.select(\"airline_iata_code\", \"airline_name\")\n",
    "            .distinct()\n",
    "            .withColumn(\"airline_id\", F.monotonically_increasing_id())\n",
    "            .select(\"airline_id\", \"airline_iata_code\", \"airline_name\")\n",
    "    )\n",
    "\n",
    "    # dim_apt\n",
    "    log.info(\"[Materialize] Materializando 'dim_apt'.\")\n",
    "    dim_apt = (\n",
    "        df.select(\n",
    "            F.col(\"origin_airport_iata_code\").alias(\"airport_iata_code\"),\n",
    "            F.col(\"origin_airport_name\").alias(\"airport_name\"),\n",
    "            F.col(\"origin_city\").alias(\"city_name\"),\n",
    "            F.col(\"origin_state\").alias(\"state_code\"),\n",
    "            F.col(\"origin_latitude\").alias(\"latitude\"),\n",
    "            F.col(\"origin_longitude\").alias(\"longitude\")\n",
    "        )\n",
    "            .union(\n",
    "                df.select(\n",
    "                    F.col(\"dest_airport_iata_code\").alias(\"airport_iata_code\"),\n",
    "                    F.col(\"dest_airport_name\").alias(\"airport_name\"),\n",
    "                    F.col(\"dest_city\").alias(\"city_name\"),\n",
    "                    F.col(\"dest_state\").alias(\"state_code\"),\n",
    "                    F.col(\"dest_latitude\").alias(\"latitude\"),\n",
    "                    F.col(\"dest_longitude\").alias(\"longitude\")\n",
    "                )\n",
    "            )\n",
    "            .distinct()\n",
    "            .withColumn(\"airport_id\", F.monotonically_increasing_id())\n",
    "            .select(\"airport_id\", \"airport_iata_code\", \"airport_name\", \"city_name\",\n",
    "                    \"state_code\", \"latitude\", \"longitude\")\n",
    "    )\n",
    "\n",
    "    # dim_dat\n",
    "    log.info(\"[Materialize] Materializando 'dim_dat'.\")\n",
    "    dim_dat = (\n",
    "        df.select(F.col(\"flight_date\").alias(\"full_date\"))\n",
    "            .distinct()\n",
    "            .withColumn(\"year\", F.year(\"full_date\"))\n",
    "            .withColumn(\"month\", F.month(\"full_date\"))\n",
    "            .withColumn(\"day\", F.dayofmonth(\"full_date\"))\n",
    "            .withColumn(\"day_of_week\", F.dayofweek(\"full_date\"))\n",
    "            .withColumn(\"quarter\", F.quarter(\"full_date\"))\n",
    "            # Augmentation de feriado\n",
    "            .join(holidays_df, F.col(\"full_date\") == F.col(\"holiday_date\"), \"left\")\n",
    "            .withColumn(\"is_holiday\", F.when(F.col(\"holiday_date\").isNotNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "            .drop(\"holiday_date\")\n",
    "            # Fim do augmentation de feriado\n",
    "            .select(\"full_date\", \"year\", \"month\", \"day\", \"day_of_week\", \"quarter\", \"is_holiday\")\n",
    "    )\n",
    "\n",
    "    # fat_flt\n",
    "    log.info(\"[Materialize] Materializando 'fat_flt'.\")\n",
    "    fat_flt = (\n",
    "        df.withColumn(\"flight_id\", F.monotonically_increasing_id())\n",
    "            .withColumnRenamed(\"flight_date\", \"full_date\")\n",
    "            .select(\n",
    "                \"flight_id\", \"full_date\", \"airline_iata_code\",\n",
    "                \"origin_airport_iata_code\", \"dest_airport_iata_code\",\n",
    "                \"distance\",\n",
    "                \"air_time\", \"elapsed_time\", \"scheduled_time\",\n",
    "                \"taxi_out\", \"taxi_in\",\n",
    "                \"departure_delay\", \"arrival_delay\", \"air_system_delay\", \n",
    "                \"security_delay\", \"airline_delay\", \"late_aircraft_delay\", \"weather_delay\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Joins para adicionar fk's\n",
    "    log.info(\"[Materialize] Adicionando fk's na 'fat_flt'.\")\n",
    "    fat_flt = (\n",
    "        fat_flt\n",
    "            .join(dim_air, on=\"airline_iata_code\", how=\"left\")\n",
    "            .join(dim_apt.select(\n",
    "                F.col(\"airport_id\").alias(\"origin_airport_id\"),\n",
    "                F.col(\"airport_iata_code\").alias(\"origin_airport_iata_code\")\n",
    "            ), on=\"origin_airport_iata_code\", how=\"left\")\n",
    "            .join(dim_apt.select(\n",
    "                F.col(\"airport_id\").alias(\"dest_airport_id\"),\n",
    "                F.col(\"airport_iata_code\").alias(\"dest_airport_iata_code\")\n",
    "            ), on=\"dest_airport_iata_code\", how=\"left\")\n",
    "            .select(\n",
    "                \"flight_id\", \"full_date\", \"airline_id\", \"origin_airport_id\", \"dest_airport_id\",\n",
    "                \"distance\", \"air_time\", \"elapsed_time\", \"scheduled_time\", \"taxi_out\",\n",
    "                \"taxi_in\", \"departure_delay\", \"arrival_delay\",\n",
    "                \"air_system_delay\", \"security_delay\", \"airline_delay\",\n",
    "                \"late_aircraft_delay\", \"weather_delay\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    log.info(\"[Materialize] Materialização concluída.\")\n",
    "    \n",
    "    return {\n",
    "        \"dim_air\": dim_air,\n",
    "        \"dim_apt\": dim_apt,\n",
    "        \"dim_dat\": dim_dat,\n",
    "        \"fat_flt\": fat_flt\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b51023-792b-4e73-908f-dcd02c7cac2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `build_and_load_gold_star_schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf860a-e188-4e9d-8f11-b6875777dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[BuildLoad] Iniciando job de materialização da gold.\")\n",
    "\n",
    "    df = read_from_postgres(\n",
    "        spark=spark,\n",
    "        db_conn_id=postgres_conn_id,\n",
    "        table_name=\"silver.silver_flights\",\n",
    "    )\n",
    "\n",
    "    log.info(f\"[BuildLoad] Datasets carregado a partir do PostgreSQL.\")\n",
    "\n",
    "    # Materializando\n",
    "    tables = materialize_gold_layer(df)\n",
    "    dim_air = tables[\"dim_air\"]\n",
    "    dim_apt = tables[\"dim_apt\"]\n",
    "    dim_dat = tables[\"dim_dat\"]\n",
    "    fat_flt = tables[\"fat_flt\"]\n",
    "\n",
    "    # Executa quality gate\n",
    "    log.info(\"[BuildLoad] Iniciando quality gate.\")\n",
    "    run_quality_gates_gold(\n",
    "        dim_air=dim_air,\n",
    "        dim_apt=dim_apt,\n",
    "        dim_dat=dim_dat,\n",
    "        fat_flt=fat_flt\n",
    "    )\n",
    "    log.info(\"[BuildLoad] Quality gate concluído com sucesso.\")\n",
    "\n",
    "    # Define partição de saída\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    output_dir = Path(gold_path) / processing_date / \"PARQUET\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Escreve os arquivos na gold (para debug)\n",
    "    log.info(\"[BuildLoad] Iniciando escrita dos arquivos na camada gold.\")\n",
    "\n",
    "    dim_air.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_air.parquet\"))\n",
    "    dim_apt.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_apt.parquet\"))\n",
    "    dim_dat.write.mode(\"overwrite\").parquet(str(output_dir / \"dim_dat.parquet\"))\n",
    "    fat_flt.write.mode(\"overwrite\").parquet(str(output_dir / \"fat_flt.parquet\"))\n",
    "\n",
    "    log.info(\"[BuildLoad] Escrita concluída com sucesso.\")\n",
    "\n",
    "    log.info(\"[BuildLoad] Iniciando carga da gold.\")\n",
    "\n",
    "    tables = {\n",
    "        \"dim_air\": dim_air,\n",
    "        \"dim_apt\": dim_apt,\n",
    "        \"dim_dat\": dim_dat,\n",
    "        \"fat_flt\": fat_flt,\n",
    "    }\n",
    "\n",
    "    # Carga no PostgreSQL e validação\n",
    "    for table_name, df in tables.items():\n",
    "        full_table_name = f\"gold.{table_name}\"\n",
    "\n",
    "        log.info(f\"[BuildLoad] Carregando tabela: {full_table_name}.\")\n",
    "        expected_count = df.count()\n",
    "\n",
    "        # Carga no PostgreSQL\n",
    "        load_to_postgres(\n",
    "            df=df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=full_table_name,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "\n",
    "        log.info(f\"[BuildLoad] Tabela '{full_table_name}' carregada. Validando integridade.\")\n",
    "\n",
    "        # Validação (fallback se falhar)\n",
    "        try:\n",
    "            assert_table_rowcount(\n",
    "                db_conn_id=postgres_conn_id,\n",
    "                table_name=full_table_name,\n",
    "                expected_count=expected_count,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log.error(f\"[BuildLoad][ERROR] Validação falhou para '{full_table_name}'. Limpando tabela.\")\n",
    "\n",
    "            import psycopg2\n",
    "\n",
    "            with psycopg2.connect(\n",
    "                host=os.getenv(\"DB_HOST\", \"localhost\"),\n",
    "                dbname=os.getenv(\"DB_NAME\", \"postgres\"),\n",
    "                user=os.getenv(\"DB_USER\", \"postgres\"),\n",
    "                password=os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "            ) as conn_pg:\n",
    "                with conn_pg.cursor() as cur:\n",
    "                    cur.execute(f\"TRUNCATE TABLE {full_table_name} CASCADE;\")\n",
    "                    conn_pg.commit()\n",
    "\n",
    "            raise ValueError(f\"[BuildLoad][ERROR] Falha na validação da tabela '{full_table_name}'.\") from e\n",
    "\n",
    "        log.info(f\"[BuildLoad] Validação concluída com sucesso: {full_table_name}.\")\n",
    "\n",
    "    log.info(\"[BuildLoad] Carga de todas as tabelas concluída com sucesso.\") \n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[BuildLoad][ERROR] Falha na construção do esquema estrela: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[BuildLoad] Job de materialização da gold encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be8a5123-d9eb-4353-84ac-3fedeed12803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "\n",
    "# Verifica arquivos\n",
    "\n",
    "df_show = {\n",
    "    \"dim_air\": dim_air,\n",
    "    \"dim_apt\": dim_apt,\n",
    "    \"dim_dat\": dim_dat,\n",
    "    \"fat_flt\": fat_flt\n",
    "}\n",
    "\n",
    "for name, d in df_show.items():\n",
    "    print(f\"\\n{name}\\n\")\n",
    "    d.printSchema()\n",
    "    d.limit(1).show(truncate=True)\n",
    "\n",
    "# Verifica tabelas\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('DB_HOST', 'localhost')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME', 'postgres')}\"\n",
    "connection_properties = {\n",
    "    \"user\": os.getenv(\"DB_USER\", \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "tables_to_check = [\"dim_air\", \"dim_apt\", \"dim_dat\"]\n",
    "for tbl in tables_to_check:\n",
    "    print(f\"\\n gold.{tbl} \\n\")\n",
    "    df_check = spark.read.jdbc(url=jdbc_url, table=f\"gold.{tbl}\", properties=connection_properties)\n",
    "    df_check.limit(1).show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c38424-aa86-46e4-8f6e-65e0e532c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerrando a sessão do Spark.\n",
    "spark.stop()\n",
    "log.info(\"[BuildLoad] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b070f4-6652-4d0c-96cf-5cb8f77ac23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
