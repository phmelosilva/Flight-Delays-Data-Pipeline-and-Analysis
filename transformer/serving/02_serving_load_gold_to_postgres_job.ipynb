{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3dc074b-fbf4-4007-805e-d248618b1e20",
   "metadata": {},
   "source": [
    "# 02_serving_load_gold_to_postgres_job\n",
    "---\n",
    "Este job executa a carga das tabelas da camada **Gold** no PostgreSQL (schema `gold`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba9327-6639-475e-8004-d1fbae92b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "gold_path = \"/opt/airflow/data-layer/gold\"\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e6ca95-4b8d-4b0f-808d-3bf7d789a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:35:01 [INFO] spark_helpers | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-09 20:35:01 [INFO] file_io | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-09 20:35:01 [INFO] serving_load_gold_to_postgres | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0bb5a320-11d8-4468-86a6-635f7c244735;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 105ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0bb5a320-11d8-4468-86a6-635f7c244735\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
      "25/11/09 20:35:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-11-09 20:35:04 [INFO] spark_helpers | [INFO] SparkSession criada: 'LoadGoldToPostgres' (master=local[*])\n",
      "2025-11-09 20:35:04 [INFO] serving_load_gold_to_postgres | [Serving][Load] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres\n",
    "from transformer.utils.file_io import find_partition\n",
    "\n",
    "log = get_logger(\"serving_load_gold_to_postgres\")\n",
    "\n",
    "\n",
    "spark = get_spark_session(\"LoadGoldToPostgres\")\n",
    "log.info(\"[Serving][Load] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6ddec-f499-4ecf-9b35-16b43750dc21",
   "metadata": {},
   "source": [
    "# Define função principal do job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2762485-006e-45e4-a662-340717eed7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gold_load_job(\n",
    "        spark: SparkSession,\n",
    "        gold_path: str,\n",
    "        postgres_conn_id: str,\n",
    "        run_mode: str = \"latest\",\n",
    "        run_date: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Executa a carga das tabelas da camada Gold no schema 'gold' do PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        gold_path (str): Caminho base da camada Gold.\n",
    "        postgres_conn_id (str): ID de conexão PostgreSQL configurado no Airflow.\n",
    "        run_mode (str): 'latest' ou 'date'.\n",
    "        run_date (str | None): Data específica, se aplicável.\n",
    "    \"\"\"\n",
    "    # Localiza partição mais recente ou específica\n",
    "    partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "    partition_dir = Path(gold_path) / partition / \"PARQUET\"\n",
    "\n",
    "    if not partition_dir.exists():\n",
    "        raise FileNotFoundError(f\"[Serving][Load][ERROR] Diretório de partição não encontrado: {partition_dir}\")\n",
    "\n",
    "    log.info(f\"[Serving][Load] Partição selecionada: {partition_dir}.\")\n",
    "\n",
    "    # Lê os parquets\n",
    "    tables = {\n",
    "        \"dim_airline\": spark.read.parquet(str(partition_dir / \"dim_airline.parquet\")),\n",
    "        \"dim_airport\": spark.read.parquet(str(partition_dir / \"dim_airport.parquet\")),\n",
    "        \"dim_date\": spark.read.parquet(str(partition_dir / \"dim_date.parquet\")),\n",
    "        \"fato_flights\": spark.read.parquet(str(partition_dir / \"fato_flights.parquet\"))\n",
    "    }\n",
    "\n",
    "    # Carga no PostgreSQL\n",
    "    for table_name, df in tables.items():\n",
    "        full_table_name = f\"gold.{table_name}\"\n",
    "        log.info(f\"[Serving][Load] Carregando tabela: {full_table_name}.\")\n",
    "        load_to_postgres(df, db_conn_id=postgres_conn_id, table_name=full_table_name, mode=\"overwrite\")\n",
    "        log.info(f\"[Serving][Load] Tabela '{full_table_name}' carregada com sucesso.\")\n",
    "\n",
    "    log.info(\"[Serving][Load] Carga de todas as tabelas concluída com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7461859e-d73a-444e-b61e-93652962ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:35:09 [INFO] serving_load_gold_to_postgres | [Serving][Load] Iniciando carga para o PostgreSQL.\n",
      "2025-11-09 20:35:09 [INFO] serving_load_gold_to_postgres | [Serving][Load] Partição selecionada: /opt/airflow/data-layer/gold/2025-11-09/PARQUET.\n",
      "2025-11-09 20:35:11 [INFO] serving_load_gold_to_postgres | [Serving][Load] Carregando tabela: gold.dim_airline.\n",
      "2025-11-09 20:35:11 [WARN] spark_helpers | [WARN] Airflow indisponível, usando variáveis de ambiente para conexão PostgreSQL.\n",
      "2025-11-09 20:35:12 [INFO] spark_helpers | [INFO] Limpando tabela gold.dim_airline com TRUNCATE CASCADE...\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "2025-11-09 20:35:12 [INFO] serving_load_gold_to_postgres | [Serving] Fim do job de job de carga da gold.\n",
      "INFO:serving_load_gold_to_postgres:[Serving] Fim do job de job de carga da gold.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mrun_gold_load_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostgres_conn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      4\u001b[39m     log.exception(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Serving][ERROR] Falha durante carga no PostgreSQL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_gold_load_job\u001b[39m\u001b[34m(spark, gold_path, postgres_conn_id, run_mode, run_date)\u001b[39m\n\u001b[32m     39\u001b[39m     full_table_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgold.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m     log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Serving][Load] Carregando tabela: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43mload_to_postgres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_conn_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpostgres_conn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_table_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Serving][Load] Tabela \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m carregada com sucesso.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33m[Serving][Load] Carga de todas as tabelas concluída com sucesso.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/airflow/transformer/utils/spark_helpers.py:65\u001b[39m, in \u001b[36mload_to_postgres\u001b[39m\u001b[34m(df, db_conn_id, table_name, mode)\u001b[39m\n\u001b[32m     62\u001b[39m         mode = \u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# força append após truncar\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# --------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# agora será sempre append após truncate\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LOAD] Dados carregados em \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m com sucesso (modo final: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1984\u001b[39m, in \u001b[36mDataFrameWriter.jdbc\u001b[39m\u001b[34m(self, url, table, mode, properties)\u001b[39m\n\u001b[32m   1982\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[32m   1983\u001b[39m     jprop.setProperty(k, properties[k])\n\u001b[32m-> \u001b[39m\u001b[32m1984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[Serving][Load] Iniciando job de carga da gold.\")\n",
    "    run_gold_load_job(spark, gold_path, postgres_conn_id, run_mode, run_date)\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Serving][LOAD][ERROR] Falha durante carga no PostgreSQL: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Serving][Load] Job de carga da gold encerrado.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce9a748-13ff-4e61-827f-a26aa9f97ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se estiver em debug ou se quiser rodar a célula.\n",
    "# Vareficação manual da carga das tabelas\n",
    "import os\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('DB_HOST', 'localhost')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME', 'postgres')}\"\n",
    "connection_properties = {\n",
    "    \"user\": os.getenv(\"DB_USER\", \"postgres\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"postgres\"),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "tables_to_check = [\"dim_airline\", \"dim_airport\", \"dim_date\"]\n",
    "for tbl in tables_to_check:\n",
    "    print(f\"\\n+-------------------------------------------------------------- gold.{tbl} --------------------------------------------------------------+\")\n",
    "    df_check = spark.read.jdbc(url=jdbc_url, table=f\"gold.{tbl}\", properties=connection_properties)\n",
    "    df_check.limit(1).show(truncate=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a858c0-3f10-4407-a121-371a1785e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"SparkUI-47\"\n"
     ]
    }
   ],
   "source": [
    "# Encerrando a sessão do Spark.\n",
    "spark.stop()\n",
    "log.info(\"[Serving] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4733-8b78-4036-b996-58e11a323d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a42587-161b-4861-8c28-5603f4eacbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
