{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa3343f-48ec-4360-aaf3-d0c2010ed9bb",
   "metadata": {},
   "source": [
    "# 03_serving_cleanup_gold_job\n",
    "\n",
    "Este notebook realiza a limpeza dos arquivos temporários presentes na camada gold, removendo o dataset intermediário `flights_aggregated.parquet` gerado na transição **Silver** -> **Gold**, mantendo a organização e otimizando o espaço de armazenamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d68841-829e-4a33-9386-9c4dea8ff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "run_mode = \"latest\"\n",
    "run_date = None\n",
    "\n",
    "gold_path = \"/opt/airflow/data-layer/gold\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72eb4f6-b785-4555-8849-c8751954b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:51:18 [INFO] spark_helpers | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-09 20:51:18 [INFO] file_io | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-09 20:51:18 [INFO] serving_cleanup_gold | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9d31e674-d51c-458e-b18a-f7ce5cb63089;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 119ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9d31e674-d51c-458e-b18a-f7ce5cb63089\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "25/11/09 20:51:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 20:51:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2025-11-09 20:51:22 [INFO] spark_helpers | [INFO] SparkSession criada: 'CleanupGoldLayer' (master=local[*])\n",
      "2025-11-09 20:51:22 [INFO] serving_cleanup_gold | [Serving][Cleanup] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.spark_helpers import get_spark_session\n",
    "from transformer.utils.file_io import find_partition, delete_files\n",
    "\n",
    "log = get_logger(\"serving_cleanup_gold\")\n",
    "spark = get_spark_session(\"CleanupGoldLayer\")\n",
    "log.info(\"[Serving][Cleanup] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0367e2bc-5881-436c-8173-380b7d3f250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gold_job(spark: SparkSession, gold_path: str, run_mode: str = \"latest\", run_date: str | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Remove o arquivos temporário flights_aggregated.parquet da camada gold.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        gold_path (str): Caminho base da camada gold.\n",
    "        run_mode (str): 'latest' ou 'date'.\n",
    "        run_date (str | None): Data específica, se aplicável.\n",
    "    \"\"\"\n",
    "    log.info(\"[Serving][Cleanup] Iniciando job de limpeza da gold.\")\n",
    "\n",
    "    partition = find_partition(gold_path, mode=run_mode, date_str=run_date)\n",
    "    partition_dir = Path(gold_path) / partition / \"PARQUET\"\n",
    "    target_file = partition_dir / \"flights_aggregated.parquet\"\n",
    "\n",
    "    if not target_file.exists():\n",
    "        log.warning(f\"[Serving][Cleanup][WARN] Arquivo temporário não encontrado: {target_file}.\")\n",
    "        return\n",
    "\n",
    "    log.info(f\"[Serving][Cleanup] Removendo arquivo: {target_file}.\")\n",
    "    \n",
    "    delete_files(spark, [str(target_file)])\n",
    "    \n",
    "    log.info(\"[Serving][Cleanup] Limpeza concluída com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d255881-301e-476d-9cdc-b4e15c69be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:52:25 [INFO] serving_cleanup_gold | [Serving][Cleanup] Iniciando job de limpeza da gold.\n",
      "2025-11-09 20:52:25 [INFO] serving_cleanup_gold | [Serving][Cleanup] Removendo arquivo: /opt/airflow/data-layer/gold/2025-11-08/PARQUET/flights_aggregated.parquet.\n",
      "2025-11-09 20:52:25 [INFO] file_io | [INFO] Deletando 1 arquivo(s).\n",
      "2025-11-09 20:52:25 [INFO] file_io | [INFO] '/opt/airflow/data-layer/gold/2025-11-08/PARQUET/flights_aggregated.parquet' deletado com sucesso.\n",
      "2025-11-09 20:52:25 [INFO] file_io | [INFO] Deleção concluída.\n",
      "2025-11-09 20:52:25 [INFO] serving_cleanup_gold | [Serving][Cleanup] Limpeza concluída com sucesso.\n",
      "2025-11-09 20:52:25 [INFO] serving_cleanup_gold | [Serving][Cleanup] Fim do job de limpeza da gold.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cleanup_gold_job(spark, gold_path, run_mode, run_date)\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Serving][Cleanup][ERROR] Falha durante limpeza da camada gold: {e}.\")\n",
    "    raise\n",
    "finally:\n",
    "    log.info(\"[Serving][Cleanup] Fim do job de limpeza da gold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0db6e-5e31-47ff-ab61-e17db28599c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"[Serving][Cleanup] Encerrando sessão Spark.\")\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
