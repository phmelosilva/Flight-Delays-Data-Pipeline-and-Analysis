{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b42faca-41d7-4fe2-9e94-e077a50b7011",
   "metadata": {},
   "source": [
    "# 01_etl_stage_to_bronze\n",
    "---\n",
    "Este notebook executa o processo `ETL` que transfere os dados da camada **Stage** para a **Bronze**. Englobando verificação, padronização, conversão e movimentação dos arquivos necessários para a continuidade do pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0456f670-5d34-4ec9-a44e-48ba0b22244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "stage_path = \"/opt/airflow/data-layer/stage\"\n",
    "bronze_path = \"/opt/airflow/data-layer/bronze\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4937e319-40b1-46b2-902b-bef61ed0f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "from transformer.utils.file_io import (check_files_in_folder, delete_files, move_files)\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.quality_gates_bronze import run_quality_gates_bronze\n",
    "from transformer.utils.spark_helpers import get_spark_session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004e362-3f54-4f94-b2e7-bd8c97fc7cbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 0: kaggle_download_and_prepare\n",
    "\n",
    "Este job executa o download, a descompactação e a preparação dos arquivos para a camada Stage. Sua execução permanece desativada porque depende de configurações manuais de acesso à API do Kaggle. Reative as células e informe suas credenciais caso deseje executá-lo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eaf596e-a869-43dc-9110-d2791c4c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "log = get_logger(\"kaggle_and_prepare\")\n",
    "\n",
    "NUM_CHUNKS=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a65cb-66cb-4c6c-8d30-dc6a559578a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo funções de download e de divisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae087c7-42c8-4e42-b5dc-585d2cace192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "def download_kaggle_dataset(dataset_name: str, dest_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Faz o download e a extração de um dataset do Kaggle, caso ainda não exista localmente.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Nome do dataset no Kaggle (ex: 'usdot/flight-delays').\n",
    "        dest_path (Path): Caminho destino para os arquivos extraídos.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Para erros de autenticação ou rede.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing_files = os.listdir(dest_path)\n",
    "        has_csv_files = any(file.endswith(\".csv\") for file in existing_files)\n",
    "\n",
    "        if has_csv_files:\n",
    "            log.info(f\"[Kaggle] Arquivos já existem em '{dest_path}'. Pulando download.\")\n",
    "            return\n",
    "\n",
    "        log.info(f\"[Kaggle] Nenhum arquivo CSV encontrado. Baixando o dataset '{dataset_name}'.\")\n",
    "        kaggle.api.dataset_download_files(dataset_name, path=dest_path, unzip=True)\n",
    "        log.info(\"[Kaggle] Download e extração concluídos com sucesso.\")\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[Kaggle] Ocorreu um erro! Verifique se a API do Kaggle está configurada: {e}.\")\n",
    "        log.exception(f\"[Kaggle] Erro: {e}.\")\n",
    "        raise\n",
    "\n",
    "def split_main_file(source_file: Path, num_chunks: int = NUM_CHUNKS) -> None:\n",
    "    \"\"\"\n",
    "    Divide o arquivo flights.csv em múltiplos chunks de tamanho aproximadamente igual.\n",
    "\n",
    "    Args:\n",
    "        source_file (Path): Caminho do arquivo csv principal.\n",
    "        num_chunks (int): Número de partes a gerar.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Se o arquivo não existir.\n",
    "        ValueError: Se o arquivo estiver vazio.\n",
    "        Exception: Para erros inesperados durante a divisão.\n",
    "    \"\"\"\n",
    "    if not source_file.exists():\n",
    "        raise FileNotFoundError(f\"[Landing] Arquivo não encontrado: '{source_file}'.\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(source_file)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"O arquivo '{source_file}' está vazio.\")\n",
    "\n",
    "        log.info(f\"[Split] Dividindo '{source_file.name}' em {num_chunks} partes aproximadamente iguais.\")\n",
    "\n",
    "        chunks = np.array_split(df, num_chunks)\n",
    "\n",
    "        for i, chunk_df in enumerate(chunks, start=1):\n",
    "            chunk_name = f\"{source_file.stem}_part_{i:02d}{source_file.suffix}\"\n",
    "            chunk_path = source_file.parent / chunk_name\n",
    "            chunk_df.to_csv(chunk_path, index=False)\n",
    "            log.info(f\"[Split] Chunk salvo: '{chunk_name}', {len(chunk_df)} linhas.\")\n",
    "\n",
    "        log.info(\"[Split] Divisão do arquivo concluída com sucesso.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"[Split] Erro ao dividir o arquivo: {e}.\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830014ed-e464-4015-9071-f5b2e5d5e60e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `kaggle_download_and_prepare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940f8e02-d10d-4897-b450-46ef584afe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "try:\n",
    "    # Download\n",
    "    download_kaggle_dataset(DATASET_NAME, stage_path)\n",
    "\n",
    "    # Divisão\n",
    "    flights_file = Path(stage_path) / \"flights.csv\"\n",
    "    if flights_file.exists():\n",
    "        split_main_file(flights_file, num_chunks=NUM_CHUNKS)\n",
    "    else:\n",
    "        log.warning(f\"[Landing] Arquivo 'flights.csv' não encontrado. Verifique a pasta.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"[Landing] Falha no processo de preparação: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea78609-e3ac-4f4e-8b41-a7abc16078bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 1: unify_flight_chunks\n",
    "\n",
    "Este job realiza a unificação dos arquivos `flights_part_*.csv` presentes na camada **Stage**, consolidando-os em um único arquivo Parquet `flights.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5de6766-554f-499e-84b3-7bc0b1c5035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 02:58:51 [INFO] unify_chunks | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "/usr/local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6d17fbee-f44d-4702-9909-9a8a893c470a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.7.3!postgresql.jar (324ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.42.0/checker-qual-3.42.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.42.0!checker-qual.jar (118ms)\n",
      ":: resolution report :: resolve 4088ms :: artifacts dl 448ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6d17fbee-f44d-4702-9909-9a8a893c470a\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (1289kB/7ms)\n",
      "25/11/16 02:58:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-11-16 02:59:00 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'UnifyFlightChunks' (master=local[*]).\n",
      "2025-11-16 02:59:00 [INFO] unify_chunks | [UnifyChunks] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"unify_chunks\")\n",
    "\n",
    "spark = get_spark_session(\"UnifyFlightChunks\")\n",
    "log.info(\"[UnifyChunks] SparkSession iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0757a-969e-4626-ad16-a4dc8e12be1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de unificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6e73d2-427b-4af4-8543-9d75e9d288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassemble_chunks(spark: SparkSession, chunk_files: list[str], header: bool = True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Lê múltiplos arquivos de chunk (flights_part_*.csv) e os unifica em um único DataFrame Spark.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        chunk_files (list[str]): Lista de caminhos completos dos arquivos csv a serem unificados.\n",
    "        header (bool, opcional): Define se os arquivos csv possuem cabeçalho. Padrão: True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame Spark consolidado com todos os chunks.\n",
    "    \"\"\"\n",
    "    if not chunk_files:\n",
    "        raise ValueError(\"[UnifyChunks][ERROR] Nenhum arquivo de chunk fornecido para unificação.\")\n",
    "\n",
    "    log.info(f\"[UnifyChunks] Lendo e concatenando {len(chunk_files)} arquivo(s) de chunk.\")\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", header)\n",
    "            .option(\"inferSchema\", False)\n",
    "            .csv(chunk_files)\n",
    "        )\n",
    "        \n",
    "        log.info(\"[UnifyChunks] Unificação dos chunks concluída com sucesso.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"[UnifyChunks][ERROR] Falha ao ler os arquivos csv: {e}.\")\n",
    "        \n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d334e4-117e-4eee-95f4-fbd0ab847e69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `unify_flight_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834864ba-d698-488a-83f6-d0040f9b95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 02:59:06 [INFO] unify_chunks | [UnifyChunks] Iniciando job de unificação de chunks.\n",
      "2025-11-16 02:59:06 [INFO] file_io | [INFO] Encontrados 12 arquivo(s).\n",
      "2025-11-16 02:59:06 [INFO] unify_chunks | [UnifyChunks] Lendo e concatenando 10 arquivo(s) de chunk.\n",
      "2025-11-16 02:59:09 [INFO] unify_chunks | [UnifyChunks] Unificação dos chunks concluída com sucesso.\n",
      "2025-11-16 02:59:09 [INFO] quality_gates_bronze | [Quality][Landing] Iniciando validações do dataset 'flights_bronze'.\n",
      "2025-11-16 02:59:10 [INFO] quality_gates_bronze | [Quality][Landing]       _check_row_count_not_empty: OK\n",
      "2025-11-16 02:59:10 [INFO] quality_gates_bronze | [Quality][Landing]       _check_schema_columns: OK\n",
      "2025-11-16 02:59:10 [INFO] quality_gates_bronze | [Quality][Landing] Todas as validações para 'flights_bronze' concluídas com sucesso.\n",
      "25/11/16 02:59:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "2025-11-16 02:59:31 [INFO] unify_chunks | [UnifyChunks] Arquivo unificado salvo em: /opt/airflow/data-layer/stage/flights.parquet.\n",
      "2025-11-16 02:59:31 [INFO] unify_chunks | [UnifyChunks] Job de unificação de chunks encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[UnifyChunks] Iniciando job de unificação de chunks.\")\n",
    "\n",
    "    # Localiza arquivos csv na stage\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    chunk_files = [f for f in csv_files if \"flights_part\" in f]\n",
    "\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"[UnifyChunks][ERROR] Nenhum arquivo de chunk encontrado em {stage_path}.\")\n",
    "\n",
    "    # Unifica os chunks\n",
    "    df_unified = reassemble_chunks(spark, chunk_files)\n",
    "\n",
    "    # Executa quality gates\n",
    "    required_columns = [\n",
    "        'YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', \n",
    "        'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', \n",
    "        'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'ELAPSED_TIME', \n",
    "        'AIR_TIME', 'DISTANCE', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', \n",
    "        'ARRIVAL_DELAY', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', \n",
    "        'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY'\n",
    "    ]\n",
    "    run_quality_gates_bronze(df_unified, \"flights_bronze\", required_columns)\n",
    "\n",
    "    # Salva o arquivo unificado\n",
    "    stage_output = f\"{stage_path}/flights.parquet\"\n",
    "    df_unified.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(stage_output)\n",
    "    \n",
    "    log.info(f\"[UnifyChunks] Arquivo unificado salvo em: {stage_output}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[UnifyChunks][ERROR] Falha durante execução: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[UnifyChunks] Job de unificação de chunks encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e03e47d1-243e-47d4-822a-d0ee535cfa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:00:06 [INFO] unify_chunks | [UnifyChunks] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[UnifyChunks] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da1699-bd25-4352-b53f-851f259ae8ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 2: convert_csv_to_parquet\n",
    "\n",
    "Este job realiza a conversão dos arquivos csv presentes na camada **Stage** (exceto os chunks `flights_part_*`) para o formato **Parquet**, mantendo o schema original dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90da6783-28f1-4715-9da0-e781317ecf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:08 [INFO] convert_csv | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-16 03:02:08 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'ConvertCsvToParquet' (master=local[*]).\n",
      "2025-11-16 03:02:08 [INFO] convert_csv | [ConvertCSV] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"convert_csv\")\n",
    "\n",
    "spark = get_spark_session(\"ConvertCsvToParquet\")\n",
    "log.info(\"[ConvertCSV] SparkSession iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a05989-8d35-4ee1-8d86-e9d8234c75d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5121ac8-c584-40e7-8103-bfe7195daca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(spark: SparkSession, csv_files: list[str], stage_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converte arquivos csv em formato parquet.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        csv_files (list[str]): Lista de caminhos dos arquivos csv a converter.\n",
    "        stage_path (str): Caminho base da camada Stage.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"[ConvertCSV][ERROR] Nenhum arquivo csv fornecido para conversão.\")\n",
    "\n",
    "    for csv in csv_files:\n",
    "        try:\n",
    "            # Resolve caminhho do arquivo\n",
    "            base_name = os.path.basename(csv).replace(\".csv\", \".parquet\")\n",
    "\n",
    "            log.info(f\"[ConvertCSV] Lendo arquivo csv: {csv}.\")\n",
    "\n",
    "            df = (\n",
    "                spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", False)\n",
    "                .csv(csv)\n",
    "            )\n",
    "\n",
    "            # Executa quality gates\n",
    "            if df.columns[1] == 'AIRLINE':\n",
    "                required_columns = ['IATA_CODE', 'AIRLINE']\n",
    "            else: \n",
    "                required_columns = ['IATA_CODE', 'AIRPORT', 'CITY', 'STATE', 'COUNTRY', 'LATITUDE', 'LONGITUDE']\n",
    "            \n",
    "            run_quality_gates_bronze(df, base_name, required_columns)\n",
    "\n",
    "            # Converte em parquet\n",
    "            parquet_path = f\"{stage_path}/{base_name}\"\n",
    "            df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "            \n",
    "            log.info(f\"[ConvertCSV] Arquivo convertido: {parquet_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"[ConvertCSV][ERROR] Falha ao converter {csv}: {e}.\")\n",
    "            raise IOError(f\"Erro ao processar {csv}: {e}.\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a41db8-c7b9-42c4-83e1-5cd8d9cab4f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `convert_csv_to_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c702b265-9f6d-4725-bdfe-04f09283d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:15 [INFO] convert_csv | [ConvertCSV] Iniciando job de conversão de csv para parquet.\n",
      "2025-11-16 03:02:15 [INFO] file_io | [INFO] Encontrados 12 arquivo(s).\n",
      "2025-11-16 03:02:15 [INFO] convert_csv | [ConvertCSV] Lendo arquivo csv: /opt/airflow/data-layer/stage/airlines.csv.\n",
      "2025-11-16 03:02:15 [INFO] quality_gates_bronze | [Quality][Landing] Iniciando validações do dataset 'airlines.parquet'.\n",
      "2025-11-16 03:02:16 [INFO] quality_gates_bronze | [Quality][Landing]       _check_row_count_not_empty: OK\n",
      "2025-11-16 03:02:16 [INFO] quality_gates_bronze | [Quality][Landing]       _check_schema_columns: OK\n",
      "2025-11-16 03:02:16 [INFO] quality_gates_bronze | [Quality][Landing] Todas as validações para 'airlines.parquet' concluídas com sucesso.\n",
      "2025-11-16 03:02:16 [INFO] convert_csv | [ConvertCSV] Arquivo convertido: /opt/airflow/data-layer/stage/airlines.parquet.\n",
      "2025-11-16 03:02:16 [INFO] convert_csv | [ConvertCSV] Lendo arquivo csv: /opt/airflow/data-layer/stage/airports.csv.\n",
      "2025-11-16 03:02:17 [INFO] quality_gates_bronze | [Quality][Landing] Iniciando validações do dataset 'airports.parquet'.\n",
      "2025-11-16 03:02:17 [INFO] quality_gates_bronze | [Quality][Landing]       _check_row_count_not_empty: OK\n",
      "2025-11-16 03:02:17 [INFO] quality_gates_bronze | [Quality][Landing]       _check_schema_columns: OK\n",
      "2025-11-16 03:02:17 [INFO] quality_gates_bronze | [Quality][Landing] Todas as validações para 'airports.parquet' concluídas com sucesso.\n",
      "2025-11-16 03:02:17 [INFO] convert_csv | [ConvertCSV] Arquivo convertido: /opt/airflow/data-layer/stage/airports.parquet.\n",
      "2025-11-16 03:02:17 [INFO] convert_csv | [ConvertCSV] Conversão concluída. 2 arquivo(s) processado(s).\n",
      "2025-11-16 03:02:17 [INFO] convert_csv | [ConvertCSV] Job de conversão encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[ConvertCSV] Iniciando job de conversão de csv para parquet.\")\n",
    "\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    target_files = [f for f in csv_files if \"flights_part\" not in f]\n",
    "\n",
    "    if not target_files:\n",
    "        raise FileNotFoundError(f\"[Landing][ConvertCSV][ERROR] Nenhum arquivo csv com o padrão encontrado em {stage_path}.\")\n",
    "\n",
    "    convert_csv_to_parquet(spark, target_files, stage_path)\n",
    "\n",
    "    log.info(f\"[ConvertCSV] Conversão concluída. {len(target_files)} arquivo(s) processado(s).\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[ConvertCSV][ERROR] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[ConvertCSV] Job de conversão encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4844540b-97aa-499a-a8e0-985fdac38042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:25 [INFO] convert_csv | [ConvertCSV] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[ConvertCSV] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda392b-5a2b-4b68-ada0-2762b67dd1c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 3: move_files_to_bronze\n",
    "\n",
    "Este job move os arquivos `parquet` da camada **Stage** para a camada **Bronze**, organizando-os por data de processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24a3bd8f-440e-49e0-935f-1b1482fc138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:36 [INFO] move_to_bronze | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-16 03:02:36 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'MoveStageToBronze' (master=local[*]).\n",
      "2025-11-16 03:02:36 [INFO] move_to_bronze | [MoveToBronze] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"move_to_bronze\")\n",
    "\n",
    "spark = get_spark_session(\"MoveStageToBronze\")\n",
    "log.info(\"[MoveToBronze] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8c8fc-4e96-4463-a77a-a09458085bfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `move_files_to_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dca82fb6-bab2-4a20-b3dc-a2fc73b759c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:40 [INFO] move_to_bronze | [MoveToBronze] Iniciando job de movimentação de arquivos.\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] Encontrados 3 arquivo(s).\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] Movendo arquivos para '/opt/airflow/data-layer/bronze'.\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] Diretório criado: /opt/airflow/data-layer/bronze/2025-11-16/PARQUET\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] 'airlines.parquet' movido para '/opt/airflow/data-layer/bronze/2025-11-16/PARQUET/airlines.parquet'.\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] 'airports.parquet' movido para '/opt/airflow/data-layer/bronze/2025-11-16/PARQUET/airports.parquet'.\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] 'flights.parquet' movido para '/opt/airflow/data-layer/bronze/2025-11-16/PARQUET/flights.parquet'.\n",
      "2025-11-16 03:02:40 [INFO] file_io | [INFO] Movimentação concluída com sucesso.\n",
      "2025-11-16 03:02:40 [INFO] move_to_bronze | [MoveToBronze] 3 arquivo(s) movido(s) para bronze/2025-11-16.\n",
      "2025-11-16 03:02:40 [INFO] move_to_bronze | [MoveToBronze] Job de movimentação encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[MoveToBronze] Iniciando job de movimentação de arquivos.\")\n",
    "\n",
    "    parquet_files = check_files_in_folder(stage_path, \"*.parquet\")\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"[MoveToBronze][ERROR] Nenhum arquivo Parquet encontrado em {stage_path}.\")\n",
    "\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    move_files(\n",
    "        spark=spark,\n",
    "        source_files=parquet_files,\n",
    "        base_dest_path=bronze_path,\n",
    "        processing_date=processing_date,\n",
    "    )\n",
    "\n",
    "    log.info(f\"[MoveToBronze] {len(parquet_files)} arquivo(s) movido(s) para bronze/{processing_date}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Landing][MoveToBronze][ERROR] Falha durante execução: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[MoveToBronze] Job de movimentação encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c257c7c-b07b-4fb9-b410-de59267e63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:46 [INFO] move_to_bronze | [MoveToBronze] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[MoveToBronze] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b1603-ee4e-4dac-8ec6-e2e371770c49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 4: cleanup_stage\n",
    "\n",
    "Este job remove os arquivos `csv` e `parquet` da camada **Stage** após a conclusão do carregamento na camada **Bronze**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42320064-44e6-4bbf-99c1-9172e94d1c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:54 [INFO] cleanup_stage | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-16 03:02:54 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'CleanupStage' (master=local[*]).\n",
      "2025-11-16 03:02:54 [INFO] cleanup_stage | [CleanupStage] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "log = get_logger(\"cleanup_stage\")\n",
    "\n",
    "spark = get_spark_session(\"CleanupStage\")\n",
    "log.info(\"[CleanupStage] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf7bdb-f11f-4202-aeaf-5a0e43a03bbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `cleanup_stage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98ba326f-acf7-4aeb-9575-f115df0c17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:02:57 [INFO] cleanup_stage | [CleanupStage] Iniciando job de limpeza da stage.\n",
      "2025-11-16 03:02:57 [INFO] file_io | [INFO] Encontrados 12 arquivo(s).\n",
      "2025-11-16 03:02:57 [INFO] file_io | [INFO] Deletando 12 arquivo(s).\n",
      "2025-11-16 03:02:57 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/airlines.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/airports.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_01.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_02.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_03.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_04.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_05.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_06.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_07.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_08.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_09.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] '/opt/airflow/data-layer/stage/flights_part_10.csv' deletado com sucesso.\n",
      "2025-11-16 03:02:58 [INFO] file_io | [INFO] Deleção concluída.\n",
      "2025-11-16 03:02:58 [INFO] cleanup_stage | [CleanupStage] 12 arquivo(s) removido(s) da stage.\n",
      "2025-11-16 03:02:58 [INFO] cleanup_stage | [CleanupStage] Job de limpeza encerrado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[CleanupStage] Iniciando job de limpeza da stage.\")\n",
    "\n",
    "    removable_files = check_files_in_folder(stage_path, \"*.*\")\n",
    "    if not removable_files:\n",
    "        log.warning(\"[CleanupStage] Nenhum arquivo encontrado para remoção.\")\n",
    "    else:\n",
    "        delete_files(spark, removable_files)\n",
    "        log.info(f\"[CleanupStage] {len(removable_files)} arquivo(s) removido(s) da stage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[CleanupStage][ERROR] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[CleanupStage] Job de limpeza encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29b3278b-e9c5-45e1-939a-4d9a937e8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 03:03:05 [INFO] cleanup_stage | [CleanupStage] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[CleanupStage] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005957e-e9df-4d52-9dc0-269cd0102e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe3dc6-ad3a-49d9-b57b-1e4c580c5180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602d2b2-f867-4157-955a-6dd1ec78cb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
