{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07186991-d0c4-496b-963e-2c898de77004",
   "metadata": {},
   "source": [
    "# 03_landing_convert_csv_to_parquet_job\n",
    "---\n",
    "Este notebook realiza a conversão dos arquivos csv presentes na camada **Stage** (exceto os chunks `flights_part_*`) para o formato **Parquet**, mantendo o schema original dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbf289a-9587-43cf-9570-094ad255198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "stage_path = \"/opt/airflow/data-layer/stage\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b797e4e-7504-4086-832b-694d8843f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 00:03:37 [INFO] landing.convert_csv | [INFO] Logger inicializado no modo standalone (INFO).\n",
      "2025-11-12 00:03:38 [INFO] spark_helpers | [INFO] SparkSession criada com sucesso: 'ConvertCsvToParquet' (master=local[*]).\n",
      "2025-11-12 00:03:38 [INFO] landing.convert_csv | [Landing][ConvertCSV] SparkSession iniciada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from transformer.utils.spark_helpers import get_spark_session\n",
    "from transformer.utils.file_io import check_files_in_folder\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.validation.quality_gates_bronze import run_quality_gates_bronze\n",
    "\n",
    "log = get_logger(\"landing.convert_csv\")\n",
    "\n",
    "spark = get_spark_session(\"ConvertCsvToParquet\")\n",
    "log.info(\"[Landing][ConvertCSV] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7eb35-5403-42de-a4e8-3bb2e08a0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(spark: SparkSession, csv_files: list[str], stage_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converte arquivos csv em formato parquet.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        csv_files (list[str]): Lista de caminhos dos arquivos csv a converter.\n",
    "        stage_path (str): Caminho base da camada Stage.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"[Landing][ConvertCSV][ERROR] Nenhum arquivo csv fornecido para conversão.\")\n",
    "\n",
    "    for csv in csv_files:\n",
    "        try:\n",
    "            base_name = os.path.basename(csv).replace(\".csv\", \".parquet\")\n",
    "            log.info(f\"[Landing][ConvertCSV] Lendo arquivo csv: {csv}.\")\n",
    "\n",
    "            # Leitura do CSV\n",
    "            df = (\n",
    "                spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", False)\n",
    "                .csv(csv)\n",
    "            )\n",
    "\n",
    "            # Validação de qualidade\n",
    "            if df.columns[1] == 'AIRLINE':\n",
    "                required_columns = ['IATA_CODE', 'AIRLINE']\n",
    "            else: \n",
    "                required_columns = ['IATA_CODE', 'AIRPORT', 'CITY', 'STATE', 'COUNTRY', 'LATITUDE', 'LONGITUDE']\n",
    "            run_quality_gates_bronze(df, base_name, required_columns)\n",
    "\n",
    "            # Escrita em parquet\n",
    "            parquet_path = f\"{stage_path}/{base_name}\"\n",
    "            df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "            log.info(f\"[Landing][ConvertCSV] Arquivo convertido: {parquet_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"[Landing][ConvertCSV][ERROR] Falha ao converter {csv}: {e}\")\n",
    "\n",
    "            raise IOError(f\"Erro ao processar {csv}: {e}.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31dab808-ae5e-43f6-8cbb-ca23293cc726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 00:03:44 [INFO] landing.convert_csv | [Landing][ConvertCSV] Iniciando job de conversão csv -> parquet.\n",
      "2025-11-12 00:03:44 [INFO] file_io | [INFO] Encontrados 12 arquivo(s).\n",
      "2025-11-12 00:03:44 [INFO] landing.convert_csv | [Landing][ConvertCSV] Lendo arquivo csv: /opt/airflow/data-layer/stage/airlines.csv.\n",
      "2025-11-12 00:03:44 [INFO] quality_gates_bronze | [Quality][Landing] Iniciando validações do dataset 'airlines.parquet'.\n",
      "2025-11-12 00:03:45 [INFO] quality_gates_bronze | [Quality][Landing] _check_row_count_not_empty para 'airlines.parquet': OK.\n",
      "2025-11-12 00:03:45 [INFO] quality_gates_bronze | [Quality][Landing] _check_schema_columns para 'airlines.parquet': OK.\n",
      "2025-11-12 00:03:45 [INFO] quality_gates_bronze | [Quality][Landing] Todas as validações para 'airlines.parquet' concluídas com sucesso.\n",
      "2025-11-12 00:03:46 [INFO] spark_helpers | [INFO] Dataframe salvo com sucesso em '/opt/airflow/data-layer/stage/airlines.parquet'.\n",
      "2025-11-12 00:03:46 [INFO] landing.convert_csv | [Landing][ConvertCSV] Arquivo convertido: /opt/airflow/data-layer/stage/airlines.parquet.\n",
      "2025-11-12 00:03:46 [INFO] landing.convert_csv | [Landing][ConvertCSV] Lendo arquivo csv: /opt/airflow/data-layer/stage/airports.csv.\n",
      "2025-11-12 00:03:46 [INFO] quality_gates_bronze | [Quality][Landing] Iniciando validações do dataset 'airports.parquet'.\n",
      "2025-11-12 00:03:46 [INFO] quality_gates_bronze | [Quality][Landing] _check_row_count_not_empty para 'airports.parquet': OK.\n",
      "2025-11-12 00:03:46 [INFO] quality_gates_bronze | [Quality][Landing] _check_schema_columns para 'airports.parquet': OK.\n",
      "2025-11-12 00:03:46 [INFO] quality_gates_bronze | [Quality][Landing] Todas as validações para 'airports.parquet' concluídas com sucesso.\n",
      "2025-11-12 00:03:47 [INFO] spark_helpers | [INFO] Dataframe salvo com sucesso em '/opt/airflow/data-layer/stage/airports.parquet'.\n",
      "2025-11-12 00:03:47 [INFO] landing.convert_csv | [Landing][ConvertCSV] Arquivo convertido: /opt/airflow/data-layer/stage/airports.parquet.\n",
      "2025-11-12 00:03:47 [INFO] landing.convert_csv | [Landing][ConvertCSV] Conversão concluída. 2 arquivo(s) processado(s).\n",
      "2025-11-12 00:03:47 [INFO] landing.convert_csv | [Landing][ConvertCSV] Job concluído com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log.info(\"[Landing][ConvertCSV] Iniciando job de conversão csv -> parquet.\")\n",
    "\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    target_files = [f for f in csv_files if \"flights_part\" not in f]\n",
    "\n",
    "    if not target_files:\n",
    "        raise FileNotFoundError(f\"[Landing][ConvertCSV][ERROR] Nenhum arquivo csv com o padrão encontrado em {stage_path}.\")\n",
    "\n",
    "    convert_csv_to_parquet(spark, target_files, stage_path)\n",
    "\n",
    "    log.info(f\"[Landing][ConvertCSV] Conversão concluída. {len(target_files)} arquivo(s) processado(s).\")\n",
    "\n",
    "    log.info(\"[Landing][ConvertCSV] Job concluído com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Landing][ConvertCSV][ERROR] Falha durante execução: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c2df38-9acb-4e27-90c2-fe90311ffb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 00:03:52 [INFO] landing.convert_csv | [Landing][ConvertCSV] Sessão Spark finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Encerrando a sessão Spark\n",
    "spark.stop()\n",
    "log.info(\"[Landing][ConvertCSV] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5d5fb-def8-4a91-a870-8688b41f7685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
