{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b42faca-41d7-4fe2-9e94-e077a50b7011",
   "metadata": {},
   "source": [
    "# 01_etl_stage_to_raw\n",
    "---\n",
    "Este notebook executa a preparação do projeto para o processo `ETL`, transferindo os dados da pasta **Stage** para a **Raw**. Englobando o download (opcional), carga de dados, conversão e movimentação dos arquivos necessários para a continuidade do pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0456f670-5d34-4ec9-a44e-48ba0b22244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "stage_path = \"/opt/airflow/data-layer/raw/stage\"\n",
    "raw_path = \"/opt/airflow/data-layer/raw\"\n",
    "\n",
    "postgres_conn_id = \"AIRFLOW_VAR_POSTGRES_CONN_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937e319-40b1-46b2-902b-bef61ed0f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "from transformer.utils.file_io import check_files_in_folder, delete_files, move_files\n",
    "from transformer.utils.logger import get_logger\n",
    "from transformer.utils.quality_gates_raw import run_quality_gates_raw\n",
    "from transformer.utils.spark_helpers import get_spark_session, load_to_postgres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004e362-3f54-4f94-b2e7-bd8c97fc7cbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 0: kaggle_download_and_prepare\n",
    "\n",
    "Este job executa o download, a descompactação e a preparação dos arquivos para a camada Stage. Sua execução permanece desativada porque depende de configurações manuais de acesso à API do Kaggle. Reative as células e informe suas credenciais caso deseje executá-lo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eaf596e-a869-43dc-9110-d2791c4c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "log = get_logger(\"kaggle_and_prepare\")\n",
    "\n",
    "NUM_CHUNKS=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a65cb-66cb-4c6c-8d30-dc6a559578a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo funções de download e de divisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae087c7-42c8-4e42-b5dc-585d2cace192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "def download_kaggle_dataset(dataset_name: str, dest_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Faz o download e a extração de um dataset do Kaggle, caso ainda não exista localmente.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Nome do dataset no Kaggle (ex: 'usdot/flight-delays').\n",
    "        dest_path (Path): Caminho destino para os arquivos extraídos.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Para erros de autenticação ou rede.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing_files = os.listdir(dest_path)\n",
    "        has_csv_files = any(file.endswith(\".csv\") for file in existing_files)\n",
    "\n",
    "        if has_csv_files:\n",
    "            log.info(f\"[Kaggle] Arquivos já existem em '{dest_path}'. Pulando download.\")\n",
    "            return\n",
    "\n",
    "        log.info(f\"[Kaggle] Nenhum arquivo CSV encontrado. Baixando o dataset '{dataset_name}'.\")\n",
    "        kaggle.api.dataset_download_files(dataset_name, path=dest_path, unzip=True)\n",
    "        log.info(\"[Kaggle] Download e extração concluídos com sucesso.\")\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[Kaggle] Ocorreu um erro! Verifique se a API do Kaggle está configurada: {e}.\")\n",
    "        log.exception(f\"[Kaggle] Erro: {e}.\")\n",
    "        raise\n",
    "\n",
    "def split_main_file(source_file: Path, num_chunks: int = NUM_CHUNKS) -> None:\n",
    "    \"\"\"\n",
    "    Divide o arquivo flights.csv em múltiplos chunks de tamanho aproximadamente igual.\n",
    "\n",
    "    Args:\n",
    "        source_file (Path): Caminho do arquivo csv principal.\n",
    "        num_chunks (int): Número de partes a gerar.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Se o arquivo não existir.\n",
    "        ValueError: Se o arquivo estiver vazio.\n",
    "        Exception: Para erros inesperados durante a divisão.\n",
    "    \"\"\"\n",
    "    if not source_file.exists():\n",
    "        raise FileNotFoundError(f\"[Landing] Arquivo não encontrado: '{source_file}'.\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(source_file)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"O arquivo '{source_file}' está vazio.\")\n",
    "\n",
    "        log.info(f\"[Split] Dividindo '{source_file.name}' em {num_chunks} partes aproximadamente iguais.\")\n",
    "\n",
    "        chunks = np.array_split(df, num_chunks)\n",
    "\n",
    "        for i, chunk_df in enumerate(chunks, start=1):\n",
    "            chunk_name = f\"{source_file.stem}_part_{i:02d}{source_file.suffix}\"\n",
    "            chunk_path = source_file.parent / chunk_name\n",
    "            chunk_df.to_csv(chunk_path, index=False)\n",
    "            log.info(f\"[Split] Chunk salvo: '{chunk_name}', {len(chunk_df)} linhas.\")\n",
    "\n",
    "        log.info(\"[Split] Divisão do arquivo concluída com sucesso.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"[Split] Erro ao dividir o arquivo: {e}.\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830014ed-e464-4015-9071-f5b2e5d5e60e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `kaggle_download_and_prepare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940f8e02-d10d-4897-b450-46ef584afe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Comentar essa linha se quiser rodar a célula.\n",
    "try:\n",
    "    # Download\n",
    "    download_kaggle_dataset(DATASET_NAME, stage_path)\n",
    "\n",
    "    # Divisão\n",
    "    flights_file = Path(stage_path) / \"flights.csv\"\n",
    "    if flights_file.exists():\n",
    "        split_main_file(flights_file, num_chunks=NUM_CHUNKS)\n",
    "    else:\n",
    "        log.warning(f\"[Kaggle] Arquivo 'flights.csv' não encontrado. Verifique a pasta.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"[Kaggle] Falha no processo de preparação: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe022cd-fe80-4382-82d3-a2d747d0e866",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 1: load_raw_dbt_data\n",
    "\n",
    "Este job carrega os **dados brutos** dos arquivos `airlines.csv`, `airports.csv` e `flights_part_1.csv` da pasta **Stage** para o schema `dbt_raw`, que servirá de base para as transformações realizadas pelo dbt. A opção por carregar apenas o arquivo `flights_part_1.csv` na tabela de voos foi tomada por motivos de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a0f11-8870-4c4b-8b1e-5da0979ef364",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"load_dbt_data\")\n",
    "spark = get_spark_session(\"load_dbt_data\")\n",
    "\n",
    "log.info(\"[dbt][Raw] Sessão Spark iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95df5a7-89a5-4aa6-9764-bece2093b703",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `load_dbt_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca6a5a-136c-4945-8090-8bc2beeb910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[dbt][Raw] Iniciando job de carga dos dados para o dbt.\")\n",
    "\n",
    "    read_options = {\n",
    "        \"header\": \"true\",\n",
    "        \"inferSchema\": \"false\",\n",
    "        \"delimiter\": \",\",\n",
    "    }\n",
    "\n",
    "    # Carregar airlines.csv (RAW via Spark)\n",
    "    try:\n",
    "        airlines_path = str(Path(stage_path) / \"airlines.csv\")\n",
    "\n",
    "        log.info(f\"[dbt][Raw] Lendo airlines: {airlines_path}.\")\n",
    "\n",
    "        airlines_df = spark.read.options(**read_options).csv(airlines_path)\n",
    "\n",
    "        log.info(\"[dbt][Raw] Inserindo dados em 'dbt_raw.airlines'.\")\n",
    "        load_to_postgres(\n",
    "            df=airlines_df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"dbt_raw.airlines\",\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[dbt][Raw][Error] Falha ao carregar airlines: {e}.\")\n",
    "        raise\n",
    "\n",
    "    # Carregar airports.csv\n",
    "    try:\n",
    "        airports_path = str(Path(stage_path) / \"airports.csv\")\n",
    "\n",
    "        log.info(f\"[dbt][Raw] Lendo airports: {airports_path}.\")\n",
    "\n",
    "        airports_df = spark.read.options(**read_options).csv(airports_path)\n",
    "\n",
    "        log.info(\"[dbt][Raw] Inserindo dados em 'dbt_raw.airports'.\")\n",
    "        load_to_postgres(\n",
    "            df=airports_df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"dbt_raw.airports\",\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[dbt][Raw][Error] Falha ao carregar airports: {e}.\")\n",
    "        raise\n",
    "\n",
    "    # Carregar flights_part_03.csv\n",
    "    try:\n",
    "        flights_chunk_path = str(Path(stage_path) / \"flights_part_01.csv\")\n",
    "\n",
    "        log.info(f\"[dbt][Raw] Lendo chunk de flights: {flights_chunk_path}.\")\n",
    "\n",
    "        flights_df = spark.read.options(**read_options).csv(flights_chunk_path)\n",
    "\n",
    "        log.info(\"[dbt][Raw] Inserindo dados em 'dbt_raw.flights'.\")\n",
    "        load_to_postgres(\n",
    "            df=flights_df,\n",
    "            db_conn_id=postgres_conn_id,\n",
    "            table_name=\"dbt_raw.flights\",\n",
    "            mode=\"overwrite\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"[dbt][Raw][Error] Falha ao carregar flights: {e}.\")\n",
    "        raise\n",
    "\n",
    "    log.info(\"[dbt][Raw] Carga concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[dbt][Raw][Error] Falha na execução do job: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[dbt][Raw] Job de carga dos dados para o dbt encerrado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea78609-e3ac-4f4e-8b41-a7abc16078bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 2: unify_flight_chunks\n",
    "\n",
    "Este job realiza a unificação dos arquivos `flights_part_*.csv` presentes na pasta **Stage**, consolidando-os em um único arquivo Parquet `flights.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de6766-554f-499e-84b3-7bc0b1c5035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"unify_chunks\")\n",
    "\n",
    "spark = get_spark_session(\"UnifyFlightChunks\")\n",
    "log.info(\"[UnifyChunks] SparkSession iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0757a-969e-4626-ad16-a4dc8e12be1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de unificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6e73d2-427b-4af4-8543-9d75e9d288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassemble_chunks(spark: SparkSession, chunk_files: list[str], header: bool = True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Lê múltiplos arquivos de chunk (flights_part_*.csv) e os unifica em um único DataFrame Spark.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        chunk_files (list[str]): Lista de caminhos completos dos arquivos csv a serem unificados.\n",
    "        header (bool, opcional): Define se os arquivos csv possuem cabeçalho. Padrão: True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame Spark consolidado com todos os chunks.\n",
    "    \"\"\"\n",
    "    if not chunk_files:\n",
    "        raise ValueError(\"[UnifyChunks][Error] Nenhum arquivo de chunk fornecido para unificação.\")\n",
    "\n",
    "    log.info(f\"[UnifyChunks] Lendo e concatenando {len(chunk_files)} arquivo(s) de chunk.\")\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", header)\n",
    "            .option(\"inferSchema\", False)\n",
    "            .csv(chunk_files)\n",
    "        )\n",
    "        \n",
    "        log.info(\"[UnifyChunks] Unificação dos chunks concluída com sucesso.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"[UnifyChunks][Error] Falha ao ler os arquivos csv: {e}.\")\n",
    "        \n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d334e4-117e-4eee-95f4-fbd0ab847e69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `unify_flight_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834864ba-d698-488a-83f6-d0040f9b95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[UnifyChunks] Iniciando job de unificação de chunks.\")\n",
    "\n",
    "    # Localiza arquivos csv na stage\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    chunk_files = [f for f in csv_files if \"flights_part\" in f]\n",
    "\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(f\"[UnifyChunks][Error] Nenhum arquivo de chunk encontrado em {stage_path}.\")\n",
    "\n",
    "    # Unifica os chunks\n",
    "    df_unified = reassemble_chunks(spark, chunk_files)\n",
    "\n",
    "    # Executa quality gates\n",
    "    required_columns = [\n",
    "        'YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', \n",
    "        'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', \n",
    "        'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'SCHEDULED_TIME', 'ELAPSED_TIME', \n",
    "        'AIR_TIME', 'DISTANCE', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', \n",
    "        'ARRIVAL_DELAY', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', \n",
    "        'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY'\n",
    "    ]\n",
    "    run_quality_gates_raw(df_unified, \"raw_flights\", required_columns)\n",
    "\n",
    "    # Salva o arquivo unificado\n",
    "    stage_output = f\"{stage_path}/flights.parquet\"\n",
    "    df_unified.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(stage_output)\n",
    "    \n",
    "    log.info(f\"[UnifyChunks] Arquivo unificado salvo em: {stage_output}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[UnifyChunks][Error] Falha durante execução: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[UnifyChunks] Job de unificação de chunks encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e47d1-243e-47d4-822a-d0ee535cfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[UnifyChunks] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da1699-bd25-4352-b53f-851f259ae8ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 3: convert_csv_to_parquet\n",
    "\n",
    "Este job realiza a conversão dos arquivos csv presentes na pasta **Stage** (exceto os chunks `flights_part_*`) para o formato **parquet**, mantendo o schema original dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da6783-28f1-4715-9da0-e781317ecf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"convert_csv\")\n",
    "\n",
    "spark = get_spark_session(\"ConvertCsvToParquet\")\n",
    "log.info(\"[ConvertCSV] SparkSession iniciada.\")\n",
    "\n",
    "# Ajustes de performance para o Spark\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a05989-8d35-4ee1-8d86-e9d8234c75d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definindo função de conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5121ac8-c584-40e7-8103-bfe7195daca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(spark: SparkSession, csv_files: list[str], stage_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converte arquivos csv em formato parquet.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Sessão Spark ativa.\n",
    "        csv_files (list[str]): Lista de caminhos dos arquivos csv a converter.\n",
    "        stage_path (str): Caminho base da camada Stage.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"[ConvertCSV][Error] Nenhum arquivo csv fornecido para conversão.\")\n",
    "\n",
    "    for csv in csv_files:\n",
    "        try:\n",
    "            # Resolve caminhho do arquivo\n",
    "            base_name = os.path.basename(csv).replace(\".csv\", \".parquet\")\n",
    "\n",
    "            log.info(f\"[ConvertCSV] Lendo arquivo csv: {csv}.\")\n",
    "\n",
    "            df = (\n",
    "                spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", False)\n",
    "                .csv(csv)\n",
    "            )\n",
    "\n",
    "            # Executa quality gates\n",
    "            if df.columns[1] == 'AIRLINE':\n",
    "                required_columns = ['IATA_CODE', 'AIRLINE']\n",
    "            else: \n",
    "                required_columns = ['IATA_CODE', 'AIRPORT', 'CITY', 'STATE', 'COUNTRY', 'LATITUDE', 'LONGITUDE']\n",
    "            \n",
    "            run_quality_gates_raw(df, base_name, required_columns)\n",
    "\n",
    "            # Converte em parquet\n",
    "            parquet_path = f\"{stage_path}/{base_name}\"\n",
    "            df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(parquet_path)\n",
    "            \n",
    "            log.info(f\"[ConvertCSV] Arquivo convertido: {parquet_path}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"[ConvertCSV][ERROR] Falha ao converter {csv}: {e}.\")\n",
    "            raise IOError(f\"Erro ao processar {csv}: {e}.\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a41db8-c7b9-42c4-83e1-5cd8d9cab4f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `convert_csv_to_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702b265-9f6d-4725-bdfe-04f09283d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[ConvertCSV] Iniciando job de conversão de csv para parquet.\")\n",
    "\n",
    "    csv_files = check_files_in_folder(stage_path, \"*.csv\")\n",
    "    target_files = [f for f in csv_files if \"flights_part\" not in f]\n",
    "\n",
    "    if not target_files:\n",
    "        raise FileNotFoundError(f\"[ConvertCSV][Error] Nenhum arquivo csv com o padrão encontrado em {stage_path}.\")\n",
    "\n",
    "    convert_csv_to_parquet(spark, target_files, stage_path)\n",
    "\n",
    "    log.info(f\"[ConvertCSV] Conversão concluída. {len(target_files)} arquivo(s) processado(s).\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[ConvertCSV][Error] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[ConvertCSV] Job de conversão encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844540b-97aa-499a-a8e0-985fdac38042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[ConvertCSV] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda392b-5a2b-4b68-ada0-2762b67dd1c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 4: move_files_to_raw\n",
    "\n",
    "Este job move os arquivos `parquet` da pasta **Stage** para a camada **Raw**, organizando-os por data de processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3bd8f-440e-49e0-935f-1b1482fc138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"move_to_raw\")\n",
    "\n",
    "spark = get_spark_session(\"MoveStageToRaw\")\n",
    "log.info(\"[MoveToRaw] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8c8fc-4e96-4463-a77a-a09458085bfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `move_files_to_raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca82fb6-bab2-4a20-b3dc-a2fc73b759c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[MoveToRaw] Iniciando job de movimentação de arquivos.\")\n",
    "\n",
    "    parquet_files = check_files_in_folder(stage_path, \"*.parquet\")\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"[MoveToRaw][Error] Nenhum arquivo parquet encontrado em {stage_path}.\")\n",
    "\n",
    "    processing_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    move_files(\n",
    "        spark=spark,\n",
    "        source_files=parquet_files,\n",
    "        base_dest_path=raw_path,\n",
    "        processing_date=processing_date,\n",
    "    )\n",
    "\n",
    "    log.info(f\"[MoveToRaw] {len(parquet_files)} arquivo(s) movido(s) para raw/{processing_date}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[MoveToRaw][Error] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[MoveToRaw] Job de movimentação encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c257c7c-b07b-4fb9-b410-de59267e63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[MoveToRaw] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b1603-ee4e-4dac-8ec6-e2e371770c49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 5: cleanup_stage\n",
    "\n",
    "Este job remove os arquivos `csv` e `parquet` da pasta **Stage** após a conclusão do carregamento na camada **Raw**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42320064-44e6-4bbf-99c1-9172e94d1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(\"cleanup_stage\")\n",
    "\n",
    "spark = get_spark_session(\"CleanupStage\")\n",
    "log.info(\"[CleanupStage] SparkSession iniciada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf7bdb-f11f-4202-aeaf-5a0e43a03bbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Runner para o job `cleanup_stage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba326f-acf7-4aeb-9575-f115df0c17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log.info(\"[CleanupStage] Iniciando job de limpeza da stage.\")\n",
    "\n",
    "    removable_files = check_files_in_folder(stage_path, \"*.*\")\n",
    "    if not removable_files:\n",
    "        log.warning(\"[CleanupStage] Nenhum arquivo encontrado para remoção.\")\n",
    "    else:\n",
    "        delete_files(spark, removable_files)\n",
    "        log.info(f\"[CleanupStage] {len(removable_files)} arquivo(s) removido(s) da stage.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[CleanupStage][Error] Falha durante execução: {e}.\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    log.info(\"[CleanupStage] Job de limpeza encerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3278b-e9c5-45e1-939a-4d9a937e8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerra a sessão Spark para o job\n",
    "spark.stop()\n",
    "log.info(\"[CleanupStage] Sessão Spark finalizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005957e-e9df-4d52-9dc0-269cd0102e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
